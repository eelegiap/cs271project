{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install PyArabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download xx_ent_wiki_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.morphology import Morphology\n",
    "\n",
    "import pyarabic.araby as araby\n",
    "import pyarabic.number as number\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY SOURCE LANGUAGE\n",
    "srclang = 'Russian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy lang models\n",
    "\n",
    "if srclang == 'Spanish':\n",
    "    sourceNLP = spacy.load(\"es_core_news_sm\")\n",
    "elif srclang == 'Russian':\n",
    "    sourceNLP = spacy.load(\"ru_core_news_sm\")\n",
    "elif srclang == 'Arabic':\n",
    "    sourceNLP = spacy.load(\"xx_ent_wiki_sm\")\n",
    "    sourceNLP.add_pipe('sentencizer')\n",
    "    \n",
    "engNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'texts/{srclang}/rawsource.txt','r') as f:\n",
    "     sourcetxt = f.read().replace('\\n',' ').replace('\\t','')\n",
    "with open(f'texts/{srclang}/rawtarget.txt','r') as f:\n",
    "     targettxt = f.read().replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if srclang == 'Arabic':\n",
    "    # sourcedoc = araby.sentence_tokenize(sourcetxt)\n",
    "    sourcedoc = sourceNLP(sourcetxt)\n",
    "else:\n",
    "    sourcedoc = sourceNLP(sourcetxt)\n",
    "targetdoc = engNLP(targettxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentenize\n",
    "rawsrcsents = []\n",
    "rawtgtsents = []\n",
    "\n",
    "for sent in sourcedoc.sents:\n",
    "    rawsrcsents.append(sent.text)\n",
    "    \n",
    "newrawsrcsents = []\n",
    "for sent in rawsrcsents:\n",
    "    if sent == '':\n",
    "        continue\n",
    "    partfound = False\n",
    "    for part in ['Part I.','Part II.','Part III.','Part IV.','Part V.']:\n",
    "        if part in sent:\n",
    "            newrawsrcsents.append(part)\n",
    "            newrawsrcsents.append(sent.split(part)[1].strip())\n",
    "            partfound = True\n",
    "    if not partfound:\n",
    "        newrawsrcsents.append(sent)\n",
    "rawsrcsents = newrawsrcsents\n",
    "    \n",
    "        \n",
    "for sent in targetdoc.sents:\n",
    "    rawtgtsents.append(sent.text)\n",
    "\n",
    "newrawtgtsents = []\n",
    "for sent in rawtgtsents:\n",
    "    if sent == '':\n",
    "        continue\n",
    "    partfound = False\n",
    "    for part in ['Part I.','Part II.','Part III.','Part IV.','Part V.']:\n",
    "        if part in sent:\n",
    "            newrawtgtsents.append(part)\n",
    "            newrawtgtsents.append(sent.split(part)[1])\n",
    "            partfound = True\n",
    "    if not partfound:\n",
    "        newrawtgtsents.append(sent)\n",
    "rawtgtsents = newrawtgtsents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write standardized files (one line per sentence) for input to Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the inputs to bleualign\n",
    "with open(f'texts/{srclang}/sourcetextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawsrcsents))\n",
    "with open(f'texts/{srclang}/targettextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawtgtsents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized sentences for data output\n",
    "srctokens = []\n",
    "if srclang == 'Arabic':\n",
    "    for srcsent in rawsrcsents:\n",
    "        tokens = araby.tokenize(srcsent)\n",
    "        srctokens.append([{'text' : t, 'lemma' : t} for t in tokens])\n",
    "else:\n",
    "    for srcsent in rawsrcsents:\n",
    "        tokens = sourceNLP(srcsent)\n",
    "        srctokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])\n",
    "tgttokens = []\n",
    "for tgtsent in rawtgtsents:\n",
    "    tokens = engNLP(tgtsent)\n",
    "    tgttokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install translators --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/164 sents translated.\n",
      "machine translation took 2.1140940189361572 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "translatedsourcesents = []\n",
    "for i, sent in enumerate(rawsrcsents[:3]):\n",
    "    if i % 25 == 0:\n",
    "        print(f'{i}/{len(rawsrcsents)} sents translated.')\n",
    "    try:\n",
    "        translatedsourcesents.append(ts.google(sent, to_language = 'en'))\n",
    "    except:\n",
    "        print('problem on',sent)\n",
    "        translatedsourcesents.append('\\n')\n",
    "end = time.time()\n",
    "print(f'machine translation took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Part I.',\n",
       " 'For this day name, and can not put it where God put him from the month and Sunnis, but can not mention this day a particular time, but almost nearly.',\n",
       " 'The biggest thought that this time was happening today in dawn or lover.']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translatedsourcesents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "if srclang not in ['Arabic','Russian']:\n",
    "    with open(f'texts/{srclang}/translatedsource.txt','w') as f:\n",
    "        f.write('\\n'.join(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in article 0: \n",
      "processing\n",
      "computing alignment between srctotarget (file 0) and target text\n",
      "Evaluating sentences with bleu\n",
      "finished\n",
      "searching for longest path of good alignments\n",
      "finished\n",
      "Fri Apr 15 10:18:45 2022\n",
      "filling gaps\n",
      "finished\n",
      "Fri Apr 15 10:18:45 2022\n",
      "Results of BLEU 1-to-1 alignment\n",
      "\u001b[92m0: 0\u001b[1;m\n",
      "\u001b[92m1: 1\u001b[1;m\n",
      "\u001b[92m2: 2\u001b[1;m\n",
      "\u001b[92m3: 3\u001b[1;m\n",
      "\u001b[92m4: 4\u001b[1;m\n",
      "\u001b[92m5: 5\u001b[1;m\n",
      "\u001b[92m6: 6\u001b[1;m\n",
      "\u001b[92m7: 7\u001b[1;m\n",
      "\u001b[92m8: 8\u001b[1;m\n",
      "\u001b[92m9: 9\u001b[1;m\n",
      "\u001b[92m10: 10\u001b[1;m\n",
      "\u001b[92m11: 11\u001b[1;m\n",
      "\u001b[92m12: 12\u001b[1;m\n",
      "\u001b[92m13: 13\u001b[1;m\n",
      "\u001b[92m14: 14\u001b[1;m\n",
      "\u001b[92m15: 15\u001b[1;m\n",
      "\u001b[92m16: 16\u001b[1;m\n",
      "\u001b[92m17: 17\u001b[1;m\n",
      "\u001b[92m18: 18\u001b[1;m\n",
      "\u001b[92m19: 19\u001b[1;m\n",
      "\u001b[92m20: 20\u001b[1;m\n",
      "\u001b[92m21: 21\u001b[1;m\n",
      "\u001b[92m22: 22\u001b[1;m\n",
      "\u001b[92m23: 23\u001b[1;m\n",
      "\u001b[92m24: 24\u001b[1;m\n",
      "\u001b[92m25: 25\u001b[1;m\n",
      "\u001b[92m26: 26\u001b[1;m\n",
      "\u001b[92m27: 27\u001b[1;m\n",
      "\u001b[92m28: 28\u001b[1;m\n",
      "\u001b[92m29: 29\u001b[1;m\n",
      "\u001b[92m30: 30\u001b[1;m\n",
      "\u001b[92m31: 32\u001b[1;m\n",
      "\u001b[92m32: 33\u001b[1;m\n",
      "\u001b[92m33: 34\u001b[1;m\n",
      "\u001b[92m34: 35\u001b[1;m\n",
      "\u001b[92m35: 36\u001b[1;m\n",
      "\u001b[92m36: 37\u001b[1;m\n",
      "\u001b[92m37: 38\u001b[1;m\n",
      "\u001b[92m38: 39\u001b[1;m\n",
      "\u001b[92m39: 40\u001b[1;m\n",
      "\u001b[92m40: 41\u001b[1;m\n",
      "\u001b[92m41: 42\u001b[1;m\n",
      "\u001b[92m42: 43\u001b[1;m\n",
      "\u001b[92m43: 44\u001b[1;m\n",
      "\u001b[1;31m44: unaligned. best cand 304\u001b[1;m\n",
      "\u001b[92m45: 46\u001b[1;m\n",
      "\u001b[92m46: 47\u001b[1;m\n",
      "\u001b[92m47: 48\u001b[1;m\n",
      "\u001b[92m48: 49\u001b[1;m\n",
      "\u001b[92m49: 51\u001b[1;m\n",
      "\u001b[92m50: 52\u001b[1;m\n",
      "\u001b[92m51: 53\u001b[1;m\n",
      "\u001b[92m52: 54\u001b[1;m\n",
      "\u001b[92m53: 55\u001b[1;m\n",
      "\u001b[92m54: 56\u001b[1;m\n",
      "\u001b[92m55: 57\u001b[1;m\n",
      "\u001b[92m56: 58\u001b[1;m\n",
      "\u001b[92m57: 59\u001b[1;m\n",
      "\u001b[92m58: 60\u001b[1;m\n",
      "\u001b[92m59: 61\u001b[1;m\n",
      "\u001b[92m60: 62\u001b[1;m\n",
      "\u001b[92m61: 63\u001b[1;m\n",
      "\u001b[92m62: 64\u001b[1;m\n",
      "\u001b[92m63: 65\u001b[1;m\n",
      "\u001b[92m64: 66\u001b[1;m\n",
      "\u001b[92m65: 67\u001b[1;m\n",
      "\u001b[92m66: 68\u001b[1;m\n",
      "\u001b[1;31m67: unaligned. best cand 248\u001b[1;m\n",
      "\u001b[92m68: 72\u001b[1;m\n",
      "\u001b[92m69: 73\u001b[1;m\n",
      "\u001b[92m70: 74\u001b[1;m\n",
      "\u001b[92m71: 75\u001b[1;m\n",
      "\u001b[92m72: 76\u001b[1;m\n",
      "\u001b[92m73: 77\u001b[1;m\n",
      "\u001b[92m74: 78\u001b[1;m\n",
      "\u001b[92m75: 79\u001b[1;m\n",
      "\u001b[92m76: 80\u001b[1;m\n",
      "\u001b[92m77: 81\u001b[1;m\n",
      "\u001b[92m78: 82\u001b[1;m\n",
      "\u001b[92m79: 83\u001b[1;m\n",
      "\u001b[92m80: 84\u001b[1;m\n",
      "\u001b[92m81: 85\u001b[1;m\n",
      "\u001b[92m82: 87\u001b[1;m\n",
      "\u001b[1;31m83: unaligned. best cand []\u001b[1;m\n",
      "\u001b[1;31m84: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m85: 90\u001b[1;m\n",
      "\u001b[92m86: 91\u001b[1;m\n",
      "\u001b[92m87: 92\u001b[1;m\n",
      "\u001b[92m88: 93\u001b[1;m\n",
      "\u001b[92m89: 94\u001b[1;m\n",
      "\u001b[92m90: 95\u001b[1;m\n",
      "\u001b[92m91: 96\u001b[1;m\n",
      "\u001b[92m92: 98\u001b[1;m\n",
      "\u001b[92m93: 99\u001b[1;m\n",
      "\u001b[1;31m94: unaligned. best cand 113\u001b[1;m\n",
      "\u001b[92m95: 102\u001b[1;m\n",
      "\u001b[92m96: 104\u001b[1;m\n",
      "\u001b[92m97: 105\u001b[1;m\n",
      "\u001b[92m98: 106\u001b[1;m\n",
      "\u001b[92m99: 107\u001b[1;m\n",
      "\u001b[92m100: 108\u001b[1;m\n",
      "\u001b[92m101: 110\u001b[1;m\n",
      "\u001b[92m102: 111\u001b[1;m\n",
      "\u001b[92m103: 112\u001b[1;m\n",
      "\u001b[92m104: 114\u001b[1;m\n",
      "\u001b[92m105: 115\u001b[1;m\n",
      "\u001b[92m106: 117\u001b[1;m\n",
      "\u001b[92m107: 118\u001b[1;m\n",
      "\u001b[92m108: 119\u001b[1;m\n",
      "\u001b[92m109: 120\u001b[1;m\n",
      "\u001b[92m110: 121\u001b[1;m\n",
      "\u001b[92m111: 122\u001b[1;m\n",
      "\u001b[92m112: 123\u001b[1;m\n",
      "\u001b[92m113: 124\u001b[1;m\n",
      "\u001b[92m114: 125\u001b[1;m\n",
      "\u001b[92m115: 126\u001b[1;m\n",
      "\u001b[92m116: 127\u001b[1;m\n",
      "\u001b[92m117: 128\u001b[1;m\n",
      "\u001b[92m118: 129\u001b[1;m\n",
      "\u001b[92m119: 130\u001b[1;m\n",
      "\u001b[92m120: 131\u001b[1;m\n",
      "\u001b[1;31m121: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m122: 133\u001b[1;m\n",
      "\u001b[1;31m123: unaligned. best cand 231\u001b[1;m\n",
      "\u001b[92m124: 135\u001b[1;m\n",
      "\u001b[92m125: 136\u001b[1;m\n",
      "\u001b[92m126: 137\u001b[1;m\n",
      "\u001b[92m127: 138\u001b[1;m\n",
      "\u001b[92m128: 139\u001b[1;m\n",
      "\u001b[92m129: 140\u001b[1;m\n",
      "\u001b[92m130: 141\u001b[1;m\n",
      "\u001b[92m131: 142\u001b[1;m\n",
      "\u001b[92m132: 143\u001b[1;m\n",
      "\u001b[92m133: 144\u001b[1;m\n",
      "\u001b[92m134: 145\u001b[1;m\n",
      "\u001b[92m135: 146\u001b[1;m\n",
      "\u001b[92m136: 147\u001b[1;m\n",
      "\u001b[92m137: 148\u001b[1;m\n",
      "\u001b[1;31m138: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m139: 150\u001b[1;m\n",
      "\u001b[92m140: 152\u001b[1;m\n",
      "\u001b[92m141: 153\u001b[1;m\n",
      "\u001b[1;31m142: unaligned. best cand 37\u001b[1;m\n",
      "\u001b[92m143: 155\u001b[1;m\n",
      "\u001b[92m144: 156\u001b[1;m\n",
      "\u001b[92m145: 157\u001b[1;m\n",
      "\u001b[92m146: 158\u001b[1;m\n",
      "\u001b[92m147: 159\u001b[1;m\n",
      "\u001b[92m148: 160\u001b[1;m\n",
      "\u001b[92m149: 161\u001b[1;m\n",
      "\u001b[92m150: 162\u001b[1;m\n",
      "\u001b[92m151: 163\u001b[1;m\n",
      "\u001b[92m152: 164\u001b[1;m\n",
      "\u001b[92m153: 165\u001b[1;m\n",
      "\u001b[92m154: 167\u001b[1;m\n",
      "\u001b[1;31m155: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m156: 169\u001b[1;m\n",
      "\u001b[92m157: 170\u001b[1;m\n",
      "\u001b[92m158: 172\u001b[1;m\n",
      "\u001b[92m159: 173\u001b[1;m\n",
      "\u001b[92m160: 174\u001b[1;m\n",
      "\u001b[92m161: 175\u001b[1;m\n",
      "\u001b[92m162: 176\u001b[1;m\n",
      "\u001b[92m163: 177\u001b[1;m\n",
      "\u001b[92m164: 178\u001b[1;m\n",
      "\u001b[92m165: 179\u001b[1;m\n",
      "\u001b[92m166: 180\u001b[1;m\n",
      "\u001b[92m167: 181\u001b[1;m\n",
      "\u001b[92m168: 182\u001b[1;m\n",
      "\u001b[92m169: 183\u001b[1;m\n",
      "\u001b[92m170: 184\u001b[1;m\n",
      "\u001b[92m171: 185\u001b[1;m\n",
      "\u001b[92m172: 186\u001b[1;m\n",
      "\u001b[92m173: 187\u001b[1;m\n",
      "\u001b[92m174: 188\u001b[1;m\n",
      "\u001b[92m175: 189\u001b[1;m\n",
      "\u001b[92m176: 190\u001b[1;m\n",
      "\u001b[92m177: 191\u001b[1;m\n",
      "\u001b[92m178: 192\u001b[1;m\n",
      "\u001b[92m179: 193\u001b[1;m\n",
      "\u001b[1;31m180: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m181: 195\u001b[1;m\n",
      "\u001b[92m182: 196\u001b[1;m\n",
      "\u001b[92m183: 197\u001b[1;m\n",
      "\u001b[92m184: 198\u001b[1;m\n",
      "\u001b[92m185: 199\u001b[1;m\n",
      "\u001b[92m186: 200\u001b[1;m\n",
      "\u001b[92m187: 201\u001b[1;m\n",
      "\u001b[92m188: 202\u001b[1;m\n",
      "\u001b[92m189: 203\u001b[1;m\n",
      "\u001b[92m190: 204\u001b[1;m\n",
      "\u001b[92m191: 205\u001b[1;m\n",
      "\u001b[92m192: 206\u001b[1;m\n",
      "\u001b[92m193: 207\u001b[1;m\n",
      "\u001b[92m194: 208\u001b[1;m\n",
      "\u001b[1;31m195: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m196: 210\u001b[1;m\n",
      "\u001b[92m197: 211\u001b[1;m\n",
      "\u001b[92m198: 212\u001b[1;m\n",
      "\u001b[92m199: 213\u001b[1;m\n",
      "\u001b[92m200: 214\u001b[1;m\n",
      "\u001b[92m201: 215\u001b[1;m\n",
      "\u001b[92m202: 216\u001b[1;m\n",
      "\u001b[92m203: 217\u001b[1;m\n",
      "\u001b[92m204: 218\u001b[1;m\n",
      "\u001b[92m205: 219\u001b[1;m\n",
      "\u001b[92m206: 220\u001b[1;m\n",
      "\u001b[92m207: 221\u001b[1;m\n",
      "\u001b[92m208: 222\u001b[1;m\n",
      "\u001b[92m209: 223\u001b[1;m\n",
      "\u001b[92m210: 224\u001b[1;m\n",
      "\u001b[1;31m211: unaligned. best cand 196\u001b[1;m\n",
      "\u001b[92m212: 226\u001b[1;m\n",
      "\u001b[92m213: 227\u001b[1;m\n",
      "\u001b[1;31m214: unaligned. best cand 113\u001b[1;m\n",
      "\u001b[1;31m215: unaligned. best cand 103\u001b[1;m\n",
      "\u001b[92m216: 230\u001b[1;m\n",
      "\u001b[92m217: 231\u001b[1;m\n",
      "\u001b[92m218: 232\u001b[1;m\n",
      "\u001b[92m219: 233\u001b[1;m\n",
      "\u001b[92m220: 234\u001b[1;m\n",
      "\u001b[92m221: 235\u001b[1;m\n",
      "\u001b[92m222: 236\u001b[1;m\n",
      "\u001b[92m223: 237\u001b[1;m\n",
      "\u001b[92m224: 238\u001b[1;m\n",
      "\u001b[92m225: 239\u001b[1;m\n",
      "\u001b[92m226: 240\u001b[1;m\n",
      "\u001b[92m227: 241\u001b[1;m\n",
      "\u001b[92m228: 242\u001b[1;m\n",
      "\u001b[92m229: 243\u001b[1;m\n",
      "\u001b[92m230: 244\u001b[1;m\n",
      "\u001b[92m231: 245\u001b[1;m\n",
      "\u001b[92m232: 246\u001b[1;m\n",
      "\u001b[92m233: 247\u001b[1;m\n",
      "\u001b[92m234: 248\u001b[1;m\n",
      "\u001b[92m235: 249\u001b[1;m\n",
      "\u001b[92m236: 250\u001b[1;m\n",
      "\u001b[92m237: 251\u001b[1;m\n",
      "\u001b[1;31m238: unaligned. best cand 133\u001b[1;m\n",
      "\u001b[92m239: 253\u001b[1;m\n",
      "\u001b[92m240: 255\u001b[1;m\n",
      "\u001b[92m241: 256\u001b[1;m\n",
      "\u001b[92m242: 257\u001b[1;m\n",
      "\u001b[92m243: 258\u001b[1;m\n",
      "\u001b[92m244: 260\u001b[1;m\n",
      "\u001b[92m245: 262\u001b[1;m\n",
      "\u001b[92m246: 263\u001b[1;m\n",
      "\u001b[92m247: 265\u001b[1;m\n",
      "\u001b[92m248: 266\u001b[1;m\n",
      "\u001b[92m249: 267\u001b[1;m\n",
      "\u001b[92m250: 268\u001b[1;m\n",
      "\u001b[92m251: 269\u001b[1;m\n",
      "\u001b[1;31m252: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m253: 272\u001b[1;m\n",
      "\u001b[92m254: 273\u001b[1;m\n",
      "\u001b[1;31m255: unaligned. best cand 37\u001b[1;m\n",
      "\u001b[92m256: 275\u001b[1;m\n",
      "\u001b[92m257: 277\u001b[1;m\n",
      "\u001b[92m258: 278\u001b[1;m\n",
      "\u001b[92m259: 279\u001b[1;m\n",
      "\u001b[92m260: 280\u001b[1;m\n",
      "\u001b[92m261: 281\u001b[1;m\n",
      "\u001b[92m262: 282\u001b[1;m\n",
      "\u001b[1;31m263: unaligned. best cand 112\u001b[1;m\n",
      "\u001b[92m264: 284\u001b[1;m\n",
      "\u001b[92m265: 285\u001b[1;m\n",
      "\u001b[92m266: 286\u001b[1;m\n",
      "\u001b[92m267: 287\u001b[1;m\n",
      "\u001b[92m268: 289\u001b[1;m\n",
      "\u001b[92m269: 290\u001b[1;m\n",
      "\u001b[92m270: 291\u001b[1;m\n",
      "\u001b[92m271: 292\u001b[1;m\n",
      "\u001b[92m272: 293\u001b[1;m\n",
      "\u001b[92m273: 294\u001b[1;m\n",
      "\u001b[92m274: 295\u001b[1;m\n",
      "\u001b[92m275: 296\u001b[1;m\n",
      "\u001b[92m276: 297\u001b[1;m\n",
      "\u001b[92m277: 298\u001b[1;m\n",
      "\u001b[92m278: 299\u001b[1;m\n",
      "\u001b[92m279: 300\u001b[1;m\n",
      "\u001b[92m280: 301\u001b[1;m\n",
      "\u001b[92m281: 302\u001b[1;m\n",
      "\u001b[92m282: 303\u001b[1;m\n",
      "\u001b[92m283: 304\u001b[1;m\n",
      "\u001b[92m284: 305\u001b[1;m\n",
      "\u001b[92m285: 306\u001b[1;m\n",
      "\u001b[92m286: 307\u001b[1;m\n",
      "\u001b[92m287: 308\u001b[1;m\n",
      "\u001b[92m288: 309\u001b[1;m\n",
      "\u001b[92m289: 311\u001b[1;m\n",
      "\u001b[92m290: 312\u001b[1;m\n",
      "\u001b[1;31m291: unaligned. best cand 112\u001b[1;m\n",
      "\u001b[92m292: 314\u001b[1;m\n",
      "\u001b[92m293: 315\u001b[1;m\n",
      "\u001b[92m294: 316\u001b[1;m\n",
      "\u001b[92m295: 317\u001b[1;m\n",
      "\u001b[92m296: 318\u001b[1;m\n",
      "\u001b[92m297: 319\u001b[1;m\n",
      "\u001b[92m298: 320\u001b[1;m\n",
      "\u001b[92m299: 321\u001b[1;m\n",
      "\u001b[92m300: 322\u001b[1;m\n",
      "\u001b[92m301: 323\u001b[1;m\n",
      "\u001b[92m302: 324\u001b[1;m\n",
      "\u001b[92m303: 325\u001b[1;m\n",
      "\u001b[92m304: 326\u001b[1;m\n",
      "\u001b[92m305: 327\u001b[1;m\n",
      "\u001b[92m306: 328\u001b[1;m\n",
      "\u001b[92m307: 329\u001b[1;m\n",
      "\u001b[92m308: 330\u001b[1;m\n",
      "\u001b[92m309: 331\u001b[1;m\n",
      "\u001b[92m310: 332\u001b[1;m\n",
      "\u001b[92m311: 333\u001b[1;m\n",
      "\u001b[92m312: 334\u001b[1;m\n",
      "\u001b[92m313: 335\u001b[1;m\n",
      "\u001b[1;31m314: unaligned. best cand 103\u001b[1;m\n",
      "\u001b[1;31m315: unaligned. best cand 130\u001b[1;m\n",
      "\u001b[92m316: 338\u001b[1;m\n",
      "\u001b[92m317: 339\u001b[1;m\n",
      "\u001b[92m318: 341\u001b[1;m\n",
      "\u001b[1;31m319: unaligned. best cand 341\u001b[1;m\n",
      "\u001b[92m320: 343\u001b[1;m\n",
      "\n",
      "298 out of 321 source sentences aligned by BLEU 92.8348909657%\n",
      "after gap filling, 318 out of 321 source sentences aligned 99.0654205607%\n",
      "after gap filling, 338 out of 344 target sentences aligned 98.2558139535%\n",
      "alignment: 0 - 0\n",
      "alignment: 1 - 1\n",
      "alignment: 2 - 2\n",
      "alignment: 3 - 3\n",
      "alignment: 4 - 4\n",
      "alignment: 5 - 5\n",
      "alignment: 6 - 6\n",
      "alignment: 7 - 7\n",
      "alignment: 8 - 8\n",
      "alignment: 9 - 9\n",
      "alignment: 10 - 10\n",
      "alignment: 11 - 11\n",
      "alignment: 12 - 12\n",
      "alignment: 13 - 13\n",
      "alignment: 14 - 14\n",
      "alignment: 15 - 15\n",
      "alignment: 16 - 16\n",
      "alignment: 17 - 17\n",
      "alignment: 18 - 18\n",
      "alignment: 19 - 19\n",
      "alignment: 20 - 20\n",
      "alignment: 21 - 21\n",
      "alignment: 22 - 22\n",
      "alignment: 23 - 23\n",
      "alignment: 24 - 24\n",
      "alignment: 25 - 25\n",
      "alignment: 26 - 26\n",
      "alignment: 27 - 27\n",
      "alignment: 28 - 28\n",
      "alignment: 29 - 29\n",
      "alignment: 30 - 30,31\n",
      "alignment: 31 - 32\n",
      "alignment: 32 - 33\n",
      "alignment: 33 - 34\n",
      "alignment: 34 - 35\n",
      "alignment: 35 - 36\n",
      "alignment: 36 - 37\n",
      "alignment: 37 - 38\n",
      "alignment: 38 - 39\n",
      "alignment: 39 - 40\n",
      "alignment: 40 - 41\n",
      "alignment: 41 - 42\n",
      "alignment: 42 - 43\n",
      "alignment: 43 - 44\n",
      "alignment: 45 - 45,46\n",
      "alignment: 46 - 47\n",
      "alignment: 47 - 48\n",
      "alignment: 48 - 49\n",
      "alignment: 49 - 51\n",
      "alignment: 50 - 52\n",
      "alignment: 51 - 53\n",
      "alignment: 52 - 54\n",
      "alignment: 53 - 55\n",
      "alignment: 54 - 56\n",
      "alignment: 55 - 57\n",
      "alignment: 56 - 58\n",
      "alignment: 57 - 59\n",
      "alignment: 58 - 60\n",
      "alignment: 59 - 61\n",
      "alignment: 60 - 62\n",
      "alignment: 61 - 63\n",
      "alignment: 62 - 64\n",
      "alignment: 63 - 65\n",
      "alignment: 64 - 66\n",
      "alignment: 65 - 67\n",
      "alignment: 66 - 68,69\n",
      "alignment: 67 - 71\n",
      "alignment: 68 - 72\n",
      "alignment: 69 - 73\n",
      "alignment: 70 - 74\n",
      "alignment: 71 - 75\n",
      "alignment: 72 - 76\n",
      "alignment: 73 - 77\n",
      "alignment: 74 - 78\n",
      "alignment: 75 - 79\n",
      "alignment: 76 - 80\n",
      "alignment: 77 - 81\n",
      "alignment: 78 - 82\n",
      "alignment: 79 - 83\n",
      "alignment: 80 - 84\n",
      "alignment: 81 - 85\n",
      "alignment: 82 - 86,87\n",
      "alignment: 83 - 88\n",
      "alignment: 84 - 89\n",
      "alignment: 85 - 90\n",
      "alignment: 86 - 91\n",
      "alignment: 87 - 92\n",
      "alignment: 88 - 93\n",
      "alignment: 89 - 94\n",
      "alignment: 90 - 95\n",
      "alignment: 91 - 96,97\n",
      "alignment: 92 - 98\n",
      "alignment: 93 - 99,100\n",
      "alignment: 95 - 101,102,103\n",
      "alignment: 96 - 104\n",
      "alignment: 97 - 105\n",
      "alignment: 98 - 106\n",
      "alignment: 99 - 107\n",
      "alignment: 100 - 108,109\n",
      "alignment: 101 - 110\n",
      "alignment: 102 - 111\n",
      "alignment: 103 - 112\n",
      "alignment: 104 - 114\n",
      "alignment: 105 - 115,116\n",
      "alignment: 106 - 117\n",
      "alignment: 107 - 118\n",
      "alignment: 108 - 119\n",
      "alignment: 109 - 120\n",
      "alignment: 110 - 121\n",
      "alignment: 111 - 122\n",
      "alignment: 112 - 123\n",
      "alignment: 113 - 124\n",
      "alignment: 114 - 125\n",
      "alignment: 115 - 126\n",
      "alignment: 116 - 127\n",
      "alignment: 117 - 128\n",
      "alignment: 118 - 129\n",
      "alignment: 119 - 130\n",
      "alignment: 120 - 131\n",
      "alignment: 121 - 132\n",
      "alignment: 122 - 133\n",
      "alignment: 123 - 134\n",
      "alignment: 124 - 135\n",
      "alignment: 125 - 136\n",
      "alignment: 126 - 137\n",
      "alignment: 127 - 138\n",
      "alignment: 128 - 139\n",
      "alignment: 129 - 140\n",
      "alignment: 130 - 141\n",
      "alignment: 131 - 142\n",
      "alignment: 132 - 143\n",
      "alignment: 133 - 144\n",
      "alignment: 134 - 145\n",
      "alignment: 135 - 146\n",
      "alignment: 136 - 147\n",
      "alignment: 137 - 148\n",
      "alignment: 139 - 149,150\n",
      "alignment: 140 - 151,152\n",
      "alignment: 141 - 153\n",
      "alignment: 142 - 154\n",
      "alignment: 143 - 155\n",
      "alignment: 144 - 156\n",
      "alignment: 145 - 157\n",
      "alignment: 146 - 158\n",
      "alignment: 147 - 159\n",
      "alignment: 148 - 160\n",
      "alignment: 149 - 161\n",
      "alignment: 150 - 162\n",
      "alignment: 151 - 163\n",
      "alignment: 152 - 164\n",
      "alignment: 153 - 165\n",
      "alignment: 154 - 167\n",
      "alignment: 155 - 168\n",
      "alignment: 156 - 169\n",
      "alignment: 157 - 170\n",
      "alignment: 158 - 171,172\n",
      "alignment: 159 - 173\n",
      "alignment: 160 - 174\n",
      "alignment: 161 - 175\n",
      "alignment: 162 - 176\n",
      "alignment: 163 - 177\n",
      "alignment: 164 - 178\n",
      "alignment: 165 - 179\n",
      "alignment: 166 - 180\n",
      "alignment: 167 - 181\n",
      "alignment: 168 - 182\n",
      "alignment: 169 - 183\n",
      "alignment: 170 - 184\n",
      "alignment: 171 - 185\n",
      "alignment: 172 - 186\n",
      "alignment: 173 - 187\n",
      "alignment: 174 - 188\n",
      "alignment: 175 - 189\n",
      "alignment: 176 - 190\n",
      "alignment: 177 - 191\n",
      "alignment: 178 - 192\n",
      "alignment: 179 - 193\n",
      "alignment: 180 - 194\n",
      "alignment: 181 - 195\n",
      "alignment: 182 - 196\n",
      "alignment: 183 - 197\n",
      "alignment: 184 - 198\n",
      "alignment: 185 - 199\n",
      "alignment: 186 - 200\n",
      "alignment: 187 - 201\n",
      "alignment: 188 - 202\n",
      "alignment: 189 - 203\n",
      "alignment: 190 - 204\n",
      "alignment: 191 - 205\n",
      "alignment: 192 - 206\n",
      "alignment: 193 - 207\n",
      "alignment: 194 - 208\n",
      "alignment: 195 - 209\n",
      "alignment: 196 - 210\n",
      "alignment: 197 - 211\n",
      "alignment: 198 - 212\n",
      "alignment: 199 - 213\n",
      "alignment: 200 - 214\n",
      "alignment: 201 - 215\n",
      "alignment: 202 - 216\n",
      "alignment: 203 - 217\n",
      "alignment: 204 - 218\n",
      "alignment: 205 - 219\n",
      "alignment: 206 - 220\n",
      "alignment: 207 - 221\n",
      "alignment: 208 - 222\n",
      "alignment: 209 - 223\n",
      "alignment: 210 - 224\n",
      "alignment: 211 - 225\n",
      "alignment: 212 - 226\n",
      "alignment: 213 - 227\n",
      "alignment: 214 - 228\n",
      "alignment: 215 - 229\n",
      "alignment: 216 - 230\n",
      "alignment: 217 - 231\n",
      "alignment: 218 - 232\n",
      "alignment: 219 - 233\n",
      "alignment: 220 - 234\n",
      "alignment: 221 - 235\n",
      "alignment: 222 - 236\n",
      "alignment: 223 - 237\n",
      "alignment: 224 - 238\n",
      "alignment: 225 - 239\n",
      "alignment: 226 - 240\n",
      "alignment: 227 - 241\n",
      "alignment: 228 - 242\n",
      "alignment: 229 - 243\n",
      "alignment: 230 - 244\n",
      "alignment: 231 - 245\n",
      "alignment: 232 - 246\n",
      "alignment: 233 - 247\n",
      "alignment: 234 - 248\n",
      "alignment: 235 - 249\n",
      "alignment: 236 - 250\n",
      "alignment: 237 - 251\n",
      "alignment: 238 - 252\n",
      "alignment: 239 - 253\n",
      "alignment: 240 - 254,255\n",
      "alignment: 241 - 256\n",
      "alignment: 242 - 257\n",
      "alignment: 243 - 258\n",
      "alignment: 244 - 259,260,261\n",
      "alignment: 245 - 262\n",
      "alignment: 246 - 263\n",
      "alignment: 247 - 264,265\n",
      "alignment: 248 - 266\n",
      "alignment: 249 - 267\n",
      "alignment: 250 - 268\n",
      "alignment: 251 - 269,270\n",
      "alignment: 252 - 271\n",
      "alignment: 253 - 272\n",
      "alignment: 254 - 273\n",
      "alignment: 255 - 274\n",
      "alignment: 256 - 275\n",
      "alignment: 257 - 276,277\n",
      "alignment: 258 - 278\n",
      "alignment: 259 - 279\n",
      "alignment: 260 - 280\n",
      "alignment: 261 - 281\n",
      "alignment: 262 - 282\n",
      "alignment: 263 - 283\n",
      "alignment: 264 - 284\n",
      "alignment: 265 - 285\n",
      "alignment: 266 - 286\n",
      "alignment: 267 - 287\n",
      "alignment: 268 - 289\n",
      "alignment: 269 - 290\n",
      "alignment: 270 - 291\n",
      "alignment: 271 - 292\n",
      "alignment: 272 - 293\n",
      "alignment: 273 - 294\n",
      "alignment: 274 - 295\n",
      "alignment: 275 - 296\n",
      "alignment: 276 - 297\n",
      "alignment: 277 - 298\n",
      "alignment: 278 - 299\n",
      "alignment: 279 - 300\n",
      "alignment: 280 - 301\n",
      "alignment: 281 - 302\n",
      "alignment: 282 - 303\n",
      "alignment: 283 - 304\n",
      "alignment: 284 - 305\n",
      "alignment: 285 - 306\n",
      "alignment: 286 - 307\n",
      "alignment: 287 - 308\n",
      "alignment: 288 - 309,310\n",
      "alignment: 289 - 311\n",
      "alignment: 290 - 312\n",
      "alignment: 291 - 313\n",
      "alignment: 292 - 314\n",
      "alignment: 293 - 315\n",
      "alignment: 294 - 316\n",
      "alignment: 295 - 317\n",
      "alignment: 296 - 318\n",
      "alignment: 297 - 319\n",
      "alignment: 298 - 320\n",
      "alignment: 299 - 321\n",
      "alignment: 300 - 322\n",
      "alignment: 301 - 323\n",
      "alignment: 302 - 324\n",
      "alignment: 303 - 325\n",
      "alignment: 304 - 326\n",
      "alignment: 305 - 327\n",
      "alignment: 306 - 328\n",
      "alignment: 307 - 329\n",
      "alignment: 308 - 330\n",
      "alignment: 309 - 331\n",
      "alignment: 310 - 332\n",
      "alignment: 311 - 333\n",
      "alignment: 312 - 334\n",
      "alignment: 313 - 335\n",
      "alignment: 314,315 - 336\n",
      "alignment: 316 - 338\n",
      "alignment: 317 - 339\n",
      "alignment: 318 - 340,341\n",
      "alignment: 319 - 342\n",
      "alignment: 320 - 343\n",
      "\n",
      "finished with article\n",
      "\n",
      "====================\n",
      "\n",
      "sentence alignment took 1.8402040004730225 seconds\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "start = time.time()\n",
    "!./bleualign.py -s texts/russian/sourcetextforbleualign.txt -t texts/russian/targettextforbleualign.txt --srctotarget texts/russian/translatedsource.txt -o texts/russian/outputfile --verbosity 2\n",
    "end = time.time()\n",
    "print(f'sentence alignment took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [START HERE] 3. Read sentence-aligned files (from Bleualign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'texts/{srclang}/outputfile-s','r') as f:\n",
    "    alignedsrc = f.read().split('\\n')\n",
    "with open(f'texts/{srclang}/outputfile-t','r') as f:\n",
    "    alignedtgt = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Не заставляйте же меня страдать еще больше!',\n",
       " 'Don’t make me suffer still more!')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(alignedsrc)))\n",
    "alignedsrc[i], alignedtgt[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Часть I (Дама с собачкой).',\n",
       " ' Говорили, что на набережной появилось новое лицо: дама с собачкой.',\n",
       " 'Дмитрий Дмитрич Гуров, проживший в Ялте уже две недели и привыкший тут, тоже стал интересоваться новыми лицами.',\n",
       " 'Сидя в павильоне у Верне, он видел, как по набережной прошла молодая дама, невысокого роста блондинка, в берете; за нею бежал белый шпиц.',\n",
       " 'И потом он встречал ее в городском саду и на сквере по нескольку раз в день.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawsrcsents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Часть I (Дама с собачкой).',\n",
       " ' Говорили, что на набережной появилось новое лицо: дама с собачкой.',\n",
       " 'Дмитрий Дмитрич Гуров, проживший в Ялте уже две недели и привыкший тут, тоже стал интересоваться новыми лицами.',\n",
       " 'Сидя в павильоне у Верне, он видел, как по набережной прошла молодая дама, невысокого роста блондинка, в берете; за нею бежал белый шпиц.',\n",
       " 'И потом он встречал ее в городском саду и на сквере по нескольку раз в день.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignedsrc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent to sent alignment\n",
    "# oneLinesrc, oneLineEng = rawsrcsents, rawtgtsents\n",
    "# alignedsrc, alignedEng = alignedsrc, alignedtgt\n",
    "\n",
    "sentAlignments = []\n",
    "alignmentLookup = dict()\n",
    "\n",
    "for alignsrcLine, aligntgtLine in zip(alignedsrc, alignedtgt):\n",
    "    srcIndices = []\n",
    "    tgtIndices = []\n",
    "    for i, srcSent in enumerate(rawsrcsents):\n",
    "        if srcSent in ['','\\n',' ',')']:\n",
    "            continue\n",
    "        if srcSent in alignsrcLine:\n",
    "            srcIndices.append(i)\n",
    "    for j, tgtSent in enumerate(rawtgtsents):\n",
    "        if tgtSent in ['','\\n',' ',')','. .']:\n",
    "            continue\n",
    "        if tgtSent in aligntgtLine:\n",
    "            tgtIndices.append(j)\n",
    "\n",
    "    sentAlignments.append({\n",
    "        'indices' : (srcIndices, tgtIndices),\n",
    "        'sents' : (alignsrcLine, aligntgtLine)\n",
    "    })\n",
    "    for i in srcIndices:\n",
    "        alignmentLookup.setdefault(i, [])\n",
    "        for j in tgtIndices:\n",
    "            alignmentLookup[i].append(j)\n",
    "    \n",
    "# for alignsrcSent, alignEngSent in zip(alignedsrc, alignedEng):\n",
    "#     if srcIndex % 50 == 0:\n",
    "#         print(f'{srcIndex}/{len(rawsrcsents)} sentences parsed.')\n",
    "#     individualEngSents = [sent.text for sent in engNLP(alignEngSent).sents]\n",
    "#     for indEngSent in individualEngSents:\n",
    "#         for i, thisEngLine in enumerate(oneLineEng):\n",
    "#             if indEngSent.strip() == thisEngLine.strip():\n",
    "#                 engIndex = i\n",
    "#         for j, thissrcLine in enumerate(oneLinesrc):\n",
    "#             if alignsrcSent.strip() == thissrcLine.strip():\n",
    "#                 srcIndex = j\n",
    "#         sentAlignments.append({\n",
    "#             'indices' : (srcIndex, engIndex),\n",
    "#             'sents' : (oneLinesrc[srcIndex], oneLineEng[engIndex])\n",
    "#         })\n",
    "#         alignmentLookup.setdefault(srcIndex,[])\n",
    "#         alignmentLookup[srcIndex].append(engIndex)\n",
    "#     srcIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: [0],\n",
       " 1: [1],\n",
       " 2: [2],\n",
       " 3: [3],\n",
       " 4: [4],\n",
       " 5: [5],\n",
       " 6: [6],\n",
       " 7: [7],\n",
       " 8: [8],\n",
       " 9: [9],\n",
       " 10: [10],\n",
       " 11: [11],\n",
       " 12: [12],\n",
       " 13: [13],\n",
       " 14: [14],\n",
       " 15: [15],\n",
       " 16: [16],\n",
       " 17: [17],\n",
       " 18: [18],\n",
       " 19: [19],\n",
       " 20: [20],\n",
       " 21: [21],\n",
       " 22: [22],\n",
       " 23: [23],\n",
       " 24: [24],\n",
       " 25: [25],\n",
       " 26: [26],\n",
       " 27: [27],\n",
       " 28: [28],\n",
       " 29: [29],\n",
       " 30: [30, 31],\n",
       " 31: [32],\n",
       " 32: [33],\n",
       " 33: [34],\n",
       " 34: [35],\n",
       " 35: [36],\n",
       " 36: [37],\n",
       " 37: [38],\n",
       " 38: [39],\n",
       " 39: [40],\n",
       " 40: [41],\n",
       " 41: [42],\n",
       " 42: [43],\n",
       " 43: [44],\n",
       " 45: [45, 46],\n",
       " 46: [47],\n",
       " 47: [48],\n",
       " 48: [49],\n",
       " 49: [51],\n",
       " 50: [52],\n",
       " 51: [53],\n",
       " 52: [54],\n",
       " 53: [55],\n",
       " 54: [56],\n",
       " 55: [57],\n",
       " 56: [58],\n",
       " 57: [59],\n",
       " 58: [60],\n",
       " 59: [61],\n",
       " 60: [62],\n",
       " 61: [63],\n",
       " 62: [64],\n",
       " 63: [65],\n",
       " 64: [66],\n",
       " 65: [67],\n",
       " 66: [68, 69],\n",
       " 67: [71],\n",
       " 68: [72],\n",
       " 69: [73],\n",
       " 70: [74],\n",
       " 71: [75],\n",
       " 72: [76],\n",
       " 73: [77],\n",
       " 74: [78, 109],\n",
       " 75: [79],\n",
       " 76: [80],\n",
       " 77: [81],\n",
       " 78: [82],\n",
       " 79: [83],\n",
       " 80: [84],\n",
       " 81: [85],\n",
       " 82: [86, 87],\n",
       " 83: [88],\n",
       " 84: [89],\n",
       " 85: [90],\n",
       " 86: [91],\n",
       " 87: [92],\n",
       " 88: [93],\n",
       " 89: [94],\n",
       " 90: [95],\n",
       " 91: [96, 97],\n",
       " 92: [98],\n",
       " 93: [99],\n",
       " 95: [101, 102, 103],\n",
       " 96: [104],\n",
       " 97: [105],\n",
       " 98: [106],\n",
       " 99: [107],\n",
       " 100: [108, 109],\n",
       " 101: [110],\n",
       " 102: [111],\n",
       " 103: [112],\n",
       " 104: [114],\n",
       " 105: [115, 116],\n",
       " 106: [117],\n",
       " 107: [118],\n",
       " 108: [119],\n",
       " 109: [120],\n",
       " 110: [121],\n",
       " 111: [122],\n",
       " 112: [123],\n",
       " 113: [124],\n",
       " 114: [125],\n",
       " 115: [126],\n",
       " 116: [127],\n",
       " 117: [128],\n",
       " 118: [129],\n",
       " 119: [130],\n",
       " 120: [131],\n",
       " 121: [132],\n",
       " 122: [133],\n",
       " 123: [134],\n",
       " 124: [135],\n",
       " 125: [136],\n",
       " 126: [137],\n",
       " 127: [138],\n",
       " 128: [139],\n",
       " 129: [140],\n",
       " 130: [141],\n",
       " 131: [142],\n",
       " 132: [143],\n",
       " 133: [144],\n",
       " 134: [145],\n",
       " 135: [146],\n",
       " 136: [147],\n",
       " 137: [148],\n",
       " 139: [149, 150],\n",
       " 140: [109, 151, 152],\n",
       " 141: [153],\n",
       " 142: [154],\n",
       " 143: [155],\n",
       " 144: [156],\n",
       " 145: [157],\n",
       " 146: [158],\n",
       " 147: [159],\n",
       " 148: [160],\n",
       " 149: [161],\n",
       " 150: [162],\n",
       " 151: [163],\n",
       " 153: [165],\n",
       " 154: [167],\n",
       " 155: [168],\n",
       " 156: [169],\n",
       " 157: [170],\n",
       " 158: [171, 172],\n",
       " 159: [173],\n",
       " 160: [174],\n",
       " 161: [175],\n",
       " 162: [176],\n",
       " 163: [177],\n",
       " 164: [178],\n",
       " 165: [179],\n",
       " 166: [180],\n",
       " 167: [181],\n",
       " 168: [182],\n",
       " 169: [183],\n",
       " 170: [184],\n",
       " 171: [185],\n",
       " 172: [186],\n",
       " 173: [187],\n",
       " 174: [188],\n",
       " 175: [189],\n",
       " 176: [190],\n",
       " 177: [191],\n",
       " 178: [192],\n",
       " 179: [193],\n",
       " 180: [194],\n",
       " 181: [195],\n",
       " 182: [196],\n",
       " 183: [197],\n",
       " 184: [198],\n",
       " 185: [199],\n",
       " 186: [200],\n",
       " 187: [201],\n",
       " 188: [202],\n",
       " 189: [203],\n",
       " 190: [204],\n",
       " 191: [205],\n",
       " 192: [206],\n",
       " 193: [207],\n",
       " 194: [208],\n",
       " 195: [209],\n",
       " 196: [210],\n",
       " 197: [211],\n",
       " 198: [212],\n",
       " 199: [213],\n",
       " 200: [214],\n",
       " 201: [215],\n",
       " 202: [216],\n",
       " 203: [217],\n",
       " 204: [218],\n",
       " 205: [219],\n",
       " 206: [220],\n",
       " 207: [221],\n",
       " 208: [222],\n",
       " 209: [223],\n",
       " 210: [224],\n",
       " 211: [225],\n",
       " 212: [226],\n",
       " 213: [227],\n",
       " 214: [228],\n",
       " 215: [229],\n",
       " 216: [230],\n",
       " 217: [231],\n",
       " 218: [232],\n",
       " 219: [233],\n",
       " 220: [234],\n",
       " 221: [235],\n",
       " 222: [236],\n",
       " 223: [237],\n",
       " 224: [238],\n",
       " 225: [239],\n",
       " 226: [240],\n",
       " 227: [241],\n",
       " 228: [242],\n",
       " 229: [243],\n",
       " 230: [244],\n",
       " 231: [245],\n",
       " 232: [246],\n",
       " 233: [247],\n",
       " 234: [248],\n",
       " 235: [249],\n",
       " 236: [250],\n",
       " 237: [251],\n",
       " 238: [252],\n",
       " 239: [253],\n",
       " 240: [254, 255],\n",
       " 241: [256],\n",
       " 242: [257],\n",
       " 243: [258],\n",
       " 244: [259, 260, 261],\n",
       " 245: [262],\n",
       " 246: [263],\n",
       " 247: [264, 265],\n",
       " 248: [266],\n",
       " 249: [267],\n",
       " 250: [268],\n",
       " 251: [269, 270],\n",
       " 252: [271],\n",
       " 253: [272],\n",
       " 254: [273],\n",
       " 255: [274],\n",
       " 256: [275],\n",
       " 257: [276, 277],\n",
       " 258: [278],\n",
       " 259: [279],\n",
       " 260: [280],\n",
       " 261: [281],\n",
       " 262: [282],\n",
       " 263: [283],\n",
       " 264: [284],\n",
       " 265: [285],\n",
       " 266: [286],\n",
       " 267: [287],\n",
       " 268: [289],\n",
       " 269: [290],\n",
       " 270: [291],\n",
       " 271: [292],\n",
       " 272: [293],\n",
       " 273: [294],\n",
       " 274: [295],\n",
       " 275: [296],\n",
       " 276: [297],\n",
       " 277: [298],\n",
       " 278: [299],\n",
       " 279: [300],\n",
       " 280: [301],\n",
       " 281: [302],\n",
       " 282: [303],\n",
       " 283: [304],\n",
       " 284: [305],\n",
       " 285: [306],\n",
       " 286: [307],\n",
       " 287: [308],\n",
       " 288: [309, 310],\n",
       " 289: [311],\n",
       " 290: [312],\n",
       " 291: [313],\n",
       " 292: [314],\n",
       " 293: [315],\n",
       " 294: [316],\n",
       " 295: [317],\n",
       " 296: [318],\n",
       " 297: [319],\n",
       " 298: [320],\n",
       " 299: [321],\n",
       " 300: [322],\n",
       " 301: [323],\n",
       " 302: [324],\n",
       " 303: [325],\n",
       " 304: [326],\n",
       " 305: [327],\n",
       " 306: [328],\n",
       " 307: [329],\n",
       " 308: [330],\n",
       " 309: [331],\n",
       " 310: [332],\n",
       " 311: [333],\n",
       " 312: [334],\n",
       " 313: [335],\n",
       " 314: [336],\n",
       " 315: [336],\n",
       " 316: [338],\n",
       " 317: [339],\n",
       " 318: [340, 341],\n",
       " 319: [340, 341, 340, 342],\n",
       " 320: [343]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignmentLookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'jsondata/{srclang}/sentAlignment4-18.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentAlignments, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE DONT NEED - check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В его наружности, в характере, во всей его натуре было что-то привлекательное, неуловимое, что располагало к нему женщин, манило их; он знал об этом, и самого его тоже какая-то сила влекла к ним.\n",
      "In his appearance, in his character, in his whole nature there was something attractive and elusive that disposed women towards him and enticed them; he knew that, and he himself was attracted to them by some force.\n"
     ]
    }
   ],
   "source": [
    "# chec, k it works\n",
    "randSentAlign = random.choice(sentAlignments)\n",
    "s, t = randSentAlign['sents']\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parse word alignment using SimAlign (recommended: fast and high coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install simalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-04-18 19:06:33,080 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading word aligner tool took 4.5657641887664795 seconds\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "start = time.time()\n",
    "# making an instance of our model.\n",
    "# You can specify the embedding model and all alignment settings in the constructor.\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")\n",
    "end = time.time()\n",
    "print(f'downloading word aligner tool took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate word alignment with SimAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(321, 344)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rawsrcsents), len(rawtgtsents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/321 sentences parsed in 0.0007679462432861328 s.\n",
      "25/321 sentences parsed in 61.51004886627197 s.\n",
      "50/321 sentences parsed in 96.67140698432922 s.\n",
      "75/321 sentences parsed in 208.13037300109863 s.\n",
      "100/321 sentences parsed in 224.94836902618408 s.\n",
      "125/321 sentences parsed in 253.2319040298462 s.\n",
      "150/321 sentences parsed in 305.49098110198975 s.\n",
      "175/321 sentences parsed in 336.7091188430786 s.\n",
      "200/321 sentences parsed in 360.60863995552063 s.\n",
      "225/321 sentences parsed in 411.69376516342163 s.\n",
      "250/321 sentences parsed in 440.1134970188141 s.\n",
      "275/321 sentences parsed in 451.2287931442261 s.\n",
      "300/321 sentences parsed in 483.06016993522644 s.\n",
      "parsed in 508.19329619407654 s\n"
     ]
    }
   ],
   "source": [
    "# get rid of white space at end\n",
    "your_data = zip(rawsrcsents, rawtgtsents)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "wordAlignmentList = []\n",
    "\n",
    "for i, srcsent in enumerate(rawsrcsents):\n",
    "    if i % 25 == 0:\n",
    "        currently = time.time()\n",
    "        print(f\"{i}/{len(rawsrcsents)} sentences parsed in {currently-start} s.\")\n",
    "        \n",
    "    srcTokens = []\n",
    "    if srclang != 'Arabic':\n",
    "        srcDoc = sourceNLP(srcsent)\n",
    "\n",
    "        for tid, token in enumerate(srcDoc):\n",
    "            srcTokens.append({\n",
    "                'tokenid' : tid,\n",
    "                'pos' : token.pos_, \n",
    "                'text' : token.text, \n",
    "                'lemma' : token.lemma_,\n",
    "                'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "            })\n",
    "    else:\n",
    "        srcDoc = araby.tokenize(srcsent)\n",
    "        for tidx, token in enumerate(srcDoc):\n",
    "            srcTokens.append({\n",
    "                'tokenid' : tidx,\n",
    "                'pos' : 'N/A', \n",
    "                'text' : token, \n",
    "                'lemma' : token,\n",
    "                'features' : 'N/A'\n",
    "            })\n",
    "\n",
    "    try:\n",
    "        jLst = alignmentLookup[i]\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    for j in jLst:\n",
    "        tgtDoc = engNLP(rawtgtsents[j])\n",
    "\n",
    "        tgtTokens = []\n",
    "        for tid, token in enumerate(tgtDoc):\n",
    "            tgtTokens.append({\n",
    "                'tokenid' : tid,\n",
    "                'pos' : token.pos_, \n",
    "                'text' : token.text, \n",
    "                'lemma' : token.lemma_,\n",
    "                'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "            })\n",
    "\n",
    "        if srclang != 'Arabic':\n",
    "            src = [t.text for t in srcDoc]\n",
    "        else:\n",
    "            src = srcDoc\n",
    "            \n",
    "        tgt = [t.text for t in tgtDoc]\n",
    "        # alignments = myaligner.get_word_aligns(src, tgt)\n",
    "        \n",
    "        try:\n",
    "            alignments = myaligner.get_word_aligns(src, tgt)\n",
    "            itermax = alignments['itermax']\n",
    "\n",
    "            wordAlignmentList.append({\n",
    "                'alignedwordindices' : itermax,\n",
    "                'alignedwords' : [(src[s], tgt[t]) for s, t in itermax],\n",
    "                'srctokens' : srcTokens,\n",
    "                'tgttokens' : tgtTokens,\n",
    "                'srcsentidx' : i,\n",
    "                'tgtsentidx' : j,\n",
    "            })\n",
    "        except:\n",
    "            print('problem on', src, tgt)\n",
    "end = time.time()\n",
    "print('parsed in',end-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alignedwordindices': [(0, 0),\n",
       "  (1, 1),\n",
       "  (2, 2),\n",
       "  (2, 3),\n",
       "  (3, 4),\n",
       "  (4, 5),\n",
       "  (4, 6),\n",
       "  (5, 7),\n",
       "  (6, 8),\n",
       "  (7, 9),\n",
       "  (10, 25),\n",
       "  (11, 27),\n",
       "  (12, 24),\n",
       "  (13, 10),\n",
       "  (13, 11),\n",
       "  (14, 12),\n",
       "  (15, 13),\n",
       "  (16, 15),\n",
       "  (16, 16),\n",
       "  (17, 17),\n",
       "  (19, 19),\n",
       "  (19, 23),\n",
       "  (20, 20),\n",
       "  (21, 22),\n",
       "  (22, 28),\n",
       "  (23, 29),\n",
       "  (24, 30),\n",
       "  (25, 31),\n",
       "  (26, 32),\n",
       "  (26, 33),\n",
       "  (27, 34),\n",
       "  (28, 35)],\n",
       " 'alignedwords': [('Сидя', 'Sitting'),\n",
       "  ('в', 'in'),\n",
       "  ('павильоне', 'a'),\n",
       "  ('павильоне', 'pavilion'),\n",
       "  ('у', 'at'),\n",
       "  ('Верне', 'Vernet'),\n",
       "  ('Верне', '’s'),\n",
       "  (',', ','),\n",
       "  ('он', 'he'),\n",
       "  ('видел', 'saw'),\n",
       "  ('по', 'along'),\n",
       "  ('набережной', 'embankment'),\n",
       "  ('прошла', 'walking'),\n",
       "  ('молодая', 'a'),\n",
       "  ('молодая', 'young'),\n",
       "  ('дама', 'woman'),\n",
       "  (',', ','),\n",
       "  ('невысокого', 'very'),\n",
       "  ('невысокого', 'tall'),\n",
       "  ('роста', ','),\n",
       "  (',', ','),\n",
       "  (',', ','),\n",
       "  ('в', 'in'),\n",
       "  ('берете', 'beret'),\n",
       "  (';', ';'),\n",
       "  ('за', 'behind'),\n",
       "  ('нею', 'her'),\n",
       "  ('бежал', 'ran'),\n",
       "  ('белый', 'a'),\n",
       "  ('белый', 'white'),\n",
       "  ('шпиц', 'spitz'),\n",
       "  ('.', '.')],\n",
       " 'srctokens': [{'tokenid': 0,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'Сидя',\n",
       "   'lemma': 'сидеть',\n",
       "   'features': {'Aspect': 'Imp',\n",
       "    'Tense': 'Pres',\n",
       "    'VerbForm': 'Conv',\n",
       "    'Voice': 'Act'}},\n",
       "  {'tokenid': 1, 'pos': 'ADP', 'text': 'в', 'lemma': 'в', 'features': {}},\n",
       "  {'tokenid': 2,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'павильоне',\n",
       "   'lemma': 'павильон',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Loc',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 3, 'pos': 'ADP', 'text': 'у', 'lemma': 'у', 'features': {}},\n",
       "  {'tokenid': 4,\n",
       "   'pos': 'PROPN',\n",
       "   'text': 'Верне',\n",
       "   'lemma': 'верне',\n",
       "   'features': {'Animacy': 'Anim',\n",
       "    'Case': 'Gen',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 5, 'pos': 'PUNCT', 'text': ',', 'lemma': ',', 'features': {}},\n",
       "  {'tokenid': 6,\n",
       "   'pos': 'PRON',\n",
       "   'text': 'он',\n",
       "   'lemma': 'он',\n",
       "   'features': {'Case': 'Nom',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing',\n",
       "    'Person': 'Third'}},\n",
       "  {'tokenid': 7,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'видел',\n",
       "   'lemma': 'видеть',\n",
       "   'features': {'Aspect': 'Imp',\n",
       "    'Gender': 'Masc',\n",
       "    'Mood': 'Ind',\n",
       "    'Number': 'Sing',\n",
       "    'Tense': 'Past',\n",
       "    'VerbForm': 'Fin',\n",
       "    'Voice': 'Act'}},\n",
       "  {'tokenid': 8, 'pos': 'PUNCT', 'text': ',', 'lemma': ',', 'features': {}},\n",
       "  {'tokenid': 9,\n",
       "   'pos': 'SCONJ',\n",
       "   'text': 'как',\n",
       "   'lemma': 'как',\n",
       "   'features': {}},\n",
       "  {'tokenid': 10, 'pos': 'ADP', 'text': 'по', 'lemma': 'по', 'features': {}},\n",
       "  {'tokenid': 11,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'набережной',\n",
       "   'lemma': 'набережная',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Dat',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 12,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'прошла',\n",
       "   'lemma': 'пройти',\n",
       "   'features': {'Aspect': 'Perf',\n",
       "    'Gender': 'Fem',\n",
       "    'Mood': 'Ind',\n",
       "    'Number': 'Sing',\n",
       "    'Tense': 'Past',\n",
       "    'VerbForm': 'Fin',\n",
       "    'Voice': 'Act'}},\n",
       "  {'tokenid': 13,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'молодая',\n",
       "   'lemma': 'молодой',\n",
       "   'features': {'Case': 'Nom',\n",
       "    'Degree': 'Pos',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 14,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'дама',\n",
       "   'lemma': 'дама',\n",
       "   'features': {'Animacy': 'Anim',\n",
       "    'Case': 'Nom',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 15, 'pos': 'PUNCT', 'text': ',', 'lemma': ',', 'features': {}},\n",
       "  {'tokenid': 16,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'невысокого',\n",
       "   'lemma': 'невысокий',\n",
       "   'features': {'Case': 'Gen',\n",
       "    'Degree': 'Pos',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 17,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'роста',\n",
       "   'lemma': 'рост',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Gen',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 18,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'блондинка',\n",
       "   'lemma': 'блондинка',\n",
       "   'features': {'Animacy': 'Anim',\n",
       "    'Case': 'Gen',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 19, 'pos': 'PUNCT', 'text': ',', 'lemma': ',', 'features': {}},\n",
       "  {'tokenid': 20, 'pos': 'ADP', 'text': 'в', 'lemma': 'в', 'features': {}},\n",
       "  {'tokenid': 21,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'берете',\n",
       "   'lemma': 'берет',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Loc',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 22, 'pos': 'PUNCT', 'text': ';', 'lemma': ';', 'features': {}},\n",
       "  {'tokenid': 23, 'pos': 'ADP', 'text': 'за', 'lemma': 'за', 'features': {}},\n",
       "  {'tokenid': 24,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'нею',\n",
       "   'lemma': 'нею',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Acc',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Plur'}},\n",
       "  {'tokenid': 25,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'бежал',\n",
       "   'lemma': 'бежать',\n",
       "   'features': {'Aspect': 'Imp',\n",
       "    'Gender': 'Masc',\n",
       "    'Mood': 'Ind',\n",
       "    'Number': 'Sing',\n",
       "    'Tense': 'Past',\n",
       "    'VerbForm': 'Fin',\n",
       "    'Voice': 'Act'}},\n",
       "  {'tokenid': 26,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'белый',\n",
       "   'lemma': 'белый',\n",
       "   'features': {'Case': 'Nom',\n",
       "    'Degree': 'Pos',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 27,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'шпиц',\n",
       "   'lemma': 'шпиц',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Nom',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing'}},\n",
       "  {'tokenid': 28, 'pos': 'PUNCT', 'text': '.', 'lemma': '.', 'features': {}}],\n",
       " 'tgttokens': [{'tokenid': 0,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'Sitting',\n",
       "   'lemma': 'sit',\n",
       "   'features': {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}},\n",
       "  {'tokenid': 1, 'pos': 'ADP', 'text': 'in', 'lemma': 'in', 'features': {}},\n",
       "  {'tokenid': 2,\n",
       "   'pos': 'DET',\n",
       "   'text': 'a',\n",
       "   'lemma': 'a',\n",
       "   'features': {'Definite': 'Ind', 'PronType': 'Art'}},\n",
       "  {'tokenid': 3,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'pavilion',\n",
       "   'lemma': 'pavilion',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 4, 'pos': 'ADP', 'text': 'at', 'lemma': 'at', 'features': {}},\n",
       "  {'tokenid': 5,\n",
       "   'pos': 'PROPN',\n",
       "   'text': 'Vernet',\n",
       "   'lemma': 'Vernet',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 6, 'pos': 'PART', 'text': '’s', 'lemma': '’s', 'features': {}},\n",
       "  {'tokenid': 7,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ',',\n",
       "   'lemma': ',',\n",
       "   'features': {'PunctType': 'Comm'}},\n",
       "  {'tokenid': 8,\n",
       "   'pos': 'PRON',\n",
       "   'text': 'he',\n",
       "   'lemma': 'he',\n",
       "   'features': {'Case': 'Nom',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing',\n",
       "    'Person': '3',\n",
       "    'PronType': 'Prs'}},\n",
       "  {'tokenid': 9,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'saw',\n",
       "   'lemma': 'see',\n",
       "   'features': {'Tense': 'Past', 'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 10,\n",
       "   'pos': 'DET',\n",
       "   'text': 'a',\n",
       "   'lemma': 'a',\n",
       "   'features': {'Definite': 'Ind', 'PronType': 'Art'}},\n",
       "  {'tokenid': 11,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'young',\n",
       "   'lemma': 'young',\n",
       "   'features': {'Degree': 'Pos'}},\n",
       "  {'tokenid': 12,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'woman',\n",
       "   'lemma': 'woman',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 13,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ',',\n",
       "   'lemma': ',',\n",
       "   'features': {'PunctType': 'Comm'}},\n",
       "  {'tokenid': 14,\n",
       "   'pos': 'PART',\n",
       "   'text': 'not',\n",
       "   'lemma': 'not',\n",
       "   'features': {'Polarity': 'Neg'}},\n",
       "  {'tokenid': 15,\n",
       "   'pos': 'ADV',\n",
       "   'text': 'very',\n",
       "   'lemma': 'very',\n",
       "   'features': {}},\n",
       "  {'tokenid': 16,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'tall',\n",
       "   'lemma': 'tall',\n",
       "   'features': {'Degree': 'Pos'}},\n",
       "  {'tokenid': 17,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ',',\n",
       "   'lemma': ',',\n",
       "   'features': {'PunctType': 'Comm'}},\n",
       "  {'tokenid': 18,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'blond',\n",
       "   'lemma': 'blond',\n",
       "   'features': {'Degree': 'Pos'}},\n",
       "  {'tokenid': 19,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ',',\n",
       "   'lemma': ',',\n",
       "   'features': {'PunctType': 'Comm'}},\n",
       "  {'tokenid': 20, 'pos': 'ADP', 'text': 'in', 'lemma': 'in', 'features': {}},\n",
       "  {'tokenid': 21,\n",
       "   'pos': 'DET',\n",
       "   'text': 'a',\n",
       "   'lemma': 'a',\n",
       "   'features': {'Definite': 'Ind', 'PronType': 'Art'}},\n",
       "  {'tokenid': 22,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'beret',\n",
       "   'lemma': 'beret',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 23,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ',',\n",
       "   'lemma': ',',\n",
       "   'features': {'PunctType': 'Comm'}},\n",
       "  {'tokenid': 24,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'walking',\n",
       "   'lemma': 'walk',\n",
       "   'features': {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}},\n",
       "  {'tokenid': 25,\n",
       "   'pos': 'ADP',\n",
       "   'text': 'along',\n",
       "   'lemma': 'along',\n",
       "   'features': {}},\n",
       "  {'tokenid': 26,\n",
       "   'pos': 'DET',\n",
       "   'text': 'the',\n",
       "   'lemma': 'the',\n",
       "   'features': {'Definite': 'Def', 'PronType': 'Art'}},\n",
       "  {'tokenid': 27,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'embankment',\n",
       "   'lemma': 'embankment',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 28, 'pos': 'PUNCT', 'text': ';', 'lemma': ';', 'features': {}},\n",
       "  {'tokenid': 29,\n",
       "   'pos': 'ADP',\n",
       "   'text': 'behind',\n",
       "   'lemma': 'behind',\n",
       "   'features': {}},\n",
       "  {'tokenid': 30,\n",
       "   'pos': 'PRON',\n",
       "   'text': 'her',\n",
       "   'lemma': 'she',\n",
       "   'features': {'Case': 'Acc',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Sing',\n",
       "    'Person': '3',\n",
       "    'PronType': 'Prs'}},\n",
       "  {'tokenid': 31,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'ran',\n",
       "   'lemma': 'run',\n",
       "   'features': {'Tense': 'Past', 'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 32,\n",
       "   'pos': 'DET',\n",
       "   'text': 'a',\n",
       "   'lemma': 'a',\n",
       "   'features': {'Definite': 'Ind', 'PronType': 'Art'}},\n",
       "  {'tokenid': 33,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'white',\n",
       "   'lemma': 'white',\n",
       "   'features': {'Degree': 'Pos'}},\n",
       "  {'tokenid': 34,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'spitz',\n",
       "   'lemma': 'spitz',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 35,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '.',\n",
       "   'lemma': '.',\n",
       "   'features': {'PunctType': 'Peri'}}],\n",
       " 'srcsentidx': 3,\n",
       " 'tgtsentidx': 3}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordAlignmentList[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'jsondata/{srclang}/wordAlignment4-18.json', 'w',encoding='utf-8') as f:\n",
    "    json.dump(wordAlignmentList, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump Sentence Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'jsondata/{srclang}/srcLineBreaks.txt','r') as f:\n",
    "    file = f.read().split('\\n')\n",
    "    srcLineBreaks = [(int(l.split(',')[0]), int(l.split(',')[1])) for l in file]\n",
    "with open(f'jsondata/{srclang}/tgtLineBreaks.txt','r') as f:\n",
    "    file = f.read().split('\\n')\n",
    "    tgtLineBreaks = [(int(l.split(',')[0]), int(l.split(',')[1])) for l in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 7), (3, 28), (5, 30)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcLineBreaks[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [7]\n",
      "3 [28]\n",
      "5 [30]\n",
      "6 [24]\n",
      "11 [35]\n",
      "11 [35, 39]\n",
      "14 [44]\n",
      "16 [28]\n",
      "19 [86]\n",
      "22 [3]\n",
      "23 [9]\n",
      "24 [10]\n",
      "26 [19]\n",
      "27 [3]\n",
      "28 [8]\n",
      "29 [2]\n",
      "30 [20]\n",
      "34 [7]\n",
      "42 [10]\n",
      "46 [12]\n",
      "47 [19]\n",
      "48 [8]\n",
      "53 [3]\n",
      "56 [24]\n",
      "59 [27]\n",
      "61 [12]\n",
      "64 [7]\n",
      "65 [4]\n",
      "66 [34]\n",
      "66 [34, 44]\n",
      "67 [4]\n",
      "70 [152]\n",
      "73 [26]\n",
      "75 [8]\n",
      "78 [9]\n",
      "79 [39]\n",
      "81 [8]\n",
      "83 [3]\n",
      "84 [4]\n",
      "96 [25]\n",
      "97 [38]\n",
      "98 [15]\n",
      "99 [11]\n",
      "103 [12]\n",
      "104 [8]\n",
      "105 [31]\n",
      "106 [43]\n",
      "107 [6]\n",
      "109 [4]\n",
      "110 [16]\n",
      "116 [73]\n",
      "119 [16]\n",
      "120 [11]\n",
      "122 [2]\n",
      "123 [4]\n",
      "128 [30]\n",
      "131 [3]\n",
      "133 [4]\n",
      "136 [15]\n",
      "138 [2]\n",
      "139 [16]\n",
      "144 [5]\n",
      "149 [30]\n",
      "152 [4]\n",
      "153 [7]\n",
      "157 [34]\n",
      "161 [9]\n",
      "170 [14]\n",
      "177 [28]\n",
      "177 [28, 39]\n",
      "178 [18]\n",
      "179 [12]\n",
      "179 [12, 16]\n",
      "180 [2]\n",
      "181 [12]\n",
      "186 [51]\n",
      "189 [16]\n",
      "192 [14]\n",
      "195 [5]\n",
      "197 [13]\n",
      "198 [19]\n",
      "207 [23]\n",
      "209 [24]\n",
      "212 [8]\n",
      "213 [18]\n",
      "215 [5]\n",
      "217 [8]\n",
      "218 [16]\n",
      "221 [14]\n",
      "224 [3]\n",
      "227 [43]\n",
      "229 [17]\n",
      "229 [17, 20]\n",
      "235 [10]\n",
      "236 [9]\n",
      "238 [7]\n",
      "239 [17]\n",
      "243 [3]\n",
      "245 [5]\n",
      "246 [23]\n",
      "249 [13]\n",
      "250 [33]\n",
      "255 [2]\n",
      "256 [8]\n",
      "264 [9]\n",
      "266 [19]\n",
      "267 [8]\n",
      "271 [13]\n",
      "274 [4]\n",
      "276 [18]\n",
      "277 [9]\n",
      "283 [26]\n",
      "287 [13]\n",
      "289 [3]\n",
      "291 [2]\n",
      "293 [8]\n",
      "294 [19]\n",
      "297 [5]\n",
      "298 [8]\n",
      "300 [36]\n",
      "301 [23]\n",
      "307 [48]\n",
      "309 [25]\n",
      "310 [25]\n",
      "312 [26]\n",
      "313 [37]\n",
      "315 [8]\n",
      "317 [6]\n",
      "319 [2]\n"
     ]
    }
   ],
   "source": [
    "srctokens = []\n",
    "tgttokens = []\n",
    "for i, srcsent in enumerate(rawsrcsents):\n",
    "    srcdoc = sourceNLP(srcsent)\n",
    "    \n",
    "    tokenswithlinebreak = []\n",
    "    for sentidx, tokenidx in srcLineBreaks:\n",
    "        if sentidx == i:\n",
    "            tokenswithlinebreak.append(tokenidx)\n",
    "            print(sentidx, tokenswithlinebreak)\n",
    "    \n",
    "    senttokens = [{\n",
    "            'tokenid' : t,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph)),\n",
    "            'linebreak' : t in tokenswithlinebreak\n",
    "        } for t, token in enumerate(srcdoc)]\n",
    "    srctokens.append({\n",
    "        'text' : srcsent,\n",
    "        'tokens' : senttokens\n",
    "    })\n",
    "for i, tgtsent in enumerate(rawtgtsents):\n",
    "    tgtdoc = engNLP(tgtsent)\n",
    "    \n",
    "    tokenswithlinebreak = []\n",
    "    for sentidx, tokenidx in tgtLineBreaks:\n",
    "        if sentidx == i:\n",
    "            tokenswithlinebreak.append(tokenidx)\n",
    "    \n",
    "    senttokens = [{\n",
    "            'tokenid' : t,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph)),\n",
    "            'linebreak' : t in tokenswithlinebreak\n",
    "        } for t, token in enumerate(tgtdoc)]\n",
    "    tgttokens.append({\n",
    "        'text' : tgtsent,\n",
    "        'tokens' : senttokens\n",
    "    })\n",
    "\n",
    "sentsInOrderJSON = {'srcSentsInOrder' : srctokens, 'tgtSentsInOrder' : tgttokens}\n",
    "with open(f'jsondata/{srclang}/sentsInOrder4-18.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentsInOrderJSON, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Часть I (Дама с собачкой).',\n",
       " 'tokens': [{'tokenid': 0,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'Часть',\n",
       "   'lemma': 'часть',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Nom',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Sing'},\n",
       "   'linebreak': False},\n",
       "  {'tokenid': 1,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'I',\n",
       "   'lemma': 'i',\n",
       "   'features': {},\n",
       "   'linebreak': False},\n",
       "  {'tokenid': 2,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '(',\n",
       "   'lemma': '(',\n",
       "   'features': {},\n",
       "   'linebreak': False},\n",
       "  {'tokenid': 3,\n",
       "   'pos': 'PROPN',\n",
       "   'text': 'Дама',\n",
       "   'lemma': 'дама',\n",
       "   'features': {'Animacy': 'Anim',\n",
       "    'Case': 'Nom',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Sing'},\n",
       "   'linebreak': False},\n",
       "  {'tokenid': 4,\n",
       "   'pos': 'ADP',\n",
       "   'text': 'с',\n",
       "   'lemma': 'с',\n",
       "   'features': {},\n",
       "   'linebreak': False},\n",
       "  {'tokenid': 5,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'собачкой',\n",
       "   'lemma': 'собачка',\n",
       "   'features': {'Animacy': 'Inan',\n",
       "    'Case': 'Ins',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Sing'},\n",
       "   'linebreak': False},\n",
       "  {'tokenid': 6,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ')',\n",
       "   'lemma': ')',\n",
       "   'features': {},\n",
       "   'linebreak': False},\n",
       "  {'tokenid': 7,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '.',\n",
       "   'lemma': '.',\n",
       "   'features': {},\n",
       "   'linebreak': True}]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srctokens[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract paragraph breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the module\n",
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "with open(f'jsondata/{srclang}/sentsInOrder.json') as json_file:\n",
    "    sentsInOrder = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcLineBreaks = []\n",
    "for i, s in enumerate(sentsInOrder['srcSentsInOrder']):\n",
    "    for tid, t in enumerate(s['tokens']):\n",
    "        islinebreak = t['linebreak']\n",
    "        j = tid\n",
    "        if islinebreak:\n",
    "            srcLineBreaks.append(str(i) +','+str(j))\n",
    "with open(f'jsondata/{srclang}/srcLineBreaks.txt','w') as f:\n",
    "    f.write('\\n'.join(srcLineBreaks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgtLineBreaks = []\n",
    "for i, s in enumerate(sentsInOrder['tgtSentsInOrder']):\n",
    "    for tid, t in enumerate(s['tokens']):\n",
    "        islinebreak = t['linebreak']\n",
    "        j = tid\n",
    "        if islinebreak:\n",
    "            tgtLineBreaks.append(str(i) +','+str(j))\n",
    "with open(f'jsondata/{srclang}/tgtLineBreaks.txt','w') as f:\n",
    "    f.write('\\n'.join(tgtLineBreaks))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
