{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download ru_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.morphology import Morphology\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIFY SOURCE LANGUAGE\n",
    "srclang = 'Russian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy lang models\n",
    "\n",
    "if srclang == 'Spanish':\n",
    "    sourceNLP = spacy.load(\"es_core_news_sm\")\n",
    "elif srclang == 'Russian':\n",
    "    sourceNLP = spacy.load(\"ru_core_news_sm\")\n",
    "    \n",
    "engNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'texts/{srclang}/rawsource.txt','r') as f:\n",
    "     sourcetxt = f.read().replace('\\n',' ')\n",
    "with open(f'texts/{srclang}/rawtarget.txt','r') as f:\n",
    "     targettxt = f.read().replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedoc = sourceNLP(sourcetxt)\n",
    "targetdoc = engNLP(targettxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentenize\n",
    "rawsrcsents = []\n",
    "rawtgtsents = []\n",
    "for sent in sourcedoc.sents:\n",
    "    rawsrcsents.append(sent.text)\n",
    "for sent in targetdoc.sents:\n",
    "    rawtgtsents.append(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write standardized files (one line per sentence) for input to Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the inputs to bleualign\n",
    "with open('sourcetextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawsrcsents))\n",
    "with open('targettextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawtgtsents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized sentences for data output\n",
    "srctokens = []\n",
    "for srcsent in rawsrcsents:\n",
    "    tokens = sourceNLP(srcsent)\n",
    "    srctokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])\n",
    "tgttokens = []\n",
    "for tgtsent in rawtgtsents:\n",
    "    tokens = engNLP(tgtsent)\n",
    "    tgttokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, I also generate sourcetexttranslation.text using Google Translate and run Bleualign on the texts on my terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install translators --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using United States server backend.\n"
     ]
    }
   ],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/317 sents translated.\n",
      "1/317 sents translated.\n",
      "2/317 sents translated.\n",
      "3/317 sents translated.\n",
      "4/317 sents translated.\n",
      "5/317 sents translated.\n",
      "6/317 sents translated.\n",
      "7/317 sents translated.\n",
      "8/317 sents translated.\n",
      "9/317 sents translated.\n",
      "10/317 sents translated.\n",
      "11/317 sents translated.\n",
      "12/317 sents translated.\n",
      "13/317 sents translated.\n",
      "14/317 sents translated.\n",
      "15/317 sents translated.\n",
      "16/317 sents translated.\n",
      "17/317 sents translated.\n",
      "18/317 sents translated.\n",
      "19/317 sents translated.\n",
      "20/317 sents translated.\n",
      "21/317 sents translated.\n",
      "22/317 sents translated.\n",
      "23/317 sents translated.\n",
      "24/317 sents translated.\n",
      "25/317 sents translated.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "translatedsourcesents = []\n",
    "for i, sent in enumerate(rawsrcsents):\n",
    "    print(f'{i}/{len(rawsrcsents)} sents translated.')\n",
    "    try:\n",
    "        translatedsourcesents.append(ts.google(sent, to_language = 'en'))\n",
    "    except:\n",
    "        print('problem on',sent)\n",
    "end = time.time()\n",
    "print(f'machine translation took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translatedsource.txt','w') as f:\n",
    "    f.write('\\n'.join(translatedsourcesents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: permission denied: ./bleualign.py\n",
      "sentence alignment took 0.6176121234893799 seconds\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "start = time.time()\n",
    "!./bleualign.py -s sourcetextforbleualign.txt -t targettextforbleualign.txt --srctotarget translatedsource.txt -o outputfile --verbosity 1\n",
    "end = time.time()\n",
    "print(f'sentence alignment took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rawsrcsents[218], rawtgtsents[276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output = cap.stdout\n",
    "\n",
    "# indexpairs = []\n",
    "# split = output.split('finished with article')[0].split('alignment: ')[1:]\n",
    "# for string in split:\n",
    "#     string = string.replace('\\r\\n','')\n",
    "#     strindices = string.split(' - ')\n",
    "#     for srcidx in strindices[0].split(','):\n",
    "#         for tgtidx in strindices[1].split(','):\n",
    "#             if (int(srcidx) < len(rawsrcsents) - 1) and (int(tgtidx) < len(rawtgtsents) - 1):\n",
    "#                 indexpairs.append((int(srcidx)+1, int(tgtidx) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawsrcsents[4], rawtgtsents[5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [START HERE] 3. Read sentence-aligned files (from Bleualign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputfile-s','r') as f:\n",
    "    alignedsrc = f.read().split('\\n')\n",
    "with open('outputfile-t','r') as f:\n",
    "    alignedtgt = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Stephen Albert prosiguió: »-No creo que su ilustre antepasado jugara ociosamente a las variaciones.',\n",
       " 'Stephen Albert continued: \"I do not think that your illustrious ancestor toyed idly with variations.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(alignedsrc)))\n",
    "alignedsrc[i], alignedtgt[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alignedsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/223 sentences parsed.\n",
      "50/223 sentences parsed.\n",
      "100/223 sentences parsed.\n",
      "150/223 sentences parsed.\n",
      "200/223 sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "# sent to sent alignment\n",
    "oneLineSpa, oneLineEng = rawsrcsents, rawtgtsents\n",
    "alignedSpa, alignedEng = alignedsrc, alignedtgt\n",
    "sentAlignments = []\n",
    "alignmentLookup = dict()\n",
    "spaIndex = 0\n",
    "for alignSpaSent, alignEngSent in zip(alignedSpa, alignedEng):\n",
    "    if spaIndex % 50 == 0:\n",
    "        print(f'{spaIndex}/{len(rawsrcsents)} sentences parsed.')\n",
    "    individualEngSents = [sent.text for sent in engNLP(alignEngSent).sents]\n",
    "    for indEngSent in individualEngSents:\n",
    "        for i, thisEngLine in enumerate(oneLineEng):\n",
    "            if indEngSent.strip() == thisEngLine.strip():\n",
    "                engIndex = i\n",
    "        for j, thisSpaLine in enumerate(oneLineSpa):\n",
    "            if alignSpaSent.strip() == thisSpaLine.strip():\n",
    "                spaIndex = j\n",
    "        sentAlignments.append({\n",
    "            'indices' : (spaIndex, engIndex),\n",
    "            'sents' : (oneLineSpa[spaIndex], oneLineEng[engIndex])\n",
    "        })\n",
    "        alignmentLookup.setdefault(spaIndex,[])\n",
    "        alignmentLookup[spaIndex].append(engIndex)\n",
    "    spaIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[97, 98, 99]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alignmentLookup[81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jsondata/sentAlignment3-28.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentAlignments, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE DONT NEED - check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Por eso, yo la había aceptado con plenitud, sin prestarle atención.\n",
      "That was why I had accepted it fully, without paying it any attention.\n"
     ]
    }
   ],
   "source": [
    "# chec, k it works\n",
    "randSentAlign = random.choice(sentAlignments)\n",
    "s, t = randSentAlign['sents']\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parse word alignment using SimAlign (recommended: fast and high coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install simalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-04-03 20:40:46,911 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading word aligner tool took 7.278181076049805 seconds\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "start = time.time()\n",
    "# making an instance of our model.\n",
    "# You can specify the embedding model and all alignment settings in the constructor.\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")\n",
    "end = time.time()\n",
    "print(f'downloading word aligner tool took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate word alignment with SimAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/223 sentences parsed in 0.0007836818695068359 s.\n",
      "25/223 sentences parsed in 27.857510805130005 s.\n",
      "50/223 sentences parsed in 62.76290583610535 s.\n",
      "75/223 sentences parsed in 86.85836291313171 s.\n",
      "100/223 sentences parsed in 121.79177784919739 s.\n",
      "125/223 sentences parsed in 136.3282699584961 s.\n",
      "150/223 sentences parsed in 155.86679196357727 s.\n",
      "175/223 sentences parsed in 179.74194407463074 s.\n",
      "200/223 sentences parsed in 201.679790019989 s.\n",
      "parsed in 210.68713808059692 s\n"
     ]
    }
   ],
   "source": [
    "# get rid of white space at end\n",
    "your_data = zip(rawsrcsents, rawtgtsents)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "wordAlignmentList = []\n",
    "\n",
    "for i, srcsent in enumerate(rawsrcsents):\n",
    "    if i % 25 == 0:\n",
    "        currently = time.time()\n",
    "        print(f\"{i}/{len(rawsrcsents)} sentences parsed in {currently-start} s.\")\n",
    "\n",
    "    srcDoc = sourceNLP(srcsent)\n",
    "    \n",
    "    srcTokens = []\n",
    "    for token in srcDoc:\n",
    "        srcTokens.append({\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        jLst = alignmentLookup[i]\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    for j in jLst:\n",
    "        tgtDoc = engNLP(rawtgtsents[j])\n",
    "\n",
    "        tgtTokens = []\n",
    "        for token in tgtDoc:\n",
    "            tgtTokens.append({\n",
    "                'tokenid' : token.idx,\n",
    "                'pos' : token.pos_, \n",
    "                'text' : token.text, \n",
    "                'lemma' : token.lemma_,\n",
    "                'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "            })\n",
    "\n",
    "        src = [t.text for t in srcDoc]\n",
    "        tgt = [t.text for t in tgtDoc]\n",
    "\n",
    "        alignments = myaligner.get_word_aligns(src, tgt)\n",
    "        itermax = alignments['itermax']\n",
    "\n",
    "        wordAlignmentList.append({\n",
    "            'alignedwordindices' : itermax,\n",
    "            'alignedwords' : [(src[s], tgt[t]) for s, t in itermax],\n",
    "            'srctokens' : srcTokens,\n",
    "            'tgttokens' : tgtTokens,\n",
    "            'srcsentidx' : i,\n",
    "            'tgtsentidx' : j,\n",
    "        })\n",
    "end = time.time()\n",
    "print('parsed in',end-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -\n",
      "2 Note\n",
      "7 by\n",
      "10 the\n",
      "14 manuscript\n",
      "25 editor\n",
      "31 .\n"
     ]
    }
   ],
   "source": [
    "for token in tgtDoc:\n",
    "    print(token.idx, token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to JSON or CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jsondata/wordAlignment3-28.json', 'w',encoding='utf-8') as f:\n",
    "    json.dump(wordAlignmentList, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "srctokens = []\n",
    "tgttokens = []\n",
    "for srcsent in rawsrcsents:\n",
    "    srcdoc = sourceNLP(srcsent)\n",
    "    srctokens.append([{\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        } for token in srcdoc])\n",
    "for tgtsent in rawtgtsents:\n",
    "    tgtdoc = engNLP(tgtsent)\n",
    "    tgttokens.append([{\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        } for token in tgtdoc])\n",
    "\n",
    "sentsInOrderJSON = {'srcSentsInOrder' : {'text' : rawsrcsents, 'tokens' : srctokens}, 'tgtSentsInOrder' : {'text' : rawtgtsents, 'tokens' : tgttokens}}\n",
    "with open('jsondata/sentsInOrder3-28.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentsInOrderJSON, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Alignment (don't need in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokenid': 0, 'pos': 'PUNCT', 'text': '-', 'lemma': '-', 'features': {}},\n",
       " {'tokenid': 2,\n",
       "  'pos': 'VERB',\n",
       "  'text': 'Note',\n",
       "  'lemma': 'note',\n",
       "  'features': {'VerbForm': 'Inf'}},\n",
       " {'tokenid': 7, 'pos': 'ADP', 'text': 'by', 'lemma': 'by', 'features': {}},\n",
       " {'tokenid': 10,\n",
       "  'pos': 'DET',\n",
       "  'text': 'the',\n",
       "  'lemma': 'the',\n",
       "  'features': {'Definite': 'Def', 'PronType': 'Art'}},\n",
       " {'tokenid': 14,\n",
       "  'pos': 'NOUN',\n",
       "  'text': 'manuscript',\n",
       "  'lemma': 'manuscript',\n",
       "  'features': {'Number': 'Sing'}},\n",
       " {'tokenid': 25,\n",
       "  'pos': 'NOUN',\n",
       "  'text': 'editor',\n",
       "  'lemma': 'editor',\n",
       "  'features': {'Number': 'Sing'}},\n",
       " {'tokenid': 31,\n",
       "  'pos': 'PUNCT',\n",
       "  'text': '.',\n",
       "  'lemma': '.',\n",
       "  'features': {'PunctType': 'Peri'}}]"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgttokens[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (1, 0),\n",
    " #  (1, 1),\n",
    " #  (1, 12),\n",
    " #  (2, 3),\n",
    " #  (3, 4),\n",
    " #  (4, 5),\n",
    " #  (5, 7),\n",
    " #  (6, 8),\n",
    " #  (7, 15),\n",
    " #  (8, 10),\n",
    " #  (9, 11),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('borges_word_alignment_3-21.pickle', 'wb') as handle:\n",
    "#     pickle.dump(alignmentList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE DONT NEED: an example of simalign on a single pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwmf : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (4, 7), (5, 4), (6, 15), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 16), (16, 16), (17, 20), (18, 17), (19, 18), (20, 22), (21, 21), (21, 23), (22, 19), (22, 24), (23, 25)]\n",
      "inter : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (16, 16), (19, 18), (20, 22), (21, 23), (22, 24), (23, 25)]\n",
      "itermax : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (5, 4), (6, 7), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 16), (16, 16), (18, 19), (19, 18), (20, 22), (21, 23), (22, 24), (23, 25)]\n"
     ]
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcDoc = sourceNLP.tokenizer(srcsents[i])\n",
    "tgtDoc = engNLP.tokenizer(tgtsents[i])\n",
    "src = [t.text for t in srcDoc]\n",
    "tgt = [t.text for t in tgtDoc]\n",
    "alignments = myaligner.get_word_aligns(src, tgt)\n",
    "\n",
    "for match in alignments:\n",
    "    print(match, ':', alignments[match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "novelista he\n",
      "genial fine\n",
      ", ,\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "sin doubtless\n",
      "duda doubtless\n",
      "no more\n",
      "se ,\n",
      "consideró considered\n",
      "un a\n",
      "mero than\n",
      "mero mere\n",
      "novelista himself\n",
      "novelista novelist\n",
      ". .\n",
      "\n",
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "duda doubtless\n",
      "consideró considered\n",
      "un a\n",
      "mero mere\n",
      "novelista novelist\n",
      ". .\n",
      "\n",
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "genial fine\n",
      ", he\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "sin doubtless\n",
      "duda doubtless\n",
      "se himself\n",
      "consideró considered\n",
      "un a\n",
      "mero mere\n",
      "novelista novelist\n",
      ". .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match in alignments:\n",
    "    for s, t in alignments[match]:\n",
    "        print(src[s], tgt[t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get rid of white space at end\n",
    "# your_data = zip(srcsents[:-1], tgtsents[:-1])\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# alignmentList = []\n",
    "# t = 0\n",
    "\n",
    "# for sent_es_str, sent_en_str in your_data:\n",
    "#     if t % 25 == 0:\n",
    "#         currently = time.time()\n",
    "#         print(f'{t/{len(srcsents)} sentences parsed in {currently-start} s.')\n",
    "\n",
    "#     srcDoc = sourceNLP(sent_es_str)\n",
    "#     tgtDoc = engNLP(sent_en_str)\n",
    "    \n",
    "#     srcTokens = []\n",
    "#     for token in srcDoc:\n",
    "#         srcTokens.append({\n",
    "#             'tokenid' : token.idx,\n",
    "#             'pos' : token.pos_, \n",
    "#             'text' : token.text, \n",
    "#             'lemma' : token.lemma_,\n",
    "#             'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "#         })\n",
    "\n",
    "#     tgtTokens = []\n",
    "#     for token in tgtDoc:\n",
    "#         tgtTokens.append({\n",
    "#             'tokenid' : token.idx,\n",
    "#             'pos' : token.pos_, \n",
    "#             'text' : token.text, \n",
    "#             'lemma' : token.lemma_,\n",
    "#             'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "#         })\n",
    "\n",
    "#     src = [t.text for t in srcDoc]\n",
    "#     tgt = [t.text for t in tgtDoc]\n",
    "    \n",
    "#     alignments = myaligner.get_word_aligns(src, tgt)\n",
    "#     itermax = alignments['itermax']\n",
    "#     try:\n",
    "#         j = alignmentLookup[i]\n",
    "#     except:\n",
    "#         j = 'No Aligned Sentence'\n",
    "    \n",
    "    \n",
    "#     alignmentList.append({\n",
    "#         'alignedwordindices' : itermax,\n",
    "#         'alignedwords' : [(src[s], tgt[t]) for s, t in itermax],\n",
    "#         'srctokens' : srcTokens,\n",
    "#         'tgttokens' : tgtTokens,\n",
    "#         'srcsentidx' : i,\n",
    "#         'tgtsentidx' : j,\n",
    "#     })\n",
    "    \n",
    "#     t += 1\n",
    "# end = time.time()\n",
    "# print('parsed in',end-start,'s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
