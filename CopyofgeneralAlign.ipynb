{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy lang models\n",
    "\n",
    "spaNLP = spacy.load(\"es_core_news_sm\")\n",
    "engNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts/rawsource.txt','r') as f:\n",
    "     sourcetxt = f.read()\n",
    "with open('texts/rawtarget.txt','r') as f:\n",
    "     targettxt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedoc = spaNLP(sourcetxt)\n",
    "targetdoc = engNLP(targettxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentenize\n",
    "rawsrcsents = []\n",
    "rawtgtsents = []\n",
    "for sent in sourcedoc.sents:\n",
    "    rawsrcsents.append(sent.text)\n",
    "for sent in targetdoc.sents:\n",
    "    rawtgtsents.append(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write standardized files (one line per sentence) for input to Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the inputs to bleualign\n",
    "with open('sourcetextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawsrcsents))\n",
    "with open('targettextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawtgtsents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, I also generate sourcetexttranslation.text using Google Translate and run Bleualign on the texts on my terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install translators --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using United States server backend.\n"
     ]
    }
   ],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "translatedsourcesents = []\n",
    "for sent in rawsrcsents:\n",
    "    translatedsourcesents.append(ts.google(sent, to_language = 'en'))\n",
    "end = time.time()\n",
    "print(f'machine translation took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translatedsource.txt','w') as f:\n",
    "    f.write('\\n'.join(translatedsourcesents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in article 0: \n",
      "processing\n",
      "computing alignment between srctotarget (file 0) and target text\n",
      "Evaluating sentences with bleu\n",
      "finished\n",
      "searching for longest path of good alignments\n",
      "finished\n",
      "filling gaps\n",
      "finished\n",
      "\n",
      "finished with article\n",
      "\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!./bleualign.py -s sourcetextforbleualign.txt -t targettextforbleualign.txt --srctotarget translatedsource.txt -o outputfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [START HERE] 3. Read sentence-aligned files (from Bleualign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputfile-s','r') as f:\n",
    "    srcsents = f.read().split('\\n')\n",
    "with open('outputfile-t','r') as f:\n",
    "    tgtsents = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Madden era implacable.', 'Madden was implacable.')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcsents[i], tgtsents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/212 sentences parsed.\n",
      "50/212 sentences parsed.\n",
      "100/212 sentences parsed.\n",
      "150/212 sentences parsed.\n",
      "200/212 sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "# sent to sent alignment\n",
    "oneLineSpa, oneLineEng = rawsrcsents, rawtgtsents\n",
    "alignedSpa, alignedEng = srcsents, tgtsents\n",
    "sentAlignments = []\n",
    "spaIndex = 0\n",
    "for alignSpaSent, alignEngSent in zip(alignedSpa, alignedEng):\n",
    "    if spaIndex % 50 == 0:\n",
    "        print(f'{spaIndex}/{len(srcsents)} sentences parsed.')\n",
    "    individualEngSents = [sent.text for sent in engNLP(alignEngSent).sents]\n",
    "    for indEngSent in individualEngSents:\n",
    "        for i, thisEngLine in enumerate(oneLineEng):\n",
    "            if indEngSent.strip() == thisEngLine.strip():\n",
    "                engIndex = i\n",
    "        for j, thisSpaLine in enumerate(oneLineSpa):\n",
    "            if alignSpaSent.strip() == thisSpaLine.strip():\n",
    "                spaIndex = j\n",
    "        sentAlignments.append({\n",
    "            'indices' : (spaIndex, engIndex),\n",
    "            'sents' : (oneLineSpa[spaIndex], oneLineEng[engIndex])\n",
    "        })\n",
    "    spaIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me pareció que el húmedo jardín que rodeaba la casa estaba saturado hasta lo infinito de invisibles personas.\n",
      "It seemed to me that the dew-damp garden surrounding the house was infinitely saturated with invisible people.\n"
     ]
    }
   ],
   "source": [
    "# chec, k it works\n",
    "randSentAlign = random.choice(sentAlignments)\n",
    "s, t = randSentAlign['sents']\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('borges_sent_alignment_3-21.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentAlignments, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parse word alignment using SimAlign (recommended: fast and high coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install simalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-03-21 20:27:54,604 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "\n",
    "# making an instance of our model.\n",
    "# You can specify the embedding model and all alignment settings in the constructor.\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # The source and target sentences should be tokenized to words.\n",
    "# src_sentence = [\"This\", \"is\", \"a\", \"test\", \".\"]\n",
    "# trg_sentence = [\"Das\", \"ist\", \"ein\", \"Test\", \".\"]\n",
    "\n",
    "# # The output is a dictionary with different matching methods.\n",
    "# # Each method has a list of pairs indicating the indexes of aligned words (The alignments are zero-indexed).\n",
    "# alignments = myaligner.get_word_aligns(src_sentence, trg_sentence)\n",
    "\n",
    "# for matching_method in alignments:\n",
    "#     print(matching_method, \":\", alignments[matching_method])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate word alignment with SimAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/212 sentences parsed.\n",
      "10/212 sentences parsed.\n",
      "20/212 sentences parsed.\n",
      "30/212 sentences parsed.\n",
      "40/212 sentences parsed.\n",
      "50/212 sentences parsed.\n",
      "60/212 sentences parsed.\n",
      "70/212 sentences parsed.\n",
      "80/212 sentences parsed.\n",
      "90/212 sentences parsed.\n",
      "100/212 sentences parsed.\n",
      "110/212 sentences parsed.\n",
      "120/212 sentences parsed.\n",
      "130/212 sentences parsed.\n",
      "140/212 sentences parsed.\n",
      "150/212 sentences parsed.\n",
      "160/212 sentences parsed.\n",
      "170/212 sentences parsed.\n",
      "180/212 sentences parsed.\n",
      "190/212 sentences parsed.\n",
      "200/212 sentences parsed.\n",
      "210/212 sentences parsed.\n",
      "parsed in 295.08400797843933 s\n"
     ]
    }
   ],
   "source": [
    "your_data = zip(srcsents[:-1], tgtsents[:-1])\n",
    "start = time.time()\n",
    "\n",
    "alignmentList = []\n",
    "i = 0\n",
    "for sent_es_str, sent_en_str in your_data:\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i}/{len(srcsents)} sentences parsed.')\n",
    "    srcDoc = spaNLP.tokenizer(sent_es_str)\n",
    "    tgtDoc = engNLP.tokenizer(sent_en_str)\n",
    "    \n",
    "    srcTokens = []\n",
    "    for idx, token in enumerate(srcDoc):\n",
    "        srcTokens.append({\n",
    "            'tokenid' : idx\n",
    "            'POS' : token.upos, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma,\n",
    "            'features' : token.feats\n",
    "        })\n",
    "\n",
    "    src = [t.text for t in srcDoc]\n",
    "    tgt = [t.text for t in tgtDoc]\n",
    "    alignments = myaligner.get_word_aligns(src, tgt)\n",
    "    itermax = alignments['itermax']\n",
    "    \n",
    "    \n",
    "    alignmentList.append({\n",
    "        'indices' : itermax,\n",
    "        'words' : [(src[s], tgt[t]) for s, t in itermax]\n",
    "\n",
    "    })\n",
    "    \n",
    "    i += 1\n",
    "end = time.time()\n",
    "print('parsed in',end-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices': [(0, 6),\n",
       "  (1, 3),\n",
       "  (2, 0),\n",
       "  (2, 4),\n",
       "  (3, 2),\n",
       "  (5, 8),\n",
       "  (6, 9),\n",
       "  (7, 10),\n",
       "  (8, 19),\n",
       "  (9, 20),\n",
       "  (10, 22),\n",
       "  (11, 7),\n",
       "  (11, 18),\n",
       "  (12, 11),\n",
       "  (13, 12),\n",
       "  (14, 13),\n",
       "  (15, 14),\n",
       "  (16, 16),\n",
       "  (17, 23),\n",
       "  (18, 24),\n",
       "  (19, 25),\n",
       "  (19, 26),\n",
       "  (20, 27),\n",
       "  (22, 28),\n",
       "  (23, 29),\n",
       "  (23, 30),\n",
       "  (23, 41),\n",
       "  (24, 31),\n",
       "  (24, 32),\n",
       "  (25, 33),\n",
       "  (26, 34),\n",
       "  (27, 35),\n",
       "  (28, 36),\n",
       "  (29, 37),\n",
       "  (30, 38),\n",
       "  (31, 39),\n",
       "  (32, 41),\n",
       "  (35, 40),\n",
       "  (36, 43),\n",
       "  (37, 42)],\n",
       " 'words': [('Leí', 'I'),\n",
       "  ('con', 'without'),\n",
       "  ('incomprensión', 'Eagerly'),\n",
       "  ('incomprensión', 'understanding'),\n",
       "  ('y', 'but'),\n",
       "  ('estas', 'the'),\n",
       "  ('palabras', 'words'),\n",
       "  ('que', 'which'),\n",
       "  ('con', 'with'),\n",
       "  ('minucioso', 'a'),\n",
       "  ('pincel', 'brush'),\n",
       "  ('redactó', 'read'),\n",
       "  ('redactó', 'written'),\n",
       "  ('un', 'a'),\n",
       "  ('hombre', 'man'),\n",
       "  ('de', 'of'),\n",
       "  ('mi', 'my'),\n",
       "  ('sangre', 'blood'),\n",
       "  (':', ':'),\n",
       "  ('\"', '\"'),\n",
       "  ('Dejo', 'I'),\n",
       "  ('Dejo', 'leave'),\n",
       "  ('a', 'to'),\n",
       "  ('varios', 'various'),\n",
       "  ('porvenires', 'future'),\n",
       "  ('porvenires', 'times'),\n",
       "  ('porvenires', 'paths'),\n",
       "  ('(', ','),\n",
       "  ('(', 'but'),\n",
       "  ('no', 'not'),\n",
       "  ('a', 'to'),\n",
       "  ('todos', 'all'),\n",
       "  (')', ','),\n",
       "  ('mi', 'my'),\n",
       "  ('jardín', 'garden'),\n",
       "  ('de', 'of'),\n",
       "  ('senderos', 'paths'),\n",
       "  ('bifurcan', 'forking'),\n",
       "  ('\"', '\"'),\n",
       "  ('.', '.')]}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(alignmentList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import to pickle (bytestream) for later access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('borges_word_alignment_3-21.pickle', 'wb') as handle:\n",
    "    pickle.dump(alignmentList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an example of simalign on a single pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwmf : [(0, 0), (1, 2), (1, 3), (2, 4), (3, 5), (4, 6), (5, 1), (5, 7), (7, 8), (8, 9), (9, 10), (9, 12), (10, 21), (11, 11), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (17, 19), (18, 20), (19, 21), (20, 22), (21, 23), (22, 24), (22, 25), (23, 26), (24, 27), (25, 28)]\n",
      "inter : [(3, 5), (4, 6), (5, 7), (7, 8), (8, 9), (9, 10), (10, 11), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (17, 19), (18, 20), (19, 21), (20, 22), (21, 23), (22, 24), (22, 25), (23, 26), (24, 27), (25, 28)]\n",
      "itermax : [(0, 0), (3, 5), (4, 6), (5, 1), (5, 7), (7, 8), (8, 9), (9, 10), (9, 12), (10, 11), (12, 13), (13, 14), (14, 15), (15, 16), (16, 17), (17, 18), (17, 19), (18, 20), (19, 21), (20, 22), (21, 23), (22, 24), (22, 25), (23, 26), (24, 27), (25, 28)]\n"
     ]
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcDoc = spaNLP.tokenizer(srcsents[i])\n",
    "tgtDoc = engNLP.tokenizer(tgtsents[i])\n",
    "src = [t.text for t in srcDoc]\n",
    "tgt = [t.text for t in tgtDoc]\n",
    "alignments = myaligner.get_word_aligns(src, tgt)\n",
    "\n",
    "for match in alignments:\n",
    "    print(match, ':', alignments[match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lo I\n",
      "hice out\n",
      "hice my\n",
      ", plan\n",
      "porque because\n",
      "yo I\n",
      "sentía carried\n",
      "sentía felt\n",
      "el the\n",
      "jefe Chief\n",
      "temía had\n",
      "temía fear\n",
      "un uncountable\n",
      "poco some\n",
      "a of\n",
      "los those\n",
      "de of\n",
      "mi my\n",
      "raza race\n",
      "-a ,\n",
      "-a of\n",
      "los those\n",
      "innumerables uncountable\n",
      "antepasados forebears\n",
      "que whose\n",
      "confluyen culmination\n",
      "confluyen lies\n",
      "en in\n",
      "mí- me\n",
      ". .\n",
      "\n",
      "porque because\n",
      "yo I\n",
      "sentía felt\n",
      "el the\n",
      "jefe Chief\n",
      "temía had\n",
      "un some\n",
      "a of\n",
      "los those\n",
      "de of\n",
      "mi my\n",
      "raza race\n",
      "-a ,\n",
      "-a of\n",
      "los those\n",
      "innumerables uncountable\n",
      "antepasados forebears\n",
      "que whose\n",
      "confluyen culmination\n",
      "confluyen lies\n",
      "en in\n",
      "mí- me\n",
      ". .\n",
      "\n",
      "Lo I\n",
      "porque because\n",
      "yo I\n",
      "sentía carried\n",
      "sentía felt\n",
      "el the\n",
      "jefe Chief\n",
      "temía had\n",
      "temía fear\n",
      "un some\n",
      "a of\n",
      "los those\n",
      "de of\n",
      "mi my\n",
      "raza race\n",
      "-a ,\n",
      "-a of\n",
      "los those\n",
      "innumerables uncountable\n",
      "antepasados forebears\n",
      "que whose\n",
      "confluyen culmination\n",
      "confluyen lies\n",
      "en in\n",
      "mí- me\n",
      ". .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match in alignments:\n",
    "    for s, t in alignments[match]:\n",
    "        print(src[s], tgt[t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Parse aligned sentences using Astred word alignment (backup, not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install astred[stanza]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/BramVanroy/awesome-align.git@astred_compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from astred import AlignedSentences, Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Astred Word Alignment (https://github.com/BramVanroy/astred/issues/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astred.aligned import AlignedSentences, Sentence\n",
    "from astred.aligner import Aligner\n",
    "from astred.utils import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 08:06:19 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-02-26 08:06:19 INFO: Use device: cpu\n",
      "2022-02-26 08:06:19 INFO: Loading: tokenize\n",
      "2022-02-26 08:06:19 INFO: Loading: pos\n",
      "2022-02-26 08:06:21 INFO: Loading: lemma\n",
      "2022-02-26 08:06:21 INFO: Loading: depparse\n",
      "2022-02-26 08:06:23 INFO: Done loading processors!\n",
      "2022-02-26 08:07:20 WARNING: Language es package default expects mwt, which has been added\n",
      "2022-02-26 08:07:20 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-02-26 08:07:20 INFO: Use device: cpu\n",
      "2022-02-26 08:07:20 INFO: Loading: tokenize\n",
      "2022-02-26 08:07:20 INFO: Loading: mwt\n",
      "2022-02-26 08:07:20 INFO: Loading: pos\n",
      "2022-02-26 08:07:22 INFO: Loading: lemma\n",
      "2022-02-26 08:07:22 INFO: Loading: depparse\n",
      "2022-02-26 08:07:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/219 sentences parsed.\n",
      "10/219 sentences parsed.\n",
      "20/219 sentences parsed.\n",
      "30/219 sentences parsed.\n",
      "40/219 sentences parsed.\n",
      "50/219 sentences parsed.\n",
      "60/219 sentences parsed.\n",
      "70/219 sentences parsed.\n",
      "80/219 sentences parsed.\n",
      "90/219 sentences parsed.\n",
      "100/219 sentences parsed.\n",
      "110/219 sentences parsed.\n",
      "120/219 sentences parsed.\n",
      "130/219 sentences parsed.\n",
      "140/219 sentences parsed.\n",
      "150/219 sentences parsed.\n",
      "160/219 sentences parsed.\n",
      "170/219 sentences parsed.\n",
      "180/219 sentences parsed.\n",
      "190/219 sentences parsed.\n",
      "200/219 sentences parsed.\n",
      "210/219 sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "nlp_en = load_parser(\"en\", \"stanza\", is_tokenized=False, verbose=True)\n",
    "nlp_es = load_parser(\"es\", \"stanza\", is_tokenized=False, verbose=True)\n",
    "aligner = Aligner()\n",
    "\n",
    "your_data = zip(srcsents, tgtsents)\n",
    "\n",
    "alignmentList = []\n",
    "i = 0\n",
    "success = 0\n",
    "for sent_es_str, sent_en_str in your_data:\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i}/{len(srcsents)} sentences parsed.')\n",
    "    try:\n",
    "#         joinedsents = sent_en_str.replace('.',';')\n",
    "#         if joinedsents[-1] == ';':\n",
    "#             joinedsents = joinedsents[:-1] + '.'\n",
    "#         sent_en = Sentence.from_text(joinedsents, nlp_en)\n",
    "        sent_en = Sentence.from_text(sent_en_str, nlp_en)\n",
    "        sent_es = Sentence.from_text(sent_es_str, nlp_es)\n",
    "        aligned = AlignedSentences(sent_es, sent_en, aligner=aligner)\n",
    "        alignmentList.append({\n",
    "            'spanish_text' : sent_es_str,\n",
    "            'english_text' : sent_en_str,\n",
    "            'spanish_nlp' : sent_es,\n",
    "            'english_nlp' : sent_en,\n",
    "            'alignment' : aligned\n",
    "        })\n",
    "        success += 1\n",
    "    except:\n",
    "        alignmentList.append({\n",
    "            'spanish_text' : sent_es_str,\n",
    "            'english_text' : sent_en_str,\n",
    "            'spanish_nlp' : 'Error',\n",
    "            'english_nlp' : 'Error',\n",
    "            'alignment' : 'Error'\n",
    "        })\n",
    "    i += 1\n",
    "#     if i == 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alignment done! View alignments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate of word alignment on sentences is 164/219 sents or 0.7488584474885844\n"
     ]
    }
   ],
   "source": [
    "print(f'success rate of word alignment on sentences is {success}/{i} sents or {success/i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence alignment AND word alignment (more importantly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t\t\n",
      " \t\tas\n",
      " \t\tas\n",
      " \t\tpossible\n",
      "Aniquilado , trémulo , me encogí en la otra punta de el sillón , \t\tShattered , trembling , I huddled in the distant corner of the seat ,\n",
      "lejos \t\tfar\n",
      "de el temido cristal . \t\tfrom the fearful window .\n",
      "\n",
      "\n",
      "[[NULL]] \t\t[[NULL]]\n",
      "[[NULL]] \t\tas\n",
      "[[NULL]] \t\tas\n",
      "[[NULL]] \t\tpossible\n",
      "Aniquilado \t\tShattered\n",
      ", \t\t,\n",
      "trémulo \t\ttrembling\n",
      ", \t\t,\n",
      "me \t\tI\n",
      "encogí \t\thuddled\n",
      "en \t\tin\n",
      "la \t\tthe\n",
      "otra \t\tdistant\n",
      "punta \t\tcorner\n",
      "de \t\tof\n",
      "el \t\tthe\n",
      "sillón \t\tseat\n",
      ", \t\t,\n",
      "lejos \t\tfar\n",
      "de \t\tfrom\n",
      "el \t\tthe\n",
      "temido \t\tfearful\n",
      "cristal \t\twindow\n",
      ". \t\t.\n"
     ]
    }
   ],
   "source": [
    "a = random.choice(alignmentList)\n",
    "for pair in a['alignment'].aligned_seq_spans:\n",
    "    print(pair[0].text, '\\t\\t'+pair[1].text)\n",
    "print()\n",
    "print()\n",
    "for pair in a['alignment'].aligned_words:\n",
    "    print(pair[0].text, '\\t\\t'+pair[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write something not completely huge to the Pickle\n",
    "pickleList = []\n",
    "for a in alignmentList:\n",
    "    if a['alignment'] != 'Error': \n",
    "        spanishTokens = [{\n",
    "            'tokenid' : token.id, \n",
    "            'POS' : token.upos, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma,\n",
    "            'features' : token.feats\n",
    "        } for token in a['spanish_nlp']]\n",
    "        englishTokens = [{\n",
    "            'tokenid' : token.id, \n",
    "            'POS' : token.upos, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma,\n",
    "            'features' : token.feats\n",
    "        } for token in a['english_nlp']]\n",
    "        alignmentTuples = [(pair[0].id, pair[1].id) for pair in a['alignment'].aligned_words]\n",
    "    \n",
    "    else:\n",
    "        spanishTokens = [{\n",
    "            'tokenid' : token.i, \n",
    "            'POS' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : 'Error'\n",
    "        } for token in spaNLP(a['spanish_text'])]\n",
    "        englishTokens = [{\n",
    "            'tokenid' : token.i, \n",
    "            'POS' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : 'Error'\n",
    "        } for token in engNLP(a['english_text'])]\n",
    "        \n",
    "        alignmentTuples = []\n",
    "    \n",
    "    # build list\n",
    "    pickleList.append({\n",
    "        'spanishRawText' : a['spanish_text'],\n",
    "        'englishRawText' : a['english_text'],\n",
    "        'spanishTokenList' : spanishTokens,\n",
    "        'englishTokenList' : englishTokens,\n",
    "        'alignmentTuples' : alignmentTuples\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "p =random.choice(pickleList)\n",
    "print(p['alignmentTuples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('alignment2-26.pickle', 'wb') as handle:\n",
    "    pickle.dump(pickleList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jank version which worked ok but took SO long (8+ hrs)\n",
    "\n",
    "# # alignment tool (takes a freaking long time)\n",
    "# start = time.time()\n",
    "\n",
    "# alignmentList = []\n",
    "# success = 0\n",
    "# total = 0\n",
    "# for i in range(len(srcsents)):\n",
    "#     if srcsents[i] == '' or tgtsents[i] == '':\n",
    "#         alignmentList.append((srcsents[i],tgtsents[i]))\n",
    "#         print(f'blank space on sentence {i}')\n",
    "#     else:\n",
    "#         try:\n",
    "#             sent_sp = Sentence.from_text(srcsents[i], \"es\", is_tokenized=False)\n",
    "#             sent_es = Sentence.from_text(tgtsents[i], \"en\", is_tokenized=False)\n",
    "#             aligned = AlignedSentences(sent_sp, sent_es)\n",
    "#             print(aligned.aligned_words)\n",
    "#             alignmentList.append(aligned)\n",
    "#             success += 1\n",
    "#             print(f'successfully aligned sentence {i}')\n",
    "#         except:\n",
    "#             alignmentList.append((srcsents[i],tgtsents[i]))\n",
    "#             print(f'spacy failure on sentence {i}')\n",
    "#     total += 1\n",
    "    \n",
    "# end = time.time()\n",
    "# print(f'only took {end-start} seconds or {(end-start)/60} minutes to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
