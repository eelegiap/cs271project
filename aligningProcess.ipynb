{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paigelee/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy lang models\n",
    "import spacy\n",
    "\n",
    "spaNLP = spacy.load(\"es_core_news_sm\")\n",
    "engNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('eng_garden.txt','r') as f:\n",
    "#      engtext = f.read()\n",
    "# with open('spa_garden.txt','r') as f:\n",
    "#      spatext = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spadoc = spaNLP(spatext)\n",
    "# engdoc = engNLP(engtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sentenize\n",
    "# spasents = []\n",
    "# engsents = []\n",
    "# for sent in spadoc.sents:\n",
    "#     spasents.append(sent.text)\n",
    "# for sent in engdoc.sents:\n",
    "#     engsents.append(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write standardized files (one line per sentence) for input to Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # these are the inputs to bleualign\n",
    "# with open('sourcetext.txt','w') as f:\n",
    "#     f.write('\\n'.join(spasents))\n",
    "# with open('targettext.txt','w') as f:\n",
    "#     f.write('\\n'.join(engsents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, I also generate sourcetexttranslation.text using Google Translate and run Bleualign on the texts on my terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [START HERE] 3. Read sentence-aligned files (from Bleualign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sourceoutput.txt','r') as f:\n",
    "    srcsents = f.read().split('\\n')\n",
    "with open('targetoutput.txt','r') as f:\n",
    "    tgtsents = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Me pareció increíble que ese día sin premoniciones ni símbolos fuera el de mi muerte 2 implacable.',\n",
       " 'It seemed incredible that this day, a day without warnings or omens, might be that of my implacable death.')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcsents[i], tgtsents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['»-¡Un laberinto de marfil! -exclamé-.',\n",
       "  'Un laberinto mínimo... »-Un laberinto de símbolos -corrigió-.',\n",
       "  'Un invisible laberinto de tiempo.'],\n",
       " ['\"An ivory labyrinth?\" I exclaimed.',\n",
       "  '\"A symbolic labyrinth,\" he corrected me. \"An invisible labyrinth of time.',\n",
       "  '\"An invisible labyrinth of time.'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcsents[129:132], tgtsents[129:132]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parse aligned sentences using Astred word alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install astred[stanza]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/BramVanroy/awesome-align.git@astred_compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from astred import AlignedSentences, Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Astred Word Alignment (https://github.com/BramVanroy/astred/issues/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astred.aligned import AlignedSentences, Sentence\n",
    "from astred.aligner import Aligner\n",
    "from astred.utils import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 08:06:19 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-02-26 08:06:19 INFO: Use device: cpu\n",
      "2022-02-26 08:06:19 INFO: Loading: tokenize\n",
      "2022-02-26 08:06:19 INFO: Loading: pos\n",
      "2022-02-26 08:06:21 INFO: Loading: lemma\n",
      "2022-02-26 08:06:21 INFO: Loading: depparse\n",
      "2022-02-26 08:06:23 INFO: Done loading processors!\n",
      "2022-02-26 08:07:20 WARNING: Language es package default expects mwt, which has been added\n",
      "2022-02-26 08:07:20 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-02-26 08:07:20 INFO: Use device: cpu\n",
      "2022-02-26 08:07:20 INFO: Loading: tokenize\n",
      "2022-02-26 08:07:20 INFO: Loading: mwt\n",
      "2022-02-26 08:07:20 INFO: Loading: pos\n",
      "2022-02-26 08:07:22 INFO: Loading: lemma\n",
      "2022-02-26 08:07:22 INFO: Loading: depparse\n",
      "2022-02-26 08:07:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/219 sentences parsed.\n",
      "10/219 sentences parsed.\n",
      "20/219 sentences parsed.\n",
      "30/219 sentences parsed.\n",
      "40/219 sentences parsed.\n",
      "50/219 sentences parsed.\n",
      "60/219 sentences parsed.\n",
      "70/219 sentences parsed.\n",
      "80/219 sentences parsed.\n",
      "90/219 sentences parsed.\n",
      "100/219 sentences parsed.\n",
      "110/219 sentences parsed.\n",
      "120/219 sentences parsed.\n",
      "130/219 sentences parsed.\n",
      "140/219 sentences parsed.\n",
      "150/219 sentences parsed.\n",
      "160/219 sentences parsed.\n",
      "170/219 sentences parsed.\n",
      "180/219 sentences parsed.\n",
      "190/219 sentences parsed.\n",
      "200/219 sentences parsed.\n",
      "210/219 sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "nlp_en = load_parser(\"en\", \"stanza\", is_tokenized=False, verbose=True)\n",
    "nlp_es = load_parser(\"es\", \"stanza\", is_tokenized=False, verbose=True)\n",
    "aligner = Aligner()\n",
    "\n",
    "your_data = zip(srcsents, tgtsents)\n",
    "\n",
    "alignmentList = []\n",
    "i = 0\n",
    "success = 0\n",
    "for sent_es_str, sent_en_str in your_data:\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i}/{len(srcsents)} sentences parsed.')\n",
    "    try:\n",
    "#         joinedsents = sent_en_str.replace('.',';')\n",
    "#         if joinedsents[-1] == ';':\n",
    "#             joinedsents = joinedsents[:-1] + '.'\n",
    "#         sent_en = Sentence.from_text(joinedsents, nlp_en)\n",
    "        sent_en = Sentence.from_text(sent_en_str, nlp_en)\n",
    "        sent_es = Sentence.from_text(sent_es_str, nlp_es)\n",
    "        aligned = AlignedSentences(sent_es, sent_en, aligner=aligner)\n",
    "        alignmentList.append({\n",
    "            'spanish_text' : sent_es_str,\n",
    "            'english_text' : sent_en_str,\n",
    "            'spanish_nlp' : sent_es,\n",
    "            'english_nlp' : sent_en,\n",
    "            'alignment' : aligned\n",
    "        })\n",
    "        success += 1\n",
    "    except:\n",
    "        alignmentList.append({\n",
    "            'spanish_text' : sent_es_str,\n",
    "            'english_text' : sent_en_str,\n",
    "            'spanish_nlp' : 'Error',\n",
    "            'english_nlp' : 'Error',\n",
    "            'alignment' : 'Error'\n",
    "        })\n",
    "    i += 1\n",
    "#     if i == 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alignment done! View alignments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate of word alignment on sentences is 164/219 sents or 0.7488584474885844\n"
     ]
    }
   ],
   "source": [
    "print(f'success rate of word alignment on sentences is {success}/{i} sents or {success/i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence alignment AND word alignment (more importantly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t\t\n",
      " \t\tas\n",
      " \t\tas\n",
      " \t\tpossible\n",
      "Aniquilado , trémulo , me encogí en la otra punta de el sillón , \t\tShattered , trembling , I huddled in the distant corner of the seat ,\n",
      "lejos \t\tfar\n",
      "de el temido cristal . \t\tfrom the fearful window .\n",
      "\n",
      "\n",
      "[[NULL]] \t\t[[NULL]]\n",
      "[[NULL]] \t\tas\n",
      "[[NULL]] \t\tas\n",
      "[[NULL]] \t\tpossible\n",
      "Aniquilado \t\tShattered\n",
      ", \t\t,\n",
      "trémulo \t\ttrembling\n",
      ", \t\t,\n",
      "me \t\tI\n",
      "encogí \t\thuddled\n",
      "en \t\tin\n",
      "la \t\tthe\n",
      "otra \t\tdistant\n",
      "punta \t\tcorner\n",
      "de \t\tof\n",
      "el \t\tthe\n",
      "sillón \t\tseat\n",
      ", \t\t,\n",
      "lejos \t\tfar\n",
      "de \t\tfrom\n",
      "el \t\tthe\n",
      "temido \t\tfearful\n",
      "cristal \t\twindow\n",
      ". \t\t.\n"
     ]
    }
   ],
   "source": [
    "a = random.choice(alignmentList)\n",
    "for pair in a['alignment'].aligned_seq_spans:\n",
    "    print(pair[0].text, '\\t\\t'+pair[1].text)\n",
    "print()\n",
    "print()\n",
    "for pair in a['alignment'].aligned_words:\n",
    "    print(pair[0].text, '\\t\\t'+pair[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write something not completely huge to the Pickle\n",
    "pickleList = []\n",
    "for a in alignmentList:\n",
    "    if a['alignment'] != 'Error': \n",
    "        spanishTokens = [{\n",
    "            'tokenid' : token.id, \n",
    "            'POS' : token.upos, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma,\n",
    "            'features' : token.feats\n",
    "        } for token in a['spanish_nlp']]\n",
    "        englishTokens = [{\n",
    "            'tokenid' : token.id, \n",
    "            'POS' : token.upos, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma,\n",
    "            'features' : token.feats\n",
    "        } for token in a['english_nlp']]\n",
    "        alignmentTuples = [(pair[0].id, pair[1].id) for pair in a['alignment'].aligned_words]\n",
    "    \n",
    "    else:\n",
    "        spanishTokens = [{\n",
    "            'tokenid' : token.i, \n",
    "            'POS' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : 'Error'\n",
    "        } for token in spaNLP(a['spanish_text'])]\n",
    "        englishTokens = [{\n",
    "            'tokenid' : token.i, \n",
    "            'POS' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : 'Error'\n",
    "        } for token in engNLP(a['english_text'])]\n",
    "        \n",
    "        alignmentTuples = []\n",
    "    \n",
    "    # build list\n",
    "    pickleList.append({\n",
    "        'spanishRawText' : a['spanish_text'],\n",
    "        'englishRawText' : a['english_text'],\n",
    "        'spanishTokenList' : spanishTokens,\n",
    "        'englishTokenList' : englishTokens,\n",
    "        'alignmentTuples' : alignmentTuples\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "p =random.choice(pickleList)\n",
    "print(p['alignmentTuples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('alignment2-26.pickle', 'wb') as handle:\n",
    "    pickle.dump(pickleList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jank version which worked ok but took SO long (8+ hrs)\n",
    "\n",
    "# # alignment tool (takes a freaking long time)\n",
    "# start = time.time()\n",
    "\n",
    "# alignmentList = []\n",
    "# success = 0\n",
    "# total = 0\n",
    "# for i in range(len(srcsents)):\n",
    "#     if srcsents[i] == '' or tgtsents[i] == '':\n",
    "#         alignmentList.append((srcsents[i],tgtsents[i]))\n",
    "#         print(f'blank space on sentence {i}')\n",
    "#     else:\n",
    "#         try:\n",
    "#             sent_sp = Sentence.from_text(srcsents[i], \"es\", is_tokenized=False)\n",
    "#             sent_es = Sentence.from_text(tgtsents[i], \"en\", is_tokenized=False)\n",
    "#             aligned = AlignedSentences(sent_sp, sent_es)\n",
    "#             print(aligned.aligned_words)\n",
    "#             alignmentList.append(aligned)\n",
    "#             success += 1\n",
    "#             print(f'successfully aligned sentence {i}')\n",
    "#         except:\n",
    "#             alignmentList.append((srcsents[i],tgtsents[i]))\n",
    "#             print(f'spacy failure on sentence {i}')\n",
    "#     total += 1\n",
    "    \n",
    "# end = time.time()\n",
    "# print(f'only took {end-start} seconds or {(end-start)/60} minutes to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
