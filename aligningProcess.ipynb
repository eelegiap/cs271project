{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import spacy\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy lang models\n",
    "\n",
    "spaNLP = spacy.load(\"es_core_news_sm\")\n",
    "engNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('raw_english.txt','r') as f:\n",
    "     engtext = f.read()\n",
    "with open('raw_spanish.txt','r') as f:\n",
    "     spatext = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spadoc = spaNLP(spatext)\n",
    "engdoc = engNLP(engtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentenize\n",
    "spasents = []\n",
    "engsents = []\n",
    "for sent in spadoc.sents:\n",
    "    spasents.append(sent.text)\n",
    "for sent in engdoc.sents:\n",
    "    engsents.append(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write standardized files (one line per sentence) for input to Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the inputs to bleualign\n",
    "with open('sourcetext.txt','w') as f:\n",
    "    f.write('\\n'.join(spasents))\n",
    "with open('targettext.txt','w') as f:\n",
    "    f.write('\\n'.join(engsents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, I also generate sourcetexttranslation.text using Google Translate and run Bleualign on the texts on my terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [START HERE] 3. Read sentence-aligned files (from Bleualign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sent_aligned_spanish.txt','r') as f:\n",
    "    srcsents = f.read().split('\\n')\n",
    "with open('sent_aligned_english.txt','r') as f:\n",
    "    tgtsents = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Reconocí, encuadernados en seda amarilla, algunos tomos manuscritos de la Enciclopedia Perdida que dirigió el Tercer Emperador de la Dinastía Luminosa y que no se dio nunca a la imprenta.',\n",
       " 'I recognized some large volumes bound in yellow silk-manuscripts of the Lost Encyclopedia which was edited by the Third Emperor of the Luminous Dynasty. They had never been printed.')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcsents[i], tgtsents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/214 sentences parsed.\n",
      "50/214 sentences parsed.\n",
      "100/214 sentences parsed.\n",
      "150/214 sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "# sent to sent alignment\n",
    "oneLineSpa, oneLineEng = spasents, engsents\n",
    "alignedSpa, alignedEng = srcsents, tgtsents\n",
    "sentAlignments = []\n",
    "spaIndex = 0\n",
    "for alignSpaSent, alignEngSent in zip(alignedSpa, alignedEng):\n",
    "    if spaIndex % 50 == 0:\n",
    "        print(f'{spaIndex}/{len(srcsents)} sentences parsed.')\n",
    "    individualEngSents = [sent.text for sent in engNLP(alignEngSent).sents]\n",
    "    for indEngSent in individualEngSents:\n",
    "        for i, thisEngLine in enumerate(oneLineEng):\n",
    "            if indEngSent.strip() == thisEngLine.strip():\n",
    "                engIndex = i\n",
    "        for j, thisSpaLine in enumerate(oneLineSpa):\n",
    "            if alignSpaSent.strip() == thisSpaLine.strip():\n",
    "                spaIndex = j\n",
    "        sentAlignments.append({\n",
    "            'indices' : (spaIndex, engIndex),\n",
    "            'sents' : (oneLineSpa[spaIndex], oneLineEng[engIndex])\n",
    "        })\n",
    "    spaIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"¿Ashgrove?\", les pregunté a unos chicos en el andén.\n",
      "\"Ashgrove?\"\n"
     ]
    }
   ],
   "source": [
    "# chec, k it works\n",
    "randSentAlign = random.choice(sentAlignments)\n",
    "s, t = randSentAlign['sents']\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('borges_sent_alignment_3-12.pickle', 'wb') as handle:\n",
    "    pickle.dump(sentAlignments, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parse word alignment using SimAlign (recommended: fast and high coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install simalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-03-12 11:28:27,941 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "\n",
    "# making an instance of our model.\n",
    "# You can specify the embedding model and all alignment settings in the constructor.\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # The source and target sentences should be tokenized to words.\n",
    "# src_sentence = [\"This\", \"is\", \"a\", \"test\", \".\"]\n",
    "# trg_sentence = [\"Das\", \"ist\", \"ein\", \"Test\", \".\"]\n",
    "\n",
    "# # The output is a dictionary with different matching methods.\n",
    "# # Each method has a list of pairs indicating the indexes of aligned words (The alignments are zero-indexed).\n",
    "# alignments = myaligner.get_word_aligns(src_sentence, trg_sentence)\n",
    "\n",
    "# for matching_method in alignments:\n",
    "#     print(matching_method, \":\", alignments[matching_method])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate word alignment with SimAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/214 sentences parsed.\n",
      "10/214 sentences parsed.\n",
      "20/214 sentences parsed.\n",
      "30/214 sentences parsed.\n",
      "40/214 sentences parsed.\n",
      "50/214 sentences parsed.\n",
      "60/214 sentences parsed.\n",
      "70/214 sentences parsed.\n",
      "80/214 sentences parsed.\n",
      "90/214 sentences parsed.\n",
      "100/214 sentences parsed.\n",
      "110/214 sentences parsed.\n",
      "120/214 sentences parsed.\n",
      "130/214 sentences parsed.\n",
      "140/214 sentences parsed.\n",
      "150/214 sentences parsed.\n",
      "160/214 sentences parsed.\n",
      "170/214 sentences parsed.\n",
      "180/214 sentences parsed.\n",
      "190/214 sentences parsed.\n",
      "200/214 sentences parsed.\n",
      "210/214 sentences parsed.\n",
      "parsed in 644.7377479076385 s\n"
     ]
    }
   ],
   "source": [
    "your_data = zip(srcsents, tgtsents)\n",
    "start = time.time()\n",
    "\n",
    "alignmentList = []\n",
    "i = 0\n",
    "for sent_es_str, sent_en_str in your_data:\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i}/{len(srcsents)} sentences parsed.')\n",
    "    srcDoc = spaNLP.tokenizer(sent_es_str)\n",
    "    tgtDoc = engNLP.tokenizer(sent_en_str)\n",
    "    src = [t.text for t in srcDoc]\n",
    "    tgt = [t.text for t in tgtDoc]\n",
    "    alignments = myaligner.get_word_aligns(src, tgt)\n",
    "    itermax = alignments['itermax']\n",
    "    \n",
    "    alignmentList.append({\n",
    "        'indices' : itermax,\n",
    "        'words' : [(src[s], tgt[t]) for s, t in itermax]\n",
    "    })\n",
    "    \n",
    "    i += 1\n",
    "end = time.time()\n",
    "print('parsed in',end-start,'s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indices': [(0, 0),\n",
       "  (1, 2),\n",
       "  (2, 3),\n",
       "  (3, 4),\n",
       "  (4, 5),\n",
       "  (5, 6),\n",
       "  (6, 7),\n",
       "  (7, 1),\n",
       "  (8, 8),\n",
       "  (9, 9)],\n",
       " 'words': [('Alcé', 'I'),\n",
       "  ('los', 'my'),\n",
       "  ('ojos', 'eyes'),\n",
       "  ('y', 'and'),\n",
       "  ('la', 'the'),\n",
       "  ('tenue', 'short'),\n",
       "  ('pesadilla', 'nightmare'),\n",
       "  ('se', 'lifted'),\n",
       "  ('disipó', 'disappeared'),\n",
       "  ('.', '.')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(alignmentList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import to pickle (bytestream) for later access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('borges_word_alignment_3-12.pickle', 'wb') as handle:\n",
    "    pickle.dump(alignmentList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### an example of simalign on a single pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwmf : [(0, 0), (5, 21), (9, 32), (10, 3), (11, 9), (16, 13), (17, 15), (18, 1), (19, 2), (21, 4), (23, 5), (23, 31), (24, 6), (25, 7), (26, 8), (27, 11), (29, 10), (30, 12), (31, 33), (32, 14), (33, 16), (34, 17), (35, 18), (36, 19), (37, 20), (38, 22), (39, 23), (40, 24), (41, 25), (42, 26), (43, 27), (44, 28), (45, 29), (46, 30), (47, 31), (48, 34)]\n",
      "inter : [(0, 0), (11, 9), (16, 13), (18, 1), (21, 4), (23, 5), (24, 6), (25, 7), (26, 8), (27, 11), (29, 10), (30, 12), (32, 14), (34, 17), (35, 18), (36, 19), (37, 20), (38, 22), (39, 23), (40, 24), (41, 25), (42, 26), (43, 27), (44, 28), (45, 29), (46, 30), (48, 32)]\n",
      "itermax : [(0, 0), (10, 3), (11, 9), (12, 10), (15, 7), (16, 13), (17, 0), (18, 1), (19, 2), (21, 4), (23, 5), (24, 6), (25, 7), (26, 8), (27, 11), (28, 9), (29, 10), (30, 12), (32, 14), (33, 16), (34, 17), (35, 18), (36, 19), (37, 20), (38, 21), (38, 22), (39, 23), (40, 24), (41, 25), (42, 26), (43, 27), (44, 28), (45, 29), (46, 30), (47, 31), (48, 32), (48, 34)]\n"
     ]
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcDoc = spaNLP.tokenizer(srcsents[i])\n",
    "tgtDoc = engNLP.tokenizer(tgtsents[i])\n",
    "src = [t.text for t in srcDoc]\n",
    "tgt = [t.text for t in tgtDoc]\n",
    "alignments = myaligner.get_word_aligns(src, tgt)\n",
    "\n",
    "for match in alignments:\n",
    "    print(match, ':', alignments[match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En In\n",
      "favorable very\n",
      ", .\n",
      "usted you\n",
      "ha have\n",
      "; .\n",
      "en yet\n",
      "otro another\n",
      ", ,\n",
      ", ,\n",
      "atravesar crossing\n",
      "atravesar phantom\n",
      "el the\n",
      "jardín garden\n",
      ", ,\n",
      "me me\n",
      "encontrado found\n",
      "muerto dead\n",
      "; \"\n",
      "en In\n",
      "otro another\n",
      ", ,\n",
      "yo I\n",
      "digo say\n",
      "estas these\n",
      "mismas same\n",
      "palabras words\n",
      ", ,\n",
      "pero but\n",
      "soy am\n",
      "un an\n",
      "error error\n",
      ", ,\n",
      "un a\n",
      "fantasma phantom\n",
      ". 8\n",
      "\n",
      "En In\n",
      "ha have\n",
      "; .\n",
      "otro another\n",
      ", ,\n",
      "atravesar crossing\n",
      "el the\n",
      "jardín garden\n",
      ", ,\n",
      "me me\n",
      "encontrado found\n",
      "muerto dead\n",
      "en In\n",
      ", ,\n",
      "yo I\n",
      "digo say\n",
      "estas these\n",
      "mismas same\n",
      "palabras words\n",
      ", ,\n",
      "pero but\n",
      "soy am\n",
      "un an\n",
      "error error\n",
      ", ,\n",
      "un a\n",
      ". .\n",
      "\n",
      "En In\n",
      "usted you\n",
      "ha have\n",
      "llegado found\n",
      "casa garden\n",
      "; .\n",
      "en In\n",
      "otro another\n",
      ", ,\n",
      ", ,\n",
      "atravesar crossing\n",
      "el the\n",
      "jardín garden\n",
      ", ,\n",
      "me me\n",
      "ha have\n",
      "encontrado found\n",
      "muerto dead\n",
      "en In\n",
      "otro another\n",
      ", ,\n",
      "yo I\n",
      "digo say\n",
      "estas these\n",
      "mismas very\n",
      "mismas same\n",
      "palabras words\n",
      ", ,\n",
      "pero but\n",
      "soy am\n",
      "un an\n",
      "error error\n",
      ", ,\n",
      "un a\n",
      "fantasma phantom\n",
      ". .\n",
      ". 8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match in alignments:\n",
    "    for s, t in alignments[match]:\n",
    "        print(src[s], tgt[t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Parse aligned sentences using Astred word alignment (backup, not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install astred[stanza]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/BramVanroy/awesome-align.git@astred_compat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from astred import AlignedSentences, Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Astred Word Alignment (https://github.com/BramVanroy/astred/issues/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astred.aligned import AlignedSentences, Sentence\n",
    "from astred.aligner import Aligner\n",
    "from astred.utils import load_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-26 08:06:19 INFO: Loading these models for language: en (English):\n",
      "========================\n",
      "| Processor | Package  |\n",
      "------------------------\n",
      "| tokenize  | combined |\n",
      "| pos       | combined |\n",
      "| lemma     | combined |\n",
      "| depparse  | combined |\n",
      "========================\n",
      "\n",
      "2022-02-26 08:06:19 INFO: Use device: cpu\n",
      "2022-02-26 08:06:19 INFO: Loading: tokenize\n",
      "2022-02-26 08:06:19 INFO: Loading: pos\n",
      "2022-02-26 08:06:21 INFO: Loading: lemma\n",
      "2022-02-26 08:06:21 INFO: Loading: depparse\n",
      "2022-02-26 08:06:23 INFO: Done loading processors!\n",
      "2022-02-26 08:07:20 WARNING: Language es package default expects mwt, which has been added\n",
      "2022-02-26 08:07:20 INFO: Loading these models for language: es (Spanish):\n",
      "=======================\n",
      "| Processor | Package |\n",
      "-----------------------\n",
      "| tokenize  | ancora  |\n",
      "| mwt       | ancora  |\n",
      "| pos       | ancora  |\n",
      "| lemma     | ancora  |\n",
      "| depparse  | ancora  |\n",
      "=======================\n",
      "\n",
      "2022-02-26 08:07:20 INFO: Use device: cpu\n",
      "2022-02-26 08:07:20 INFO: Loading: tokenize\n",
      "2022-02-26 08:07:20 INFO: Loading: mwt\n",
      "2022-02-26 08:07:20 INFO: Loading: pos\n",
      "2022-02-26 08:07:22 INFO: Loading: lemma\n",
      "2022-02-26 08:07:22 INFO: Loading: depparse\n",
      "2022-02-26 08:07:25 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/219 sentences parsed.\n",
      "10/219 sentences parsed.\n",
      "20/219 sentences parsed.\n",
      "30/219 sentences parsed.\n",
      "40/219 sentences parsed.\n",
      "50/219 sentences parsed.\n",
      "60/219 sentences parsed.\n",
      "70/219 sentences parsed.\n",
      "80/219 sentences parsed.\n",
      "90/219 sentences parsed.\n",
      "100/219 sentences parsed.\n",
      "110/219 sentences parsed.\n",
      "120/219 sentences parsed.\n",
      "130/219 sentences parsed.\n",
      "140/219 sentences parsed.\n",
      "150/219 sentences parsed.\n",
      "160/219 sentences parsed.\n",
      "170/219 sentences parsed.\n",
      "180/219 sentences parsed.\n",
      "190/219 sentences parsed.\n",
      "200/219 sentences parsed.\n",
      "210/219 sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "nlp_en = load_parser(\"en\", \"stanza\", is_tokenized=False, verbose=True)\n",
    "nlp_es = load_parser(\"es\", \"stanza\", is_tokenized=False, verbose=True)\n",
    "aligner = Aligner()\n",
    "\n",
    "your_data = zip(srcsents, tgtsents)\n",
    "\n",
    "alignmentList = []\n",
    "i = 0\n",
    "success = 0\n",
    "for sent_es_str, sent_en_str in your_data:\n",
    "    if i % 10 == 0:\n",
    "        print(f'{i}/{len(srcsents)} sentences parsed.')\n",
    "    try:\n",
    "#         joinedsents = sent_en_str.replace('.',';')\n",
    "#         if joinedsents[-1] == ';':\n",
    "#             joinedsents = joinedsents[:-1] + '.'\n",
    "#         sent_en = Sentence.from_text(joinedsents, nlp_en)\n",
    "        sent_en = Sentence.from_text(sent_en_str, nlp_en)\n",
    "        sent_es = Sentence.from_text(sent_es_str, nlp_es)\n",
    "        aligned = AlignedSentences(sent_es, sent_en, aligner=aligner)\n",
    "        alignmentList.append({\n",
    "            'spanish_text' : sent_es_str,\n",
    "            'english_text' : sent_en_str,\n",
    "            'spanish_nlp' : sent_es,\n",
    "            'english_nlp' : sent_en,\n",
    "            'alignment' : aligned\n",
    "        })\n",
    "        success += 1\n",
    "    except:\n",
    "        alignmentList.append({\n",
    "            'spanish_text' : sent_es_str,\n",
    "            'english_text' : sent_en_str,\n",
    "            'spanish_nlp' : 'Error',\n",
    "            'english_nlp' : 'Error',\n",
    "            'alignment' : 'Error'\n",
    "        })\n",
    "    i += 1\n",
    "#     if i == 20:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Alignment done! View alignments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success rate of word alignment on sentences is 164/219 sents or 0.7488584474885844\n"
     ]
    }
   ],
   "source": [
    "print(f'success rate of word alignment on sentences is {success}/{i} sents or {success/i}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequence alignment AND word alignment (more importantly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \t\t\n",
      " \t\tas\n",
      " \t\tas\n",
      " \t\tpossible\n",
      "Aniquilado , trémulo , me encogí en la otra punta de el sillón , \t\tShattered , trembling , I huddled in the distant corner of the seat ,\n",
      "lejos \t\tfar\n",
      "de el temido cristal . \t\tfrom the fearful window .\n",
      "\n",
      "\n",
      "[[NULL]] \t\t[[NULL]]\n",
      "[[NULL]] \t\tas\n",
      "[[NULL]] \t\tas\n",
      "[[NULL]] \t\tpossible\n",
      "Aniquilado \t\tShattered\n",
      ", \t\t,\n",
      "trémulo \t\ttrembling\n",
      ", \t\t,\n",
      "me \t\tI\n",
      "encogí \t\thuddled\n",
      "en \t\tin\n",
      "la \t\tthe\n",
      "otra \t\tdistant\n",
      "punta \t\tcorner\n",
      "de \t\tof\n",
      "el \t\tthe\n",
      "sillón \t\tseat\n",
      ", \t\t,\n",
      "lejos \t\tfar\n",
      "de \t\tfrom\n",
      "el \t\tthe\n",
      "temido \t\tfearful\n",
      "cristal \t\twindow\n",
      ". \t\t.\n"
     ]
    }
   ],
   "source": [
    "a = random.choice(alignmentList)\n",
    "for pair in a['alignment'].aligned_seq_spans:\n",
    "    print(pair[0].text, '\\t\\t'+pair[1].text)\n",
    "print()\n",
    "print()\n",
    "for pair in a['alignment'].aligned_words:\n",
    "    print(pair[0].text, '\\t\\t'+pair[1].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Write something not completely huge to the Pickle\n",
    "pickleList = []\n",
    "for a in alignmentList:\n",
    "    if a['alignment'] != 'Error': \n",
    "        spanishTokens = [{\n",
    "            'tokenid' : token.id, \n",
    "            'POS' : token.upos, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma,\n",
    "            'features' : token.feats\n",
    "        } for token in a['spanish_nlp']]\n",
    "        englishTokens = [{\n",
    "            'tokenid' : token.id, \n",
    "            'POS' : token.upos, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma,\n",
    "            'features' : token.feats\n",
    "        } for token in a['english_nlp']]\n",
    "        alignmentTuples = [(pair[0].id, pair[1].id) for pair in a['alignment'].aligned_words]\n",
    "    \n",
    "    else:\n",
    "        spanishTokens = [{\n",
    "            'tokenid' : token.i, \n",
    "            'POS' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : 'Error'\n",
    "        } for token in spaNLP(a['spanish_text'])]\n",
    "        englishTokens = [{\n",
    "            'tokenid' : token.i, \n",
    "            'POS' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : 'Error'\n",
    "        } for token in engNLP(a['english_text'])]\n",
    "        \n",
    "        alignmentTuples = []\n",
    "    \n",
    "    # build list\n",
    "    pickleList.append({\n",
    "        'spanishRawText' : a['spanish_text'],\n",
    "        'englishRawText' : a['english_text'],\n",
    "        'spanishTokenList' : spanishTokens,\n",
    "        'englishTokenList' : englishTokens,\n",
    "        'alignmentTuples' : alignmentTuples\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "p =random.choice(pickleList)\n",
    "print(p['alignmentTuples'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('alignment2-26.pickle', 'wb') as handle:\n",
    "    pickle.dump(pickleList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ignore below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jank version which worked ok but took SO long (8+ hrs)\n",
    "\n",
    "# # alignment tool (takes a freaking long time)\n",
    "# start = time.time()\n",
    "\n",
    "# alignmentList = []\n",
    "# success = 0\n",
    "# total = 0\n",
    "# for i in range(len(srcsents)):\n",
    "#     if srcsents[i] == '' or tgtsents[i] == '':\n",
    "#         alignmentList.append((srcsents[i],tgtsents[i]))\n",
    "#         print(f'blank space on sentence {i}')\n",
    "#     else:\n",
    "#         try:\n",
    "#             sent_sp = Sentence.from_text(srcsents[i], \"es\", is_tokenized=False)\n",
    "#             sent_es = Sentence.from_text(tgtsents[i], \"en\", is_tokenized=False)\n",
    "#             aligned = AlignedSentences(sent_sp, sent_es)\n",
    "#             print(aligned.aligned_words)\n",
    "#             alignmentList.append(aligned)\n",
    "#             success += 1\n",
    "#             print(f'successfully aligned sentence {i}')\n",
    "#         except:\n",
    "#             alignmentList.append((srcsents[i],tgtsents[i]))\n",
    "#             print(f'spacy failure on sentence {i}')\n",
    "#     total += 1\n",
    "    \n",
    "# end = time.time()\n",
    "# print(f'only took {end-start} seconds or {(end-start)/60} minutes to run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
