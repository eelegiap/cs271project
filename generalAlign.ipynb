{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.morphology import Morphology\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy lang models\n",
    "\n",
    "spaNLP = spacy.load(\"es_core_news_sm\")\n",
    "engNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts/rawsource.txt','r') as f:\n",
    "     sourcetxt = f.read()\n",
    "with open('texts/rawtarget.txt','r') as f:\n",
    "     targettxt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedoc = spaNLP(sourcetxt)\n",
    "targetdoc = engNLP(targettxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentenize\n",
    "rawsrcsents = []\n",
    "rawtgtsents = []\n",
    "for sent in sourcedoc.sents:\n",
    "    rawsrcsents.append(sent.text)\n",
    "for sent in targetdoc.sents:\n",
    "    rawtgtsents.append(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write standardized files (one line per sentence) for input to Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the inputs to bleualign\n",
    "with open('sourcetextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawsrcsents))\n",
    "with open('targettextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawtgtsents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized sentences for data output\n",
    "srctokens = []\n",
    "for srcsent in rawsrcsents:\n",
    "    tokens = spaNLP(srcsent)\n",
    "    srctokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])\n",
    "tgttokens = []\n",
    "for tgtsent in rawtgtsents:\n",
    "    tokens = engNLP(tgtsent)\n",
    "    tgttokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, I also generate sourcetexttranslation.text using Google Translate and run Bleualign on the texts on my terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install translators --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine translation took 212.9293007850647 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "translatedsourcesents = []\n",
    "for sent in rawsrcsents:\n",
    "    translatedsourcesents.append(ts.google(sent, to_language = 'en'))\n",
    "end = time.time()\n",
    "print(f'machine translation took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translatedsource.txt','w') as f:\n",
    "    f.write('\\n'.join(translatedsourcesents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in article 0: \n",
      "processing\n",
      "computing alignment between srctotarget (file 0) and target text\n",
      "Evaluating sentences with bleu\n",
      "finished\n",
      "searching for longest path of good alignments\n",
      "finished\n",
      "Wed Mar 30 15:26:57 2022\n",
      "filling gaps\n",
      "finished\n",
      "Wed Mar 30 15:26:57 2022\n",
      "Results of BLEU 1-to-1 alignment\n",
      "\u001b[92m0: 0\u001b[1;m\n",
      "\u001b[1;31m1: unaligned. best cand 108\u001b[1;m\n",
      "\u001b[92m2: 2\u001b[1;m\n",
      "\u001b[92m3: 3\u001b[1;m\n",
      "\u001b[92m4: 6\u001b[1;m\n",
      "\u001b[92m5: 7\u001b[1;m\n",
      "\u001b[92m6: 8\u001b[1;m\n",
      "\u001b[92m7: 9\u001b[1;m\n",
      "\u001b[92m8: 10\u001b[1;m\n",
      "\u001b[92m9: 11\u001b[1;m\n",
      "\u001b[92m10: 12\u001b[1;m\n",
      "\u001b[92m11: 13\u001b[1;m\n",
      "\u001b[92m12: 16\u001b[1;m\n",
      "\u001b[92m13: 17\u001b[1;m\n",
      "\u001b[92m14: 18\u001b[1;m\n",
      "\u001b[92m15: 19\u001b[1;m\n",
      "\u001b[92m16: 20\u001b[1;m\n",
      "\u001b[92m17: 22\u001b[1;m\n",
      "\u001b[92m18: 23\u001b[1;m\n",
      "\u001b[92m19: 24\u001b[1;m\n",
      "\u001b[1;31m20: unaligned. best cand 39\u001b[1;m\n",
      "\u001b[92m21: 25\u001b[1;m\n",
      "\u001b[92m22: 26\u001b[1;m\n",
      "\u001b[92m23: 28\u001b[1;m\n",
      "\u001b[92m24: 29\u001b[1;m\n",
      "\u001b[92m25: 31\u001b[1;m\n",
      "\u001b[92m26: 32\u001b[1;m\n",
      "\u001b[92m27: 33\u001b[1;m\n",
      "\u001b[92m28: 35\u001b[1;m\n",
      "\u001b[92m29: 36\u001b[1;m\n",
      "\u001b[92m30: 37\u001b[1;m\n",
      "\u001b[92m31: 38\u001b[1;m\n",
      "\u001b[92m32: 40\u001b[1;m\n",
      "\u001b[92m33: 41\u001b[1;m\n",
      "\u001b[92m34: 42\u001b[1;m\n",
      "\u001b[92m35: 43\u001b[1;m\n",
      "\u001b[92m36: 44\u001b[1;m\n",
      "\u001b[92m37: 45\u001b[1;m\n",
      "\u001b[92m38: 46\u001b[1;m\n",
      "\u001b[92m39: 47\u001b[1;m\n",
      "\u001b[92m40: 48\u001b[1;m\n",
      "\u001b[92m41: 49\u001b[1;m\n",
      "\u001b[92m42: 50\u001b[1;m\n",
      "\u001b[92m43: 51\u001b[1;m\n",
      "\u001b[92m44: 52\u001b[1;m\n",
      "\u001b[92m45: 53\u001b[1;m\n",
      "\u001b[92m46: 55\u001b[1;m\n",
      "\u001b[92m47: 56\u001b[1;m\n",
      "\u001b[92m48: 58\u001b[1;m\n",
      "\u001b[92m49: 59\u001b[1;m\n",
      "\u001b[92m50: 60\u001b[1;m\n",
      "\u001b[92m51: 61\u001b[1;m\n",
      "\u001b[92m52: 63\u001b[1;m\n",
      "\u001b[92m53: 64\u001b[1;m\n",
      "\u001b[92m54: 65\u001b[1;m\n",
      "\u001b[92m55: 66\u001b[1;m\n",
      "\u001b[92m56: 67\u001b[1;m\n",
      "\u001b[92m57: 68\u001b[1;m\n",
      "\u001b[92m58: 69\u001b[1;m\n",
      "\u001b[92m59: 70\u001b[1;m\n",
      "\u001b[92m60: 71\u001b[1;m\n",
      "\u001b[92m61: 72\u001b[1;m\n",
      "\u001b[92m62: 73\u001b[1;m\n",
      "\u001b[92m63: 75\u001b[1;m\n",
      "\u001b[92m64: 76\u001b[1;m\n",
      "\u001b[92m65: 77\u001b[1;m\n",
      "\u001b[92m66: 78\u001b[1;m\n",
      "\u001b[92m67: 79\u001b[1;m\n",
      "\u001b[92m68: 81\u001b[1;m\n",
      "\u001b[92m69: 82\u001b[1;m\n",
      "\u001b[1;31m70: unaligned. best cand 14\u001b[1;m\n",
      "\u001b[92m71: 84\u001b[1;m\n",
      "\u001b[92m72: 86\u001b[1;m\n",
      "\u001b[92m73: 87\u001b[1;m\n",
      "\u001b[1;31m74: unaligned. best cand 87\u001b[1;m\n",
      "\u001b[92m75: 89\u001b[1;m\n",
      "\u001b[92m76: 90\u001b[1;m\n",
      "\u001b[92m77: 91\u001b[1;m\n",
      "\u001b[92m78: 92\u001b[1;m\n",
      "\u001b[92m79: 95\u001b[1;m\n",
      "\u001b[92m80: 97\u001b[1;m\n",
      "\u001b[92m81: 99\u001b[1;m\n",
      "\u001b[92m82: 101\u001b[1;m\n",
      "\u001b[92m83: 102\u001b[1;m\n",
      "\u001b[92m84: 103\u001b[1;m\n",
      "\u001b[92m85: 105\u001b[1;m\n",
      "\u001b[92m86: 107\u001b[1;m\n",
      "\u001b[92m87: 108\u001b[1;m\n",
      "\u001b[92m88: 109\u001b[1;m\n",
      "\u001b[92m89: 110\u001b[1;m\n",
      "\u001b[92m90: 111\u001b[1;m\n",
      "\u001b[92m91: 112\u001b[1;m\n",
      "\u001b[92m92: 113\u001b[1;m\n",
      "\u001b[92m93: 114\u001b[1;m\n",
      "\u001b[92m94: 115\u001b[1;m\n",
      "\u001b[92m95: 116\u001b[1;m\n",
      "\u001b[92m96: 117\u001b[1;m\n",
      "\u001b[92m97: 118\u001b[1;m\n",
      "\u001b[92m98: 119\u001b[1;m\n",
      "\u001b[92m99: 121\u001b[1;m\n",
      "\u001b[92m100: 122\u001b[1;m\n",
      "\u001b[92m101: 124\u001b[1;m\n",
      "\u001b[92m102: 125\u001b[1;m\n",
      "\u001b[92m103: 126\u001b[1;m\n",
      "\u001b[92m104: 127\u001b[1;m\n",
      "\u001b[1;31m105: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m106: 129\u001b[1;m\n",
      "\u001b[92m107: 130\u001b[1;m\n",
      "\u001b[92m108: 131\u001b[1;m\n",
      "\u001b[92m109: 133\u001b[1;m\n",
      "\u001b[92m110: 134\u001b[1;m\n",
      "\u001b[1;31m111: unaligned. best cand 244\u001b[1;m\n",
      "\u001b[92m112: 137\u001b[1;m\n",
      "\u001b[92m113: 139\u001b[1;m\n",
      "\u001b[92m114: 140\u001b[1;m\n",
      "\u001b[92m115: 141\u001b[1;m\n",
      "\u001b[92m116: 142\u001b[1;m\n",
      "\u001b[1;31m117: unaligned. best cand 229\u001b[1;m\n",
      "\u001b[92m118: 143\u001b[1;m\n",
      "\u001b[92m119: 145\u001b[1;m\n",
      "\u001b[92m120: 146\u001b[1;m\n",
      "\u001b[92m121: 147\u001b[1;m\n",
      "\u001b[92m122: 148\u001b[1;m\n",
      "\u001b[92m123: 149\u001b[1;m\n",
      "\u001b[92m124: 150\u001b[1;m\n",
      "\u001b[92m125: 151\u001b[1;m\n",
      "\u001b[92m126: 154\u001b[1;m\n",
      "\u001b[92m127: 156\u001b[1;m\n",
      "\u001b[92m128: 157\u001b[1;m\n",
      "\u001b[92m129: 159\u001b[1;m\n",
      "\u001b[92m130: 160\u001b[1;m\n",
      "\u001b[92m131: 161\u001b[1;m\n",
      "\u001b[1;31m132: unaligned. best cand 228\u001b[1;m\n",
      "\u001b[1;31m133: unaligned. best cand 144\u001b[1;m\n",
      "\u001b[92m134: 164\u001b[1;m\n",
      "\u001b[92m135: 165\u001b[1;m\n",
      "\u001b[92m136: 168\u001b[1;m\n",
      "\u001b[92m137: 170\u001b[1;m\n",
      "\u001b[92m138: 171\u001b[1;m\n",
      "\u001b[1;31m139: unaligned. best cand 171\u001b[1;m\n",
      "\u001b[1;31m140: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m141: 173\u001b[1;m\n",
      "\u001b[1;31m142: unaligned. best cand 250\u001b[1;m\n",
      "\u001b[1;31m143: unaligned. best cand 169\u001b[1;m\n",
      "\u001b[92m144: 177\u001b[1;m\n",
      "\u001b[92m145: 178\u001b[1;m\n",
      "\u001b[92m146: 179\u001b[1;m\n",
      "\u001b[92m147: 180\u001b[1;m\n",
      "\u001b[92m148: 181\u001b[1;m\n",
      "\u001b[92m149: 182\u001b[1;m\n",
      "\u001b[92m150: 183\u001b[1;m\n",
      "\u001b[92m151: 184\u001b[1;m\n",
      "\u001b[92m152: 185\u001b[1;m\n",
      "\u001b[1;31m153: unaligned. best cand 177\u001b[1;m\n",
      "\u001b[92m154: 188\u001b[1;m\n",
      "\u001b[92m155: 189\u001b[1;m\n",
      "\u001b[92m156: 190\u001b[1;m\n",
      "\u001b[92m157: 192\u001b[1;m\n",
      "\u001b[92m158: 193\u001b[1;m\n",
      "\u001b[92m159: 194\u001b[1;m\n",
      "\u001b[92m160: 198\u001b[1;m\n",
      "\u001b[92m161: 199\u001b[1;m\n",
      "\u001b[92m162: 201\u001b[1;m\n",
      "\u001b[92m163: 202\u001b[1;m\n",
      "\u001b[92m164: 203\u001b[1;m\n",
      "\u001b[92m165: 204\u001b[1;m\n",
      "\u001b[1;31m166: unaligned. best cand 182\u001b[1;m\n",
      "\u001b[92m167: 209\u001b[1;m\n",
      "\u001b[92m168: 210\u001b[1;m\n",
      "\u001b[92m169: 212\u001b[1;m\n",
      "\u001b[92m170: 213\u001b[1;m\n",
      "\u001b[92m171: 214\u001b[1;m\n",
      "\u001b[92m172: 215\u001b[1;m\n",
      "\u001b[92m173: 216\u001b[1;m\n",
      "\u001b[92m174: 218\u001b[1;m\n",
      "\u001b[92m175: 219\u001b[1;m\n",
      "\u001b[92m176: 220\u001b[1;m\n",
      "\u001b[92m177: 221\u001b[1;m\n",
      "\u001b[92m178: 222\u001b[1;m\n",
      "\u001b[92m179: 223\u001b[1;m\n",
      "\u001b[1;31m180: unaligned. best cand []\u001b[1;m\n",
      "\u001b[1;31m181: unaligned. best cand 250\u001b[1;m\n",
      "\u001b[92m182: 227\u001b[1;m\n",
      "\u001b[92m183: 228\u001b[1;m\n",
      "\u001b[92m184: 230\u001b[1;m\n",
      "\u001b[92m185: 232\u001b[1;m\n",
      "\u001b[92m186: 233\u001b[1;m\n",
      "\u001b[92m187: 234\u001b[1;m\n",
      "\u001b[92m188: 238\u001b[1;m\n",
      "\u001b[92m189: 239\u001b[1;m\n",
      "\u001b[92m190: 240\u001b[1;m\n",
      "\u001b[1;31m191: unaligned. best cand 188\u001b[1;m\n",
      "\u001b[92m192: 243\u001b[1;m\n",
      "\u001b[92m193: 246\u001b[1;m\n",
      "\u001b[92m194: 248\u001b[1;m\n",
      "\u001b[92m195: 250\u001b[1;m\n",
      "\u001b[1;31m196: unaligned. best cand 251\u001b[1;m\n",
      "\u001b[92m197: 251\u001b[1;m\n",
      "\u001b[1;31m198: unaligned. best cand 54\u001b[1;m\n",
      "\u001b[92m199: 253\u001b[1;m\n",
      "\u001b[92m200: 254\u001b[1;m\n",
      "\u001b[92m201: 255\u001b[1;m\n",
      "\u001b[92m202: 256\u001b[1;m\n",
      "\u001b[92m203: 257\u001b[1;m\n",
      "\u001b[92m204: 259\u001b[1;m\n",
      "\u001b[1;31m205: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m206: 262\u001b[1;m\n",
      "\u001b[92m207: 263\u001b[1;m\n",
      "\u001b[92m208: 264\u001b[1;m\n",
      "\u001b[92m209: 265\u001b[1;m\n",
      "\u001b[92m210: 266\u001b[1;m\n",
      "\u001b[92m211: 267\u001b[1;m\n",
      "\u001b[92m212: 268\u001b[1;m\n",
      "\u001b[92m213: 270\u001b[1;m\n",
      "\u001b[92m214: 272\u001b[1;m\n",
      "\u001b[1;31m215: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m216: 274\u001b[1;m\n",
      "\u001b[92m217: 275\u001b[1;m\n",
      "\u001b[1;31m218: unaligned. best cand []\u001b[1;m\n",
      "\u001b[1;31m219: unaligned. best cand []\u001b[1;m\n",
      "\u001b[92m220: 278\u001b[1;m\n",
      "\u001b[1;31m221: unaligned. best cand 244\u001b[1;m\n",
      "\u001b[1;31m222: unaligned. best cand []\u001b[1;m\n",
      "\u001b[1;31m223: unaligned. best cand []\u001b[1;m\n",
      "\n",
      "197 out of 224 source sentences aligned by BLEU 87.9464285714%\n",
      "after gap filling, 221 out of 224 source sentences aligned 98.6607142857%\n",
      "after gap filling, 258 out of 281 target sentences aligned 91.8149466192%\n",
      "alignment: 0 - 0\n",
      "alignment: 1 - 1\n",
      "alignment: 2 - 2\n",
      "alignment: 3 - 3\n",
      "alignment: 4 - 5,6\n",
      "alignment: 5 - 7\n",
      "alignment: 6 - 8\n",
      "alignment: 7 - 9\n",
      "alignment: 8 - 10\n",
      "alignment: 9 - 11\n",
      "alignment: 10 - 12\n",
      "alignment: 11 - 13\n",
      "alignment: 12 - 14,15,16\n",
      "alignment: 13 - 17\n",
      "alignment: 14 - 18\n",
      "alignment: 15 - 19\n",
      "alignment: 16 - 20\n",
      "alignment: 17 - 22\n",
      "alignment: 18 - 23\n",
      "alignment: 19,20 - 24\n",
      "alignment: 21 - 25\n",
      "alignment: 22 - 26,27\n",
      "alignment: 23 - 28\n",
      "alignment: 24 - 29,30\n",
      "alignment: 25 - 31\n",
      "alignment: 26 - 32\n",
      "alignment: 27 - 33\n",
      "alignment: 28 - 34,35\n",
      "alignment: 29 - 36\n",
      "alignment: 30 - 37\n",
      "alignment: 31 - 38\n",
      "alignment: 32 - 39,40\n",
      "alignment: 33 - 41\n",
      "alignment: 34 - 42\n",
      "alignment: 35 - 43\n",
      "alignment: 36 - 44\n",
      "alignment: 37 - 45\n",
      "alignment: 38 - 46\n",
      "alignment: 39 - 47\n",
      "alignment: 40 - 48\n",
      "alignment: 41 - 49\n",
      "alignment: 42 - 50\n",
      "alignment: 43 - 51\n",
      "alignment: 44 - 52\n",
      "alignment: 45 - 53\n",
      "alignment: 46 - 54,55\n",
      "alignment: 47 - 56\n",
      "alignment: 48 - 57,58\n",
      "alignment: 49 - 59\n",
      "alignment: 50 - 60\n",
      "alignment: 51 - 61\n",
      "alignment: 52 - 62,63\n",
      "alignment: 53 - 64\n",
      "alignment: 54 - 65\n",
      "alignment: 55 - 66\n",
      "alignment: 56 - 67\n",
      "alignment: 57 - 68\n",
      "alignment: 58 - 69\n",
      "alignment: 59 - 70\n",
      "alignment: 60 - 71\n",
      "alignment: 61 - 72\n",
      "alignment: 62 - 73\n",
      "alignment: 63 - 74,75\n",
      "alignment: 64 - 76\n",
      "alignment: 65 - 77\n",
      "alignment: 66 - 78\n",
      "alignment: 67 - 79\n",
      "alignment: 68 - 80,81\n",
      "alignment: 69 - 82\n",
      "alignment: 71 - 83,84\n",
      "alignment: 72 - 85,86\n",
      "alignment: 73 - 87\n",
      "alignment: 74 - 88\n",
      "alignment: 75 - 89\n",
      "alignment: 76 - 90\n",
      "alignment: 77 - 91\n",
      "alignment: 78 - 92\n",
      "alignment: 79 - 94,95\n",
      "alignment: 80 - 96,97\n",
      "alignment: 81 - 98,99,100\n",
      "alignment: 82 - 101\n",
      "alignment: 83 - 102\n",
      "alignment: 84 - 103,104\n",
      "alignment: 85 - 105\n",
      "alignment: 86 - 107\n",
      "alignment: 87 - 108\n",
      "alignment: 88 - 109\n",
      "alignment: 89 - 110\n",
      "alignment: 90 - 111\n",
      "alignment: 91 - 112\n",
      "alignment: 92 - 113\n",
      "alignment: 93 - 114\n",
      "alignment: 94 - 115\n",
      "alignment: 95 - 116\n",
      "alignment: 96 - 117\n",
      "alignment: 97 - 118\n",
      "alignment: 98 - 119,120\n",
      "alignment: 99 - 121\n",
      "alignment: 100 - 122\n",
      "alignment: 101 - 124\n",
      "alignment: 102 - 125\n",
      "alignment: 103 - 126\n",
      "alignment: 104 - 127\n",
      "alignment: 105 - 128\n",
      "alignment: 106 - 129\n",
      "alignment: 107 - 130\n",
      "alignment: 108 - 131,132\n",
      "alignment: 109 - 133\n",
      "alignment: 110 - 134\n",
      "alignment: 111,112 - 136,137\n",
      "alignment: 113 - 139\n",
      "alignment: 114 - 140\n",
      "alignment: 115 - 141\n",
      "alignment: 116 - 142\n",
      "alignment: 117,118 - 143\n",
      "alignment: 119 - 145\n",
      "alignment: 120 - 146\n",
      "alignment: 121 - 147\n",
      "alignment: 122 - 148\n",
      "alignment: 123 - 149\n",
      "alignment: 124 - 150\n",
      "alignment: 125 - 151\n",
      "alignment: 126 - 153,154,155\n",
      "alignment: 127 - 156\n",
      "alignment: 128 - 157\n",
      "alignment: 129 - 159\n",
      "alignment: 130 - 160\n",
      "alignment: 131 - 161\n",
      "alignment: 132,133 - 162\n",
      "alignment: 134 - 164\n",
      "alignment: 135 - 165,166\n",
      "alignment: 136 - 167,168,169\n",
      "alignment: 137 - 170\n",
      "alignment: 138,139 - 171\n",
      "alignment: 141 - 172,173\n",
      "alignment: 142 - 175\n",
      "alignment: 143 - 176\n",
      "alignment: 144 - 177\n",
      "alignment: 145 - 178\n",
      "alignment: 146 - 179\n",
      "alignment: 147 - 180\n",
      "alignment: 148 - 181\n",
      "alignment: 149 - 182\n",
      "alignment: 150 - 183\n",
      "alignment: 151 - 184\n",
      "alignment: 152 - 185\n",
      "alignment: 153 - 186\n",
      "alignment: 154 - 187,188\n",
      "alignment: 155 - 189\n",
      "alignment: 156 - 190,191\n",
      "alignment: 157 - 192\n",
      "alignment: 158 - 193\n",
      "alignment: 159 - 194,195\n",
      "alignment: 160 - 197,198\n",
      "alignment: 161 - 199\n",
      "alignment: 162 - 200,201\n",
      "alignment: 163 - 202\n",
      "alignment: 164 - 203\n",
      "alignment: 165 - 204\n",
      "alignment: 167 - 209\n",
      "alignment: 168 - 210,211\n",
      "alignment: 169 - 212\n",
      "alignment: 170 - 213\n",
      "alignment: 171 - 214\n",
      "alignment: 172 - 215\n",
      "alignment: 173 - 216\n",
      "alignment: 174 - 217,218\n",
      "alignment: 175 - 219\n",
      "alignment: 176 - 220\n",
      "alignment: 177 - 221\n",
      "alignment: 178 - 222\n",
      "alignment: 179 - 223\n",
      "alignment: 180 - 224\n",
      "alignment: 181 - 225\n",
      "alignment: 182 - 226,227\n",
      "alignment: 183 - 228\n",
      "alignment: 184 - 229,230\n",
      "alignment: 185 - 232\n",
      "alignment: 186 - 233\n",
      "alignment: 187 - 234,235\n",
      "alignment: 188 - 237,238\n",
      "alignment: 189 - 239\n",
      "alignment: 190 - 240\n",
      "alignment: 191 - 241\n",
      "alignment: 192 - 242,243\n",
      "alignment: 193 - 244,245,246\n",
      "alignment: 194 - 249,250\n",
      "alignment: 195 - 250\n",
      "alignment: 196,197 - 251\n",
      "alignment: 198 - 252\n",
      "alignment: 199 - 253\n",
      "alignment: 200 - 254\n",
      "alignment: 201 - 255\n",
      "alignment: 202 - 256\n",
      "alignment: 203 - 257,258\n",
      "alignment: 204 - 259\n",
      "alignment: 205,206 - 261,262\n",
      "alignment: 207 - 263\n",
      "alignment: 208 - 264\n",
      "alignment: 209 - 265\n",
      "alignment: 210 - 266\n",
      "alignment: 211 - 267\n",
      "alignment: 212 - 268\n",
      "alignment: 213 - 269,270\n",
      "alignment: 214 - 271,272\n",
      "alignment: 215,216 - 274\n",
      "alignment: 217 - 275\n",
      "alignment: 218 - 276\n",
      "alignment: 219 - 277\n",
      "alignment: 220,221 - 278\n",
      "alignment: 222,223 - 280\n",
      "\n",
      "finished with article\n",
      "\n",
      "====================\n",
      "\n",
      "sentence alignment took 1.4142279624938965 seconds\n"
     ]
    }
   ],
   "source": [
    "# %%capture cap --no-stderr\n",
    "start = time.time()\n",
    "!./bleualign.py -s sourcetextforbleualign.txt -t targettextforbleualign.txt --srctotarget translatedsource.txt -o outputfile --verbosity 2\n",
    "end = time.time()\n",
    "print(f'sentence alignment took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# rawsrcsents[218], rawtgtsents[276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# output = cap.stdout\n",
    "\n",
    "# indexpairs = []\n",
    "# split = output.split('finished with article')[0].split('alignment: ')[1:]\n",
    "# for string in split:\n",
    "#     string = string.replace('\\r\\n','')\n",
    "#     strindices = string.split(' - ')\n",
    "#     for srcidx in strindices[0].split(','):\n",
    "#         for tgtidx in strindices[1].split(','):\n",
    "#             if (int(srcidx) < len(rawsrcsents) - 1) and (int(tgtidx) < len(rawtgtsents) - 1):\n",
    "#                 indexpairs.append((int(srcidx)+1, int(tgtidx) + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rawsrcsents[4], rawtgtsents[5:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [START HERE] 3. Read sentence-aligned files (from Bleualign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputfile-s','r') as f:\n",
    "    alignedsrc = f.read().split('\\n')\n",
    "with open('outputfile-t','r') as f:\n",
    "    alignedtgt = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Sé que de todos los problemas, ninguno lo inquietó y lo trabajó como el abismal problema del tiempo.',\n",
       " 'I know that of all problems, none disquieted him more, and none concerned him more than the profound one of time.')"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(alignedsrc)))\n",
    "alignedsrc[i], alignedtgt[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(alignedsrc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent to sent alignment\n",
    "\n",
    "sentAlignments = []\n",
    "alignmentLookup = dict()\n",
    "\n",
    "def isthisamatch(sent1,sent2,alignedsent):\n",
    "    if (sent1 + ' '+ sent2).strip() == alignedsent.strip():\n",
    "        return True\n",
    "    elif (sent2 + ' '+ sent1).strip() == alignedsent.strip():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "for srcsent, tgtsent in zip(alignedsrc, alignedtgt):\n",
    "    srcmatches = []\n",
    "    tgtmatches = []\n",
    "    for s1, src1 in enumerate(rawsrcsents):\n",
    "        if src1.strip() == srcsent.strip():\n",
    "            srcmatches = [{'index' : s1, 'sent' : src1}]\n",
    "\n",
    "        else:\n",
    "            for s2, src2 in enumerate(rawsrcsents):\n",
    "                if isthisamatch(src1, src2, srcsent):\n",
    "                    srcmatches = [{'index' : s1 ,'sent' : src1}, {'index' : s2,'sent' : src2}]\n",
    "                    continue\n",
    "                    \n",
    "    for t1, tgt1 in enumerate(rawtgtsents):\n",
    "        if tgt1.strip() == tgtsent.strip():\n",
    "            tgtmatches = [{'index' : t1, 'sent' : tgt1}]\n",
    "\n",
    "        else:\n",
    "            for t2, tgt2 in enumerate(rawtgtsents):\n",
    "                if isthisamatch(tgt1, tgt1, tgtsent):\n",
    "                    tgtmatches = [{'index' : t1 ,'sent' : tgt1}, {'index' : t2,'sent' : tgt2}]\n",
    "                    continue\n",
    "                \n",
    "    for s in srcmatches:\n",
    "        for t in tgtmatches:\n",
    "            sentAlignments.append({\n",
    "                'alignedsentindices' : (s['index'], t['index']),\n",
    "                'alignedsents' : (rawsrcsents[s['index']], rawtgtsents[t['index']])\n",
    "            })\n",
    "            \n",
    "            alignmentLookup[s['index']] = t['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentAlignment3-28.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentAlignments, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE DONT NEED - check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me apresuré; el próximo saldría a las nueve y media.\n",
      "I hurried, for the next would not go until half past nine.\n"
     ]
    }
   ],
   "source": [
    "# chec, k it works\n",
    "randSentAlign = random.choice(sentAlignments)\n",
    "s, t = randSentAlign['alignedsents']\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parse word alignment using SimAlign (recommended: fast and high coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install simalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-03-29 09:32:48,750 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading word aligner tool took 8.547110080718994 seconds\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "start = time.time()\n",
    "# making an instance of our model.\n",
    "# You can specify the embedding model and all alignment settings in the constructor.\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")\n",
    "end = time.time()\n",
    "print(f'downloading word aligner tool took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate word alignment with SimAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/223 sentences parsed in 0.00034689903259277344 s.\n",
      "25/223 sentences parsed in 25.049719095230103 s.\n",
      "50/223 sentences parsed in 35.06890416145325 s.\n",
      "75/223 sentences parsed in 48.04298210144043 s.\n",
      "100/223 sentences parsed in 65.13521909713745 s.\n",
      "125/223 sentences parsed in 100.23047614097595 s.\n",
      "150/223 sentences parsed in 117.76058912277222 s.\n",
      "150/223 sentences parsed in 117.82086110115051 s.\n",
      "150/223 sentences parsed in 117.83392810821533 s.\n",
      "parsed in 128.95235204696655 s\n"
     ]
    }
   ],
   "source": [
    "# get rid of white space at end\n",
    "your_data = zip(srcsents[:-1], tgtsents[:-1])\n",
    "your_data = zip(rawsrcsents, rawtgtsents)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "wordAlignmentList = []\n",
    "\n",
    "for i, srcsent in enumerate(rawsrcsents):\n",
    "    if i % 25 == 0:\n",
    "        currently = time.time()\n",
    "        print(f\"{i}/{len(rawsrcsents)} sentences parsed in {currently-start} s.\")\n",
    "\n",
    "    srcDoc = spaNLP(srcsent)\n",
    "    \n",
    "    srcTokens = []\n",
    "    for token in srcDoc:\n",
    "        srcTokens.append({\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        })\n",
    "\n",
    "    try:\n",
    "        j = alignmentLookup[i]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    tgtDoc = engNLP(rawtgtsents[j])\n",
    "\n",
    "    tgtTokens = []\n",
    "    for token in tgtDoc:\n",
    "        tgtTokens.append({\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        })\n",
    "    \n",
    "    src = [t.text for t in srcDoc]\n",
    "    tgt = [t.text for t in tgtDoc]\n",
    "\n",
    "    alignments = myaligner.get_word_aligns(src, tgt)\n",
    "    itermax = alignments['itermax']\n",
    "\n",
    "    wordAlignmentList.append({\n",
    "        'alignedwordindices' : itermax,\n",
    "        'alignedwords' : [(src[s], tgt[t]) for s, t in itermax],\n",
    "        'srctokens' : srcTokens,\n",
    "        'tgttokens' : tgtTokens,\n",
    "        'srcsentidx' : i,\n",
    "        'tgtsentidx' : j,\n",
    "    })\n",
    "end = time.time()\n",
    "print('parsed in',end-start,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to JSON or CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordAlignment3-28.json', 'w',encoding='utf-8') as f:\n",
    "    json.dump(alignmentList, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "srctokens = []\n",
    "tgttokens = []\n",
    "for srcsent in rawsrcsents:\n",
    "    srcdoc = spaNLP(srcsent)\n",
    "    srctokens.append([{\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        } for token in srcdoc])\n",
    "for tgtsent in rawtgtsents:\n",
    "    tgtdoc = engNLP(tgtsent)\n",
    "    tgttokens.append([{\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        } for token in tgtdoc])\n",
    "\n",
    "sentsInOrderJSON = {'srcSentsInOrder' : {'text' : rawsrcsents, 'tokens' : srctokens}, 'tgtSentsInOrder' : {'text' : rawtgtsents, 'tokens' : tgttokens}}\n",
    "with open('sentsInOrder3-28.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentsInOrderJSON, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Alignment (don't need in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alignedwordindices': [(0, 9),\n",
       "  (1, 0),\n",
       "  (1, 1),\n",
       "  (1, 12),\n",
       "  (2, 3),\n",
       "  (3, 4),\n",
       "  (4, 5),\n",
       "  (5, 7),\n",
       "  (6, 8),\n",
       "  (7, 15),\n",
       "  (8, 10),\n",
       "  (9, 11),\n",
       "  (10, 13),\n",
       "  (11, 14)],\n",
       " 'alignedwords': [('»', '\"'),\n",
       "  ('Reflexioné', 'I'),\n",
       "  ('Reflexioné', 'thought'),\n",
       "  ('Reflexioné', 'is'),\n",
       "  ('un', 'a'),\n",
       "  ('momento', 'moment'),\n",
       "  ('y', 'and'),\n",
       "  ('repuse', 'replied'),\n",
       "  (':', ':'),\n",
       "  ('»', '\"'),\n",
       "  ('-La', 'The'),\n",
       "  ('palabra', 'word'),\n",
       "  ('ajedrez', 'chess'),\n",
       "  ('.', '.')],\n",
       " 'srctokens': [{'tokenid': 0,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '»',\n",
       "   'lemma': '»',\n",
       "   'features': {'PunctType': 'Colo'}},\n",
       "  {'tokenid': 1,\n",
       "   'pos': 'PROPN',\n",
       "   'text': 'Reflexioné',\n",
       "   'lemma': 'Reflexioné',\n",
       "   'features': {}},\n",
       "  {'tokenid': 12,\n",
       "   'pos': 'DET',\n",
       "   'text': 'un',\n",
       "   'lemma': 'uno',\n",
       "   'features': {'Definite': 'Ind',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing',\n",
       "    'PronType': 'Art'}},\n",
       "  {'tokenid': 15,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'momento',\n",
       "   'lemma': 'momento',\n",
       "   'features': {'Gender': 'Masc', 'Number': 'Sing'}},\n",
       "  {'tokenid': 23, 'pos': 'CCONJ', 'text': 'y', 'lemma': 'y', 'features': {}},\n",
       "  {'tokenid': 25,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'repuse',\n",
       "   'lemma': 'repuse',\n",
       "   'features': {'Mood': 'Imp',\n",
       "    'Number': 'Plur',\n",
       "    'Person': '3',\n",
       "    'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 31,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ':',\n",
       "   'lemma': ':',\n",
       "   'features': {'PunctType': 'Colo'}},\n",
       "  {'tokenid': 33, 'pos': 'PROPN', 'text': '»', 'lemma': '»', 'features': {}},\n",
       "  {'tokenid': 34,\n",
       "   'pos': 'PROPN',\n",
       "   'text': '-La',\n",
       "   'lemma': '-La',\n",
       "   'features': {}},\n",
       "  {'tokenid': 38,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'palabra',\n",
       "   'lemma': 'palabra',\n",
       "   'features': {'Gender': 'Fem', 'Number': 'Sing'}},\n",
       "  {'tokenid': 46,\n",
       "   'pos': 'PROPN',\n",
       "   'text': 'ajedrez',\n",
       "   'lemma': 'ajedrez',\n",
       "   'features': {}},\n",
       "  {'tokenid': 53,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '.',\n",
       "   'lemma': '.',\n",
       "   'features': {'PunctType': 'Peri'}}],\n",
       " 'tgttokens': [{'tokenid': 0,\n",
       "   'pos': 'PRON',\n",
       "   'text': 'I',\n",
       "   'lemma': 'I',\n",
       "   'features': {'Case': 'Nom',\n",
       "    'Number': 'Sing',\n",
       "    'Person': '1',\n",
       "    'PronType': 'Prs'}},\n",
       "  {'tokenid': 2,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'thought',\n",
       "   'lemma': 'think',\n",
       "   'features': {'Tense': 'Past', 'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 10, 'pos': 'ADP', 'text': 'for', 'lemma': 'for', 'features': {}},\n",
       "  {'tokenid': 14,\n",
       "   'pos': 'DET',\n",
       "   'text': 'a',\n",
       "   'lemma': 'a',\n",
       "   'features': {'Definite': 'Ind', 'PronType': 'Art'}},\n",
       "  {'tokenid': 16,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'moment',\n",
       "   'lemma': 'moment',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 23,\n",
       "   'pos': 'CCONJ',\n",
       "   'text': 'and',\n",
       "   'lemma': 'and',\n",
       "   'features': {'ConjType': 'Cmp'}},\n",
       "  {'tokenid': 27,\n",
       "   'pos': 'ADV',\n",
       "   'text': 'then',\n",
       "   'lemma': 'then',\n",
       "   'features': {'PronType': 'Dem'}},\n",
       "  {'tokenid': 32,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'replied',\n",
       "   'lemma': 'reply',\n",
       "   'features': {'Tense': 'Past', 'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 39, 'pos': 'PUNCT', 'text': ':', 'lemma': ':', 'features': {}},\n",
       "  {'tokenid': 41,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '\"',\n",
       "   'lemma': '\"',\n",
       "   'features': {'PunctSide': 'Ini', 'PunctType': 'Quot'}},\n",
       "  {'tokenid': 42,\n",
       "   'pos': 'DET',\n",
       "   'text': 'The',\n",
       "   'lemma': 'the',\n",
       "   'features': {'Definite': 'Def', 'PronType': 'Art'}},\n",
       "  {'tokenid': 46,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'word',\n",
       "   'lemma': 'word',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 51,\n",
       "   'pos': 'AUX',\n",
       "   'text': 'is',\n",
       "   'lemma': 'be',\n",
       "   'features': {'Mood': 'Ind',\n",
       "    'Number': 'Sing',\n",
       "    'Person': '3',\n",
       "    'Tense': 'Pres',\n",
       "    'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 54,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'chess',\n",
       "   'lemma': 'chess',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 59,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '.',\n",
       "   'lemma': '.',\n",
       "   'features': {'PunctType': 'Peri'}},\n",
       "  {'tokenid': 60,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '\"',\n",
       "   'lemma': '\"',\n",
       "   'features': {'PunctSide': 'Fin', 'PunctType': 'Quot'}}],\n",
       " 'srcsentidx': 183,\n",
       " 'tgtsentidx': 227}"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(wordAlignmentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # (1, 0),\n",
    " #  (1, 1),\n",
    " #  (1, 12),\n",
    " #  (2, 3),\n",
    " #  (3, 4),\n",
    " #  (4, 5),\n",
    " #  (5, 7),\n",
    " #  (6, 8),\n",
    " #  (7, 15),\n",
    " #  (8, 10),\n",
    " #  (9, 11),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('borges_word_alignment_3-21.pickle', 'wb') as handle:\n",
    "#     pickle.dump(alignmentList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE DONT NEED: an example of simalign on a single pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwmf : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (4, 7), (5, 4), (6, 15), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 16), (16, 16), (17, 20), (18, 17), (19, 18), (20, 22), (21, 21), (21, 23), (22, 19), (22, 24), (23, 25)]\n",
      "inter : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (16, 16), (19, 18), (20, 22), (21, 23), (22, 24), (23, 25)]\n",
      "itermax : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (5, 4), (6, 7), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 16), (16, 16), (18, 19), (19, 18), (20, 22), (21, 23), (22, 24), (23, 25)]\n"
     ]
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcDoc = spaNLP.tokenizer(srcsents[i])\n",
    "tgtDoc = engNLP.tokenizer(tgtsents[i])\n",
    "src = [t.text for t in srcDoc]\n",
    "tgt = [t.text for t in tgtDoc]\n",
    "alignments = myaligner.get_word_aligns(src, tgt)\n",
    "\n",
    "for match in alignments:\n",
    "    print(match, ':', alignments[match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "novelista he\n",
      "genial fine\n",
      ", ,\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "sin doubtless\n",
      "duda doubtless\n",
      "no more\n",
      "se ,\n",
      "consideró considered\n",
      "un a\n",
      "mero than\n",
      "mero mere\n",
      "novelista himself\n",
      "novelista novelist\n",
      ". .\n",
      "\n",
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "duda doubtless\n",
      "consideró considered\n",
      "un a\n",
      "mero mere\n",
      "novelista novelist\n",
      ". .\n",
      "\n",
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "genial fine\n",
      ", he\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "sin doubtless\n",
      "duda doubtless\n",
      "se himself\n",
      "consideró considered\n",
      "un a\n",
      "mero mere\n",
      "novelista novelist\n",
      ". .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match in alignments:\n",
    "    for s, t in alignments[match]:\n",
    "        print(src[s], tgt[t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get rid of white space at end\n",
    "# your_data = zip(srcsents[:-1], tgtsents[:-1])\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# alignmentList = []\n",
    "# t = 0\n",
    "\n",
    "# for sent_es_str, sent_en_str in your_data:\n",
    "#     if t % 25 == 0:\n",
    "#         currently = time.time()\n",
    "#         print(f'{t/{len(srcsents)} sentences parsed in {currently-start} s.')\n",
    "\n",
    "#     srcDoc = spaNLP(sent_es_str)\n",
    "#     tgtDoc = engNLP(sent_en_str)\n",
    "    \n",
    "#     srcTokens = []\n",
    "#     for token in srcDoc:\n",
    "#         srcTokens.append({\n",
    "#             'tokenid' : token.idx,\n",
    "#             'pos' : token.pos_, \n",
    "#             'text' : token.text, \n",
    "#             'lemma' : token.lemma_,\n",
    "#             'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "#         })\n",
    "\n",
    "#     tgtTokens = []\n",
    "#     for token in tgtDoc:\n",
    "#         tgtTokens.append({\n",
    "#             'tokenid' : token.idx,\n",
    "#             'pos' : token.pos_, \n",
    "#             'text' : token.text, \n",
    "#             'lemma' : token.lemma_,\n",
    "#             'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "#         })\n",
    "\n",
    "#     src = [t.text for t in srcDoc]\n",
    "#     tgt = [t.text for t in tgtDoc]\n",
    "    \n",
    "#     alignments = myaligner.get_word_aligns(src, tgt)\n",
    "#     itermax = alignments['itermax']\n",
    "#     try:\n",
    "#         j = alignmentLookup[i]\n",
    "#     except:\n",
    "#         j = 'No Aligned Sentence'\n",
    "    \n",
    "    \n",
    "#     alignmentList.append({\n",
    "#         'alignedwordindices' : itermax,\n",
    "#         'alignedwords' : [(src[s], tgt[t]) for s, t in itermax],\n",
    "#         'srctokens' : srcTokens,\n",
    "#         'tgttokens' : tgtTokens,\n",
    "#         'srcsentidx' : i,\n",
    "#         'tgtsentidx' : j,\n",
    "#     })\n",
    "    \n",
    "#     t += 1\n",
    "# end = time.time()\n",
    "# print('parsed in',end-start,'s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
