{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install libraries, modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "from spacy.morphology import Morphology\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import json\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install spacy lang models\n",
    "\n",
    "spaNLP = spacy.load(\"es_core_news_sm\")\n",
    "engNLP = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load raw texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('texts/rawsource.txt','r') as f:\n",
    "     sourcetxt = f.read()\n",
    "with open('texts/rawtarget.txt','r') as f:\n",
    "     targettxt = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourcedoc = spaNLP(sourcetxt)\n",
    "targetdoc = engNLP(targettxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentenize\n",
    "rawsrcsents = []\n",
    "rawtgtsents = []\n",
    "for sent in sourcedoc.sents:\n",
    "    rawsrcsents.append(sent.text)\n",
    "for sent in targetdoc.sents:\n",
    "    rawtgtsents.append(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write standardized files (one line per sentence) for input to Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the inputs to bleualign\n",
    "with open('sourcetextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawsrcsents))\n",
    "with open('targettextforbleualign.txt','w') as f:\n",
    "    f.write('\\n'.join(rawtgtsents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized sentences for data output\n",
    "srctokens = []\n",
    "for srcsent in rawsrcsents:\n",
    "    tokens = spaNLP(srcsent)\n",
    "    srctokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])\n",
    "tgttokens = []\n",
    "for tgtsent in rawtgtsents:\n",
    "    tokens = engNLP(tgtsent)\n",
    "    tgttokens.append([{'text' : t.text, 'lemma' : t.lemma_} for t in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentsInOrder3-28.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump({'srcSentsInOrder' : {'text' : rawsrcsents, 'tokens' : srctokens}, 'tgtSentsInOrder' : {'text' : rawtgtsents, 'tokens' : tgttokens}}, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### At this point, I also generate sourcetexttranslation.text using Google Translate and run Bleualign on the texts on my terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bleualign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install translators --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machine translation took 212.9293007850647 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "translatedsourcesents = []\n",
    "for sent in rawsrcsents:\n",
    "    translatedsourcesents.append(ts.google(sent, to_language = 'en'))\n",
    "end = time.time()\n",
    "print(f'machine translation took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translatedsource.txt','w') as f:\n",
    "    f.write('\\n'.join(translatedsourcesents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in article 0: \n",
      "processing\n",
      "computing alignment between srctotarget (file 0) and target text\n",
      "Evaluating sentences with bleu\n",
      "finished\n",
      "searching for longest path of good alignments\n",
      "finished\n",
      "filling gaps\n",
      "finished\n",
      "\n",
      "finished with article\n",
      "\n",
      "====================\n",
      "\n",
      "sentence alignment took 2.1918561458587646 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "!./bleualign.py -s sourcetextforbleualign.txt -t targettextforbleualign.txt --srctotarget translatedsource.txt -o outputfile\n",
    "end = time.time()\n",
    "print(f'sentence alignment took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [START HERE] 3. Read sentence-aligned files (from Bleualign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputfile-s','r') as f:\n",
    "    srcsents = f.read().split('\\n')\n",
    "with open('outputfile-t','r') as f:\n",
    "    tgtsents = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Madden, en el departamento de Viktor Runeberg, quería decir el fin de nuestros afanes y -pero eso parecía muy secundario, o debía parecérmelo- también de nuestras vidas.',\n",
       " \"Madden, in Viktor Runeberg's office, meant the end of all our work and - though this seemed a secondary matter, or should have seemed so to me - of our lives also.\")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcsents[i], tgtsents[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/212 sentences parsed.\n",
      "50/212 sentences parsed.\n",
      "100/212 sentences parsed.\n",
      "150/212 sentences parsed.\n",
      "200/212 sentences parsed.\n"
     ]
    }
   ],
   "source": [
    "# sent to sent alignment\n",
    "oneLineSpa, oneLineEng = rawsrcsents, rawtgtsents\n",
    "alignedSpa, alignedEng = srcsents, tgtsents\n",
    "sentAlignments = []\n",
    "alignmentLookup = dict()\n",
    "\n",
    "spaIndex = 0\n",
    "for alignSpaSent, alignEngSent in zip(alignedSpa, alignedEng):\n",
    "    if spaIndex % 50 == 0:\n",
    "        print(f'{spaIndex}/{len(srcsents)} sentences parsed.')\n",
    "    individualEngSents = [sent.text for sent in engNLP(alignEngSent).sents]\n",
    "    for indEngSent in individualEngSents:\n",
    "        for i, thisEngLine in enumerate(oneLineEng):\n",
    "            if indEngSent.strip() == thisEngLine.strip():\n",
    "                engIndex = i\n",
    "        for j, thisSpaLine in enumerate(oneLineSpa):\n",
    "            if alignSpaSent.strip() == thisSpaLine.strip():\n",
    "                spaIndex = j\n",
    "        sentAlignments.append({\n",
    "            'alignedsentindices' : (spaIndex, engIndex),\n",
    "            'alignedsents' : (oneLineSpa[spaIndex], oneLineEng[engIndex])\n",
    "        })\n",
    "        alignmentLookup[spaIndex] = engIndex\n",
    "    spaIndex += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sentAlignment3-28.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(sentAlignments, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLE DONT NEED - check it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"¿Ashgrove?\", les pregunté a unos chicos en el andén.\n",
      "I asked some children on the platform.\n"
     ]
    }
   ],
   "source": [
    "# chec, k it works\n",
    "randSentAlign = random.choice(sentAlignments)\n",
    "s, t = randSentAlign['alignedsents']\n",
    "print(s)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Parse word alignment using SimAlign (recommended: fast and high coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install simalign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-03-29 09:32:48,750 - simalign.simalign - INFO - Initialized the EmbeddingLoader with model: bert-base-multilingual-cased\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading word aligner tool took 8.547110080718994 seconds\n"
     ]
    }
   ],
   "source": [
    "from simalign import SentenceAligner\n",
    "start = time.time()\n",
    "# making an instance of our model.\n",
    "# You can specify the embedding model and all alignment settings in the constructor.\n",
    "myaligner = SentenceAligner(model=\"bert\", token_type=\"bpe\", matching_methods=\"mai\")\n",
    "end = time.time()\n",
    "print(f'downloading word aligner tool took {end-start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate word alignment with SimAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/212 sentences parsed in 0.003075122833251953 s.\n",
      "25/212 sentences parsed in 29.386754035949707 s.\n",
      "50/212 sentences parsed in 53.92922806739807 s.\n",
      "75/212 sentences parsed in 71.35570216178894 s.\n",
      "100/212 sentences parsed in 104.39477396011353 s.\n",
      "125/212 sentences parsed in 120.35754227638245 s.\n",
      "150/212 sentences parsed in 142.1159610748291 s.\n",
      "175/212 sentences parsed in 160.01262426376343 s.\n",
      "200/212 sentences parsed in 177.03865003585815 s.\n",
      "parsed in 183.1890082359314 s\n"
     ]
    }
   ],
   "source": [
    "your_data = zip(srcsents[:-1], tgtsents[:-1])\n",
    "start = time.time()\n",
    "\n",
    "alignmentList = []\n",
    "i = 0\n",
    "for sent_es_str, sent_en_str in your_data:\n",
    "    if i % 25 == 0:\n",
    "        currently = time.time()\n",
    "        print(f'{i}/{len(srcsents)} sentences parsed in {currently-start} s.')\n",
    "    srcDoc = spaNLP(sent_es_str)\n",
    "    tgtDoc = engNLP(sent_en_str)\n",
    "    \n",
    "    srcTokens = []\n",
    "    for token in srcDoc:\n",
    "        srcTokens.append({\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        })\n",
    "    tgtTokens = []\n",
    "    for token in tgtDoc:\n",
    "        tgtTokens.append({\n",
    "            'tokenid' : token.idx,\n",
    "            'pos' : token.pos_, \n",
    "            'text' : token.text, \n",
    "            'lemma' : token.lemma_,\n",
    "            'features' : Morphology.feats_to_dict(str(token.morph))\n",
    "        })\n",
    "\n",
    "    src = [t.text for t in srcDoc]\n",
    "    tgt = [t.text for t in tgtDoc]\n",
    "    \n",
    "    alignments = myaligner.get_word_aligns(src, tgt)\n",
    "    itermax = alignments['itermax']\n",
    "    try:\n",
    "        j = alignmentLookup[i]\n",
    "    except:\n",
    "        j = 'No Aligned Sentence'\n",
    "    \n",
    "    \n",
    "    alignmentList.append({\n",
    "        'alignedwordindices' : itermax,\n",
    "        'alignedwords' : [(src[s], tgt[t]) for s, t in itermax],\n",
    "        'srctokens' : srcTokens,\n",
    "        'tgttokens' : tgtTokens,\n",
    "        'srcsentidx' : i,\n",
    "        'tgtsentidx' : j,\n",
    "    })\n",
    "    \n",
    "    i += 1\n",
    "end = time.time()\n",
    "print('parsed in',end-start,'s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to JSON or CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordAlignment3-28.json', 'w',encoding='utf-8') as f:\n",
    "    json.dump(alignmentList, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of Alignment (don't need in Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alignedwordindices': [(0, 0),\n",
       "  (1, 1),\n",
       "  (2, 3),\n",
       "  (3, 4),\n",
       "  (5, 5),\n",
       "  (6, 7),\n",
       "  (7, 8),\n",
       "  (8, 12),\n",
       "  (10, 13),\n",
       "  (11, 9),\n",
       "  (12, 14)],\n",
       " 'alignedwords': [('El', 'The'),\n",
       "  ('camino', 'road'),\n",
       "  ('bajaba', 'descending'),\n",
       "  ('y', 'and'),\n",
       "  ('bifurcaba', 'branching'),\n",
       "  (',', ','),\n",
       "  ('entre', 'through'),\n",
       "  ('las', 'the'),\n",
       "  ('confusas', 'twilight'),\n",
       "  ('praderas', 'meadows'),\n",
       "  ('.', '.')],\n",
       " 'srctokens': [{'tokenid': 0,\n",
       "   'pos': 'DET',\n",
       "   'text': 'El',\n",
       "   'lemma': 'el',\n",
       "   'features': {'Definite': 'Def',\n",
       "    'Gender': 'Masc',\n",
       "    'Number': 'Sing',\n",
       "    'PronType': 'Art'}},\n",
       "  {'tokenid': 3,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'camino',\n",
       "   'lemma': 'camino',\n",
       "   'features': {'Gender': 'Masc', 'Number': 'Sing'}},\n",
       "  {'tokenid': 10,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'bajaba',\n",
       "   'lemma': 'bajar',\n",
       "   'features': {'Mood': 'Ind',\n",
       "    'Number': 'Sing',\n",
       "    'Person': '3',\n",
       "    'Tense': 'Imp',\n",
       "    'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 17, 'pos': 'CCONJ', 'text': 'y', 'lemma': 'y', 'features': {}},\n",
       "  {'tokenid': 19,\n",
       "   'pos': 'PRON',\n",
       "   'text': 'se',\n",
       "   'lemma': 'él',\n",
       "   'features': {'Case': 'Acc',\n",
       "    'Person': '3',\n",
       "    'PrepCase': 'Npr',\n",
       "    'PronType': 'Prs',\n",
       "    'Reflex': 'Yes'}},\n",
       "  {'tokenid': 22,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'bifurcaba',\n",
       "   'lemma': 'bifurcar',\n",
       "   'features': {'Mood': 'Ind',\n",
       "    'Number': 'Sing',\n",
       "    'Person': '3',\n",
       "    'Tense': 'Imp',\n",
       "    'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 31,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ',',\n",
       "   'lemma': ',',\n",
       "   'features': {'PunctType': 'Comm'}},\n",
       "  {'tokenid': 33,\n",
       "   'pos': 'ADP',\n",
       "   'text': 'entre',\n",
       "   'lemma': 'entre',\n",
       "   'features': {}},\n",
       "  {'tokenid': 39,\n",
       "   'pos': 'DET',\n",
       "   'text': 'las',\n",
       "   'lemma': 'el',\n",
       "   'features': {'Definite': 'Def',\n",
       "    'Gender': 'Fem',\n",
       "    'Number': 'Plur',\n",
       "    'PronType': 'Art'}},\n",
       "  {'tokenid': 43, 'pos': 'ADV', 'text': 'ya', 'lemma': 'ya', 'features': {}},\n",
       "  {'tokenid': 46,\n",
       "   'pos': 'ADJ',\n",
       "   'text': 'confusas',\n",
       "   'lemma': 'confuso',\n",
       "   'features': {'Gender': 'Fem', 'Number': 'Plur'}},\n",
       "  {'tokenid': 55,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'praderas',\n",
       "   'lemma': 'pradera',\n",
       "   'features': {'Gender': 'Fem', 'Number': 'Plur'}},\n",
       "  {'tokenid': 63,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '.',\n",
       "   'lemma': '.',\n",
       "   'features': {'PunctType': 'Peri'}}],\n",
       " 'tgttokens': [{'tokenid': 0,\n",
       "   'pos': 'DET',\n",
       "   'text': 'The',\n",
       "   'lemma': 'the',\n",
       "   'features': {'Definite': 'Def', 'PronType': 'Art'}},\n",
       "  {'tokenid': 4,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'road',\n",
       "   'lemma': 'road',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 9,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'kept',\n",
       "   'lemma': 'keep',\n",
       "   'features': {'Tense': 'Past', 'VerbForm': 'Fin'}},\n",
       "  {'tokenid': 14,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'descending',\n",
       "   'lemma': 'descend',\n",
       "   'features': {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}},\n",
       "  {'tokenid': 25,\n",
       "   'pos': 'CCONJ',\n",
       "   'text': 'and',\n",
       "   'lemma': 'and',\n",
       "   'features': {'ConjType': 'Cmp'}},\n",
       "  {'tokenid': 29,\n",
       "   'pos': 'VERB',\n",
       "   'text': 'branching',\n",
       "   'lemma': 'branch',\n",
       "   'features': {'Aspect': 'Prog', 'Tense': 'Pres', 'VerbForm': 'Part'}},\n",
       "  {'tokenid': 39, 'pos': 'ADP', 'text': 'off', 'lemma': 'off', 'features': {}},\n",
       "  {'tokenid': 42,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': ',',\n",
       "   'lemma': ',',\n",
       "   'features': {'PunctType': 'Comm'}},\n",
       "  {'tokenid': 44,\n",
       "   'pos': 'ADP',\n",
       "   'text': 'through',\n",
       "   'lemma': 'through',\n",
       "   'features': {}},\n",
       "  {'tokenid': 52,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'meadows',\n",
       "   'lemma': 'meadow',\n",
       "   'features': {'Number': 'Plur'}},\n",
       "  {'tokenid': 60,\n",
       "   'pos': 'NUM',\n",
       "   'text': 'misty',\n",
       "   'lemma': 'misty',\n",
       "   'features': {'NumType': 'Card'}},\n",
       "  {'tokenid': 66, 'pos': 'ADP', 'text': 'in', 'lemma': 'in', 'features': {}},\n",
       "  {'tokenid': 69,\n",
       "   'pos': 'DET',\n",
       "   'text': 'the',\n",
       "   'lemma': 'the',\n",
       "   'features': {'Definite': 'Def', 'PronType': 'Art'}},\n",
       "  {'tokenid': 73,\n",
       "   'pos': 'NOUN',\n",
       "   'text': 'twilight',\n",
       "   'lemma': 'twilight',\n",
       "   'features': {'Number': 'Sing'}},\n",
       "  {'tokenid': 81,\n",
       "   'pos': 'PUNCT',\n",
       "   'text': '.',\n",
       "   'lemma': '.',\n",
       "   'features': {'PunctType': 'Peri'}}]}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choice(alignmentList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "\n",
    "# with open('borges_word_alignment_3-21.pickle', 'wb') as handle:\n",
    "#     pickle.dump(alignmentList, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXAMPLE DONT NEED: an example of simalign on a single pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mwmf : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (4, 7), (5, 4), (6, 15), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 16), (16, 16), (17, 20), (18, 17), (19, 18), (20, 22), (21, 21), (21, 23), (22, 19), (22, 24), (23, 25)]\n",
      "inter : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (16, 16), (19, 18), (20, 22), (21, 23), (22, 24), (23, 25)]\n",
      "itermax : [(0, 0), (1, 1), (2, 2), (3, 3), (4, 5), (5, 4), (6, 7), (7, 6), (8, 9), (9, 8), (10, 10), (11, 11), (12, 12), (13, 13), (14, 14), (15, 16), (16, 16), (18, 19), (19, 18), (20, 22), (21, 23), (22, 24), (23, 25)]\n"
     ]
    }
   ],
   "source": [
    "i = random.choice(range(len(srcsents)))\n",
    "srcDoc = spaNLP.tokenizer(srcsents[i])\n",
    "tgtDoc = engNLP.tokenizer(tgtsents[i])\n",
    "src = [t.text for t in srcDoc]\n",
    "tgt = [t.text for t in tgtDoc]\n",
    "alignments = myaligner.get_word_aligns(src, tgt)\n",
    "\n",
    "for match in alignments:\n",
    "    print(match, ':', alignments[match])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "novelista he\n",
      "genial fine\n",
      ", ,\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "sin doubtless\n",
      "duda doubtless\n",
      "no more\n",
      "se ,\n",
      "consideró considered\n",
      "un a\n",
      "mero than\n",
      "mero mere\n",
      "novelista himself\n",
      "novelista novelist\n",
      ". .\n",
      "\n",
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "duda doubtless\n",
      "consideró considered\n",
      "un a\n",
      "mero mere\n",
      "novelista novelist\n",
      ". .\n",
      "\n",
      "Ts’ui Ts'ui\n",
      "Pên Pen\n",
      "fue was\n",
      "un a\n",
      "novelista novelist\n",
      "genial fine\n",
      ", he\n",
      "pero but\n",
      "también also\n",
      "fue was\n",
      "un a\n",
      "hombre man\n",
      "de of\n",
      "letras letters\n",
      "que who\n",
      "sin doubtless\n",
      "duda doubtless\n",
      "se himself\n",
      "consideró considered\n",
      "un a\n",
      "mero mere\n",
      "novelista novelist\n",
      ". .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for match in alignments:\n",
    "    for s, t in alignments[match]:\n",
    "        print(src[s], tgt[t])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
